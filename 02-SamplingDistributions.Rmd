# Hypothetical repeated sampling and the t-test

This chapter introduces some of the foundational ideas behind hypothesis testing in the frequentist framework. The key idea is that of hypothetical repeated sampling. When we fit a model to a given data-set, we are assuming that this is one of  potentially infinite numbers of exact repetitions of an experiment. We will leverage some amazing  properties of these exact repetitions in order to draw inferences from our particular data. The key idea to understand here is the central limit theorem, which we will discuss below. Before we do that, it is useful to introduce some terminology relating to typical experiment designs in linguistics and psychology.

## Some terminology surrounding typical experiment designs in linguistics and psychology

An experiment typically involves taking a *random sample* from some population. For example, we could take a sample of participants and record their reading times for two types of sentences, for example easy and difficult sentences. How one operationalizes easy and difficult is not important at this point. However, to make the  discussion concrete, consider this well-known example from psycholinguistics. We could be comparing reading times in active vs. passive sentences:

Active sentence: The man threw the ball.

Passive sentence: The ball was thrown by the man.

One claim in the literature is that, due to their simpler structure, active sentences are easier to process than passive sentences. So this is an example of a simpler vs. complex sentence. If we want to investigate whether actives are easier to process than passives, we can, as an example, compare the total reading times for each sentence type (there are some problems that arise here because the sentences don't have the same length, so the comparison is not really fair; these issues we will discuss later). 

Technically, we are supposed to randomly choose participants; in practice, this randomization rarely happens. Instead we take whatever participants we get,  such as university students who happen to apply to participate in an experiment! So, random sampling is an assumption in all the discussions in this chapter, but in practice this assumption may or may not be perfectly met.

One design decision that the researcher must take is whether to collect only one response from each participant in each condition, or whether we collect multiple measures from each participant. If we collect only one data point from each participant, we will say that the data points are *independent* of each other. When we collect multiple measurements from each participant, we will say that we have *dependent* data; such multiple measurements are also called *repeated measures*. With repeated measures, there will be some correlation between the data-points because there are  multiple measurements from a common source.

In psycholinguistics, repeated measures experiments are the norm. Furthermore, it is common to use a so-called *Latin Square* design. The term Latin Square refers to the fact that the ordering of the conditions forms a square (Table \@ref(tab:LatinSquare)); the word Latin apparently only refers to the fact the symbols for each condition were from the Latin script.

```{r LatinSquare,echo=FALSE,results="asis"}
latsq <- data.frame(rbind(letters[1:2], letters[2:1]))
colnames(latsq) <- c("Group 1", "Group 2")

kableExtra::kable(latsq,
  digits = 2, #booktabs = TRUE,
  vline = "", format="latex",
  caption = "The Latin Square with two conditions labeled a and b."
)
```

A characteristic property of the Latin Square is that, as shown in Table \@ref(tab:LatinSquare), each condition appears in each row exactly once. The Latin square generalizes beyond two conditions easily. Suppose we have four conditions; then, the Latin Square is as in Table \@ref(tab:LatinSquare4). Similarly, if there are eight conditions, the Latin Square table would have the form shown in Table \@ref(tab:LatinSquare8). 


```{r LatinSquare4,echo=FALSE,results="asis"}
latsq4 <- data.frame(rbind(
  letters[1:4],
  letters[c(2:4, 1)],
  letters[c(3:4, 1:2)],
  letters[c(4, 1:3)]
))
colnames(latsq4) <- c("Group 1", "Group 2", "Group 3", "Group 4")

kableExtra::kable(latsq4,
  digits = 2, booktabs = TRUE,
  vline = "",  format="latex",
  caption = "A Latin Square design with four conditions labeled a, b, c, and d."
)
```


```{r LatinSquare8,echo=FALSE,results="asis"}
latsq8 <- data.frame(rbind(
  letters[1:8],
  letters[c(2:8, 1)],
  letters[c(3:8, 1:2)],
  letters[c(4:8, 1:3)],
  letters[c(5:8, 1:4)],
  letters[c(6:8, 1:5)],
  letters[c(7:8, 1:6)],
  letters[c(8, 1:7)]
))
colnames(latsq8) <- paste(rep("G", 8), 1:8, sep = "")

kableExtra::kable(latsq8,
  digits = 2, booktabs = TRUE,
  vline = "",  format="latex",
  caption = "A Latin Square design with eight conditions labeled a, b, c, d, e, f, g, and h. The column names representing the eight groups are abbreviated as G1-G8."
)
```

As an aside, it is rarely a good idea to design such complex experiments as the one shown in Table \@ref(tab:LatinSquare8). The more conditions you have in an experiment, the more decisions you will have to make about how to analyze the data, the more complex the model will get, and the harder the interpretation of the data. Later in this course, we will demonstrate the price one has to pay for setting up overly complex designs. This point is mentioned here because it is a common beginner error to want to have as many conditions as possible in a single experiment. In fact, the first experiment a student of ours once proposed to us was a 180 condition design. 

The Latin Square design is attractive for psycholinguistics and psychology because it has some important optimality properties. Experiment design and the properties of different designs is a whole field in itself in statistics, but we won't get into these details.  

The Latin square design can be extended so that each participant sees repeated sets of each condition. For example, in a two-condition design, we may decide to create multiple (say 4) sets of items: item 1 will have two instances of conditions a and b; item 2 to 4 will each have two instances of conditions a and b. The Latin square then looks like the one shown in Table \@ref(tab:LatinSquarerep2).

```{r LatinSquarerep2,echo=FALSE,results="asis"}
latsqrep2 <- data.frame(cbind(item = 1:4, rep(letters[1:2], 2), rep(letters[2:1], 2)))

colnames(latsqrep2) <- c("item", "Group 1", "Group 2")

kableExtra::kable(latsqrep2,
  digits = 2, booktabs = TRUE,
  vline = "",  format="latex",
  caption = "The Latin Square with two conditions labeled a and b, and four items."
)
```

Now, when each incoming participants is randomly assigned to Group 1 or Group 2, they will see two instances of each condition, but---crucially--each instance of each item will be seen only once. This has the advantage that we obtain repeated measures from each participant for each condition, leading to more accurate estimates from each participant for each condition; and we obtain measurements from multiple items as well. However, each participant sees each item only once; this can be important in cases where showing the same item to the participant more than once could bias their response. For example, if we show the same word twice to a participant, they might process the second instance of that word more easily, biasing the average response to that word. There can however be situations where it is appropriate/necessary to show the same item multiple times to a participant. In most of the designs considered in this book, participants will see one item only once.

Having repeated measurements from participants and from items allows us to generalize beyond the specific participants and items that we have in the experiment.

To come closer to a realistic experiment design, suppose that we have two participants (this is just for illustration---normally we will have many more than two participants). Then, the data frame will look like this:

```{r}
condition <- c(rep(letters[1:2], 2), rep(letters[2:1], 2))
item <- rep(1:4, 2)
subj <- rep(1:2, each = 4)
df_example <- data.frame(subj, item, condition)
df_example
```

As a consequence of this design, we say that we have a fully crossed subjects (participants) and items design: each participant sees exactly one item:

```{r}
xtabs(~ subj + item, df_example)
```

You can also check that each subject sees two instances of each condition:

```{r}
xtabs(~ subj + condition, df_example)
```

Notice also that there is exactly one measurement from each item for each condition. This is because only two subjects are involved here. If there were 10 subjects, there would be five instance of each condition for each item.

```{r}
xtabs(~ item + condition, df_example)
```

So, one consequence of the above repeated measurements design is that there will, in general, be repeated measurements not just from subjects but also from items. Thus, items can also be seen as producing dependent data.  Later on, we will see that this design has far-reaching consequences regarding the type of data analysis one can and should do. 

This is a very brief introduction to experiment design; but the reader should keep in mind that these are the kinds of designs we will primarily focus on in this book. 
We now turn to the idea of hypothetical repeated sampling, and the central limit theorem.

## The central limit theorem using simulation

Suppose we collect some data, which can be represented by a vector $y$; this is a *single sample*. Given data $y$, and assuming for concreteness that the underlying likelihood is a $Normal(\mu=500,\sigma=100)$, the sample mean and standard deviation, $\bar{y}$ and $s$  give us an estimate of the unknown parameters mean $\mu$ and the standard deviation $\sigma$ of the distribution from which we assume that our data come from. Figure \@ref(fig:normalsample) shows the distribution of a particular sample, where the number of data points is $n=1000$. Note that in this example the parameters are specified by us, so they are not unknown; in a real data-collection situation, the sample mean and standard deviation are all we have as estimates of the parameters.

(ref:normalsample) A sample of data y of size n=1000, from the distribution  Normal(500,100). The vertical line shows the true mean of the distribution we are sampling from.

```{r normalsample,out.width='75%', fig.cap = "(ref:normalsample)", fig.height = 2}
## sample size:
n<-1000
##  independent and identically distributed sample:
y<-rnorm(n,mean=500,sd=100)
## histogram of data:
hist(y,freq=FALSE)
## true value of mean:
abline(v=500,lwd=2)
```

Suppose now that you had not a single sample of size 1000 but many repeated samples. This isn't something one can normally do in real life; we often run a single experiment or, at most, repeat the same experiment once. However, one can simulate repeated sampling easily within R. Let us take 100 repeated samples like the one above, and save the samples in a matrix containing $n=1000$ rows and 2000 columns, each column representing an experiment:

```{r}
mu <- 500
sigma <- 100
## number of experiments:
k <- 2000
## store for data:
y_matrix <- matrix(rep(NA, n * k), ncol = k)
for (i in 1:k) {
  ## expt result with sample size n:
  y_matrix[, i] <- rnorm(n, mean = mu, sd = sigma)
}
```

Now, if we compute the means $\bar{y}_k$ of *each* of the $k=1,\dots,100$ experiments we just carried out, if certain conditions are met, these means will be normally distributed, with mean $\mu$  and standard deviation $\sigma/\sqrt{n}$. To understand this point, it is useful to first visualize the distribution of means and graphically summarize this standard deviation, which confusingly is called *standard error*.

```{r}
## compute means from each replication:
y_means <- colMeans(y_matrix)
## the mean and sd (=standard error) of the means
mean(y_means)
sd(y_means)
```

```{r sdsmnormal,echo=FALSE,out.width='75%', fig.show = "hold", fig.cap = "Sampling from a normal distribution (left); and the sampling distribution of the means under repeated sampling (right). The right-hand plot shows an overlaid normal distribution, and the standard deviation (standard error) as error bars.", tidy = FALSE}
op<-par(mfrow=c(1,2),pty="s")

for(i in 1:k){
  if(i==1){
plot(density(y_matrix[,i]),main="Samples from a \n Normal distribution",
     xlab="")
  } else {
lines(density(y_matrix[,i]),lty=2)
  }
}

hist(y_means,freq=FALSE,main="Sampling distribution of means",
     xlab="means under repeated sampling",
     ylim=c(0,0.15))
abline(v=mean(y_means),lty=2)
lines(seq(9,11,by=0.01),
      dnorm(seq(9,11,by=0.01),mean(y_means),sd(y_means)))
arrows(x0=mean(y_means)-sd(y_means),
      y0=0.04,
      x1=mean(y_means)+sd(y_means),
      y1=0.04,angle = 90,code=3)
lines(seq(490,510,by=0.01),
      dnorm(seq(490,510,by=0.01),
            mean(y_means),sd(y_means)))
```

The sampling distribution of means has a  normal distribution provided two conditions are met: (a) the sample size should be ``large enough'', and (b)  $\mu$ and $\sigma$ are defined for the probability density or mass function that generated the data. This fact is called the **central limit theorem** (CLT). The significance of the CLT for us as researchers is that from the summary statistics computed from a *single* sample, we can obtain an estimate of this distribution of means: $Normal(\bar{y},s/\sqrt{n})$.  



:::: {.blackbox data-latex=""}

::: {data-latex=""}
**Derivation of the sampling distribution of the mean**
:::

The statement that the sampling distribution of the meas will be normal, with mean $\mu$  and standard deviation $\sigma/\sqrt{n}$, can be derived  formally through a simple application of random variable theory. Suppose that we gather independent and identically distributed data $y_1, \dots, y_n$, each of which is assumed to be generated by a random variable $Y\sim Normal(\mu,\sigma)$.

When we compute the mean $\bar{y}$ for each sample, we are assuming that each of the means is coming from a random variable $\bar{Y}$, which is just a linear combination of values generated by instances of the random variable $Y$, which itself has a pdf with mean (expectation) $\mu$ and variance $\sigma^2$:

\begin{equation}
\bar{Y}=\frac{1}{n} \sum_{i=1}^n Y = \frac{1}{n}Y_1 + \dots + \frac{1}{n}Y_n
 \end{equation}

So, the expectation of $\bar{Y}$ is 

\begin{equation}
\begin{split}
E[\bar{Y}] =& E[\frac{1}{n}Y_1 + \dots + \frac{1}{n}Y_n]\\
=& \frac{1}{n} (E[Y] + \dots + E[Y])\\
=& \frac{1}{n} (\mu + \dots + \mu)\\
=& \frac{1}{n} n\mu \\
=& \mu \\
\end{split}
\end{equation}

And the variance of $\bar{Y}$ is

\begin{equation}
\begin{split}
Var(\bar{Y}) =& Var(\frac{1}{n}Y_1 + \dots + \frac{1}{n}
Y_n)\\
=& \frac{1}{n^2} Var(Y_1 + \dots + Y_n)\\
\end{split}
\end{equation}

The last line above arises because the variance of a random variable $Z$ multiplied by a constant $a$, $Var(aZ)$ is $a^2 Var(Z)$. Here, $a=1/n$, so $a^2 = 1/n^2$. 
Because $Y_1,\dots,Y_n$ are independent, we can compute the variance $Var(Y_1 + \dots + Y_n)$ by using the fact that the variance of the sum of independent random variables is the sum of their variances. This fact gives us:

\begin{equation} \label{sdsmderivation}
\begin{split}
\frac{1}{n^2} Var(Y_1 + \dots + Y_n) =& \frac{1}{n^2} (Var(Y) + \dots + Var(Y))\\
=&  \frac{1}{n^2}  n Var(Y)\\
=&  \frac{1}{n}  Var(Y)\\
=&  \frac{\sigma^2}{n}\\
\end{split}
\end{equation}

This derives the above result that the expectation (i.e., the mean) and variance of the sampling distribution of the sample means are

\begin{equation}
E[\bar{Y}] = \mu \quad Var(\bar{Y}) = \frac{\sigma^2}{n}
\end{equation}
::::

The above means that we can estimate the expectation $\bar{Y}$ and the variance  of $\bar{Y}$ from a *single* sample. (Of course, whether these estimates are accurate or not is subject to variation, as we will see below).

**Central Limit Theorem**

Let $f(Y)$ be the pdf of a random variable $Y$, and assume that the pdf has mean $\mu$ and variance $\sigma^2$. Then, the distribution of the mean $\bar{Y}$ has the following form when the sample size $n$ is large:

\begin{equation}
\bar{Y} \sim Normal(\mu,\sigma^2/n)  
\end{equation}

For us, the practical implication of this result is huge. From a *single* sample of $n$ independent data points $y_1,\dots, y_n$, we can derive the distribution of *hypothetical* sample means under repeated sampling. That is, it becomes possible to say something about what the plausible and implausible values of the sample mean are under repeated sampling. This is the basis for all hypothesis testing and statistical inference in the frequentist framework that we will look at in this book.

Sometimes the central limit theorem is misunderstood to imply that the pdf $f(Y)$ that is assumed to generate the data is always going to be normal. It is important to understand that there are two pdfs we are talking about here. First, there is the pdf that the data were generated from; this need not be normal. For example, you could get data from a Normal, Exponential, Gamma, or other distribution. Second, there is the sampling distribution of the *sample mean* under repeated sampling. It is the sampling distribution that the central limit theorem is about, not the distribution that generated the data.

Technically, the correct statement of the central limit theorem is as in Box 2.2.1.

:::: {.mytheorem data-latex=""}
::: {data-latex=""}
**The central limit theorem**
:::
If $Y_1,\dots, Y_n$ are random samples from a random variable $X$ with mean $\mu$ and standard deviation $\sigma$, then as the sample size $n$ approaches infinity, the distribution of the random variable

\begin{equation}
Z = \frac{\bar{Y}-\mu}{\sigma/\sqrt{n}}
\end{equation}

is the standard normal distribution ($\mathit{normal}(\mu=0,\sigma=1)$). 

For a formal proof, see p. 267 of @millermiller.
::::

The theorem is stated in terms of $Z$ and not $\bar{Y}$ as presented above; this is because the variance of $\bar{Y}$ approaches 0 as n approaches infinity (because $\sigma/\sqrt{n}$ will approach 0). But the way that we present the central limit theorem above is fine if we are talking about large $n$ (and not about $n$ approaching infinity).


## Three examples of the sampling distribution

In the above discussion, the underlying pdf we sampled from above was a normal distribution. However, it need not be. Consider two examples first: the underlying pdf is an Exponential or a Gamma distribution. 

The Exponential distribution has a parameter $\lambda$ (parameterized in `R` as a `rate`, $1/\lambda$); its mean is $\lambda$ and its variance is $1/\lambda^2$. The sampling distribution is normal, even though the underlying distribution is an Exponential; see Figure \@ref(fig:sdsmexp).

```{r sdsmexp,echo=FALSE,out.width='85%', fig.show = "hold", fig.cap = "Sampling from an exponential.", tidy = FALSE}
## Sampling from an Exponential:
## number of experiments:
k<-2000
y_matrix<-matrix(rep(NA,n*k),ncol=k)
for(i in 1:k){
  y_matrix[,i]<-rexp(n,rate=1/10)
}

op<-par(mfrow=c(1,2),pty="s")
for(i in 1:k){
  if(i==1){
plot(density(y_matrix[,i]),main="Samples from an \n  Exponential distribution",
     xlab="",ylim=c(0,0.15))
  } else {
lines(density(y_matrix[,i]),lty=2)
  }
}

## compute means from each replication:
y_means<-colMeans(y_matrix)
hist(y_means,freq=FALSE,main="Sampling distribution of means \n (Data sampled \n from Exponential)",
     xlab="means under repeated sampling",
     ylim=c(0,2))
abline(v=mean(y_means),lty=2)
lines(seq(9,11,by=0.01),
      dnorm(seq(9,11,by=0.01),mean(y_means),sd(y_means)))
```

A further example is samples from a Gamma distribution. The Gamma distribution has two parameters, a and b, and is written Gamma(a,b).
In R, the parameters a and b are called shape and rate, respectively. The mean of the Gamma distribution is $\frac{a}{b}$ and the variance is $\frac{a}{b^2}$. Suppose we sample from a Gamma distribution with shape parameter chosen arbitrarily to be 1. The distribution of means is again going to be approximately normal; see Figure \@ref(fig:sdsmgamma).


```{r sdsmgamma,echo=FALSE,out.width='75%', fig.show = "hold", fig.cap = "Sampling from a Gamma distribution.", tidy = FALSE}
## Sampling from a Gamma:
## number of experiments:
k<-2000
y_matrix<-matrix(rep(NA,n*k),ncol=k)
for(i in 1:k){
  y_matrix[,i]<-rgamma(n,shape=1,rate=1)
}

op<-par(mfrow=c(1,2),pty="s")
for(i in 1:k){
  if(i==1){
plot(density(y_matrix[,i]),main="Samples from a \n Gamma distribution",
     xlab="")
  } else {
lines(density(y_matrix[,i]),lty=2)
  }
}
## compute means from each replication:
y_means<-colMeans(y_matrix)
hist(y_means,freq=FALSE,main="Sampling distribution of means \n (Data sampled from Gamma)",
     xlab="means under repeated sampling")
abline(v=mean(y_means),lty=2)
lines(seq(.9,1.1,by=0.01),
      dnorm(seq(.9,1.1,by=0.01),mean(y_means),sd(y_means)))
```

As a final example, consider what happens if sample from a distribution, the Cauchy, that doesn't have any mean or variance defined for it. 

```{r sdsmcauchy,echo=FALSE,out.width='75%', fig.show = "hold", fig.cap = "Sampling from a Cauchy distribution.", tidy = FALSE}
## Sampling from a Cauchy:
## number of experiments:
k<-2000
y_matrix<-matrix(rep(NA,n*k),ncol=k)
for(i in 1:k){
  y_matrix[,i]<-rcauchy(n)
}

op<-par(mfrow=c(1,2),pty="s")
for(i in 1:k){
  if(i==1){
plot(density(y_matrix[,i]),main="Samples from a \n Cauchy distribution",
     xlab="")
  } else {
lines(density(y_matrix[,i]),lty=2)
  }
}
## compute means from each replication:
y_means<-colMeans(y_matrix)
hist(y_means,freq=FALSE,main="Sampling distribution of means \n (Data sampled from Cauchy)",
     xlab="means under repeated sampling")
abline(v=median(y_means),lty=2)
lines(seq(.9,1.1,by=0.01),
      dnorm(seq(.9,1.1,by=0.01),mean(y_means),sd(y_means)))
```

As Figure \@ref(fig:sdsmcauchy) illustrates, when the mean and variance for the likelihood are undefined, the central limit theorem doesn't hold. In the rest of this book, we will always assume that the data are coming from a distribution that has a mean and variance defined for it.

## The confidence interval, and what it's good for

Once we have sample of data $y$, and once the sample mean $\bar{y}$ and the $SE = s/\sqrt{n}$ have been computed, it is common to define a so-called 95% confidence interval:

\begin{equation}
\bar{y} \pm 2 SE
\end{equation}

Because the sampling distribution of means is normally distributed, and because 95% of the area under the curve is covered by two times the standard deviation of the normal distribution,  
the upper and lower bounds of the interval defined by the interval $\bar{y} \pm 2 SE$ covers approximately 95\% of the area under the curve in the sampling distribution. 

This interval is usually computed after estimating the sample mean and  standard error from a data-set, and is called the *confidence interval* (CI). It has the following meaning: If you take samples repeatedly and compute the CI each time, 95\% of those CIs will contain the true population mean $\mu$. To understand this point, one can simulate this situation. This time we will do 1000 repeated experiments instead of 100. 

```{r}
mu <- 500
sigma <- 100
n <- 1000
nsim <- 1000
lower <- rep(NA, nsim)
upper <- rep(NA, nsim)
for (i in 1:nsim) {
  y <- rnorm(n, mean = mu, sd = sigma)
  lower[i] <- mean(y) - 2 * sd(y) / sqrt(n)
  upper[i] <- mean(y) + 2 * sd(y) / sqrt(n)
}

## check how many CIs contain mu:
CIs <- ifelse(lower < mu & upper > mu, 1, 0)
## approx. 95% of the CIs contain true mean:
round(mean(CIs), 2)
```

Figure \@ref(fig:ciplot) visualizes the coverage properties of the confidence interval in 100 simulations; by coverage we mean here the proportion of cases where the true $\mu$ is contained in the CI.

```{r ciplot,echo=FALSE,out.width='75%', fig.show = "hold", fig.cap = "Illustration of the meaning of a  95 percent confidence interval (CI). The thicker bars represent the CIs which do not contain the true mean.", tidy = FALSE}
## define function for SE
se <- function(x)
      {
        y <- x[!is.na(x)] # remove the missing values, if any
        sqrt(var(as.vector(y))/length(y))
}
## define function for CI:
ci <- function (scores){
m <- mean(scores,na.rm=TRUE)
stderr <- se(scores)
len <- length(scores)
upper <- m + 2 * stderr 
lower <- m - 2 * stderr 
return(data.frame(lower=lower,upper=upper))
}

nsim<-100
## sample size:
n<-1000
lower <- rep(NA,nsim)
upper <- rep(NA,nsim)

store <- rep(NA,nsim)

for(i in 1:nsim){ 
  y <- rnorm(n,mean=mu,sd=sigma)
  lower[i] <- ci(y)$lower
  upper[i] <- ci(y)$upper
  if(lower[i]<mu & upper[i]>mu){
    store[i] <- TRUE} else {
      store[i] <- FALSE}
}

## need this for the plot below:
cis <- cbind(lower,upper)

main.title<-"95% CIs in 100 repeated samples"

line.width<-ifelse(store==FALSE,2,1)
cis<-cbind(cis,line.width)
y<-seq(450,550,by=1)
x<-1:length(y)
plot(x,y,type="n",xlab="i-th repeated sample",
     ylab="y",main=main.title)
abline(500,0,lwd=2)
x0<-x
x1<-x
arrows(x0,y0=cis[,1],
       x1,y1=cis[,2],length=0,lwd=cis[,3])
```

### Confidence interals are often misinterpreted

The confidence interval is widely misinterpreted, i.e., as representing the range of plausible value of the $\mu$ parameter. This is the wrong interpretation because $\mu$ is a point value by assumption, it doesn't have a pdf associated with it. The frequentist CI is defined with reference to the sampling distribution of the mean under repeated sampling, not the probability distribution of $\mu$.
By contrast, the Bayesian credible interval does have this interpretation. 
In most modeling settings that the authors have encountered in their work, the frequentist confidence interval and Bayesian credible interval have very similar widths, with the Bayesian interval being slightly wider depending on the prior specifications. But these similarities in the intervals do not change the fact that they have different meanings.

Given the convoluted meaning of the CI, and the impossibility of interpreting a single CI, it is reasonable to ask: what good is a CI? One can treat the CI as a summary that tells us the width of the sampling distribution of the mean---the wider the sampling distribution, the more the implied variability under repeated sampling. The confidence interval can therefore be used to assess how uncertain we can be about the estimate of the sample mean under hypothetical repeated sampling. See @cumming2014new for a useful perspective relating to using confidence intervals for inference. As discussed later in the book, we will use the CI to informally assess uncertainty.

We turn next to the central ideas behind the hypothesis test. We begin with the humble one-sample t-test, which contains many subtleties and is well worth close study before we move on to the main topic of this book: linear mixed models.

## Hypothesis testing: The one sample t-test

With the central limit theorem and the idea of hypothetical repeated sampling behind us, we turn  now to one of the simplest statistical tests that one  can do with continuous data: the t-test. 

Due to its simplicity, it is tempting to take only a cursory look at the t-test and move on immediately to the linear (mixed) model. This would be a mistake. The humble t-test is surprising in many ways, and holds several important lessons for us. There  are subtleties in this test, and a close connection to the linear mixed model. For these reasons, it is worth slowing down and spending some time understanding this test. Once the t-test is clear, more complex statistical tests will be easier to follow, because the logic of these more complex tests will essentially be more of the same, or variations on this general theme. You will see later that t-test can be seen as an analysis of variance or ANOVA; and the paired t-test is exactly the linear mixed model with varying intercepts.

### The one-sample t-test

As in our running example, suppose we have a random sample $y$ of size $n$, and the data come from a $N(\mu,\sigma)$ distribution, with unknown parameters $\mu$ and $\sigma$. An assumption of the t-test is that the data points are independent in the sense discussed at the beginning of this chapter.
We can estimate $\mu$ from the sample mean $\bar{y}$, which we will sometimes also write as $\hat \mu$. We can also estimate $\sigma$ from the sample standard deviation $s$, which we can also write as  $\hat\sigma$. These estimates in turn  allow us to estimate the sampling distribution of the mean under (hypothetical) repeated sampling:

\begin{equation}
N(\hat\mu,\frac{\hat \sigma}{\sqrt{n}})
\end{equation}

It is important to realize here that the above sampling distribution is only as realistic as the estimates of the mean and standard deviation parameters---if those happen to be inaccurately estimated, then the sampling distribution is not realistic either.

Assume as before that we take an independent random sample of size $1000$ from a random variable $Y$ that is normally distributed, with mean 500 and standard deviation 100. As usual, begin by estimating the mean and SE:

```{r}
n <- 1000
mu <- 500
sigma <- 100
## generate simulated data:
y <- rnorm(n, mean = 500, sd = 100)
## compute summary statistics:
y_bar <- mean(y)
SE <- sd(y) / sqrt(n)
```

The null hypothesis significance testing (NHST) approach as practised in psychology and other areas is to set up a null hypothesis that $\mu$ has some fixed value. Just as an  example, assume that our null hypothesis is:

\begin{equation}
H_0: \mu = 450
\end{equation}

This amounts to assuming that the true sampling distribution of sample means is (approximately) normally distributed and centered around 450, with the standard error estimated from the data.


```{r echo=FALSE,fig.cap="The sampling distribution of the mean when the null hypothesis is that the mean is 450. Also shown is the observed sample mean."}
x <- seq(400, 525, by = 0.1)
plot(x, dnorm(x, mean = 450, sd = SE),
  type = "l",
  main = "The sampling distribution with mu=450",
  ylab = "density",
  yaxs = "i",
  ylim = c(0, .15)
)
points(y_bar, 0.001, col = "black", pch = 20)
text(x = y_bar, y = 0.01, label = "sample \n mean", col = "black")
```

The intuitive idea here is that 

- if the sample mean $\bar{y}$ is "near" the hypothesized $\mu$ (here, 450), the data are possibly (but by no means necessarily) consistent with the null hypothesis distribution.
- if the sample mean $\bar{y}$ is "far" from the hypothesized $\mu$, the data are inconsistent with the null hypothesis distribution.  

The terms "near" and "far" will be quantified by determining how many standard error units the sample mean is from the hypothesized mean. This way of thinking shifts the focus away from the sampling distribution above, towards the distance measured in standard error units from the null hypothesis mean.

The distance between the sample mean and the hypothesized mean can be written in SE units as follows. We will say that the sample mean is $t$ standard errors away from the hypothesized mean:

\begin{equation}
t \times SE = \bar{x} - \mu 
\end{equation}

If we divide both sides with the standard error, we obtain the so-called observed t-statistic:

\begin{equation}
t  = \frac{\bar{x} - \mu}{SE}
\end{equation}

This observed t-value, an expression of the distance between  the sample mean and  the hypothesized mean, becomes the basis for the statistical test. 

Notice that the t-value is a random variable: it is a transformation of $\bar{X}$, the random variable generating the sample means. The t-value can therefore be seen as  an instance of the following transformed random variable $T$:

\begin{equation}
T  = \frac{\bar{X} - \mu}{SE}
\end{equation}

This random variable has a pdf associated with it, the  t-distribution, which is defined in terms of the sample size $n$; the pdf is written $t(n-1)$. Under repeated sampling, the t-distribution is generated from this random variable $T$. 

Figure \@ref(fig:tdistrn) quickly  visualizes this: sample 10000 times from a $Y\sim Normal(\mu=450,\sigma=100)$. Assume that the null hypothesis is $\mu_0=450$; that is, the null hypothesis is in fact true in this case. The sample size is $n=5$. Then compute the t statistic each time, and plot the distribution of the t-values as a histogram. Plot a $t(n-1)$ distribution on top of this histogram to compare to the two distributions. A Normal(0,1) distribution is plotted as a broken line for comparison with the $t(df=4)$ distribution.

```{r tdistrn,fig.cap="The distribution of the t-value under repeated sampling assuming that the null hypothesis is true, compared with a t(n-1) distribution (solid line) and a Normal(0,1) distribution (broken line)."}
set.seed(4321)
nsim <- 10000
n <- 5
mu <- 450
## null hypothesis mean:
mu0 <- 450
sigma <- 100
tval <- rep(NA, nsim)
se <- sigma / sqrt(n)
for (i in 1:nsim) {
  y <- rnorm(n, mean = mu, sd = sigma)
  xbar <- mean(y)
  tval[i] <- (xbar - mu0) / se
}

hist(tval, freq = FALSE, main = "t-value distribution")
x <- seq(-4, 4, by = 0.001)
lines(x, dt(x, df = n - 1))
lines(x, dnorm(x), lty = 2)
```

We will compactly express the statement that "the observed t-value is assumed to be generated (under repeated sampling) from a t-distribution with  n-1 degrees of freedom" as:  

\begin{equation}
T \sim t(n-1)
\end{equation}

For large $n$, the pdf of the random variable $T$ approaches $N(0,1)$. This is illustrated in Figure  \@ref(fig:tnormal); notice that the t-distribution has fatter tails than the normal for small $n$, say $n<20$, but for larger n, the t-distribution and the normal become increasingly similar in shape. Incidentally,  when n=2, the t-distribution $t(1)$ is the Cauchy distribution we saw earlier; this distribution is characterized by fat tails, and has no mean or variance defined for it.


```{r tnormal,echo=FALSE,fig.cap="A visual comparison of the t-distribution (with degrees of freedom ranging from 1 to 50) with the standard normal distribution (N(0,1)). The dashed line represents the standard normal distribution, and the solid line the t-distribution with the relevant degrees of freedom."}
range <- seq(-4, 4, .01)

op <- par(mfrow = c(2, 3), mar = c(2, 2, 3, 2), pty = "s")


for (i in c(1, 2, 5, 15, 20, 50)) {
  plot(range, dnorm(range),
    type = "l", lty = 2,
    xlab = "", ylab = "",
    cex.axis = 1, cex.axis = 0.8,
    yaxs = "i"
  )
  lines(range, dt(range, df = i), lty = 1, lwd = 1)
  mtext(paste("df=", i), cex = 1.2)
}
```

Thus, given a sample size $n$, we can define a t-distribution corresponding to the null hypothesis distribution. For large values of $n$, we could even use $N(0,1)$, although it is traditional in psychology and linguistics to always use the t-distribution no matter how large $n$ is.

The null hypothesis testing procedure proceeds as follows:

- Define the null hypothesis: in our example, the null  hypothesis was that $\mu = 450$.  This amounts to making a commitment about what fixed value we think the true underlying distribution of sample means is centered at. 
- Given data of size $n$, estimate $\bar{y}$, standard deviation $s$, and from that, estimate the standard error $s/\sqrt{n}$.  The standard error will be used  to describe the sampling distribution's standard  deviation.
- Compute the observed t-value:

\begin{equation}
t=\frac{\bar{y}-\mu}{s/\sqrt{n}}
\end{equation}

- Reject null hypothesis if the observed t-value is "large" (to be made more precise next).
- Fail to reject the null hypothesis, or (under some conditions, to be made clear later) even go so far as to accept the null hypothesis, if the observed t-value is "small".

What constitutes a large or small observed t-value?
Intuitively, the t-value from the sample is large when we end up far in *either* tail of the distribution. The two tails of the t-distribution will be referred to as the *rejection region*. The word *region* here refers to the real number line along the x-axis, under the tails of the distribution. The rejection region will go off to infinity on the outer sides, and is demarcated by a vertical line on the inner side of each tail. This is shown in Figure \@ref(fig:tails). It goes off to infinity because the support---the range of possible values---of the random variable that the t-distribution belongs to stretches from minus infinity to plus infinity. 

```{r tails,echo=FALSE,fig.cap="The rejection region in a t-distribution (sample size 20) assuming that the null hypothesis is true. The rejection region is the x-axis under the gray-colored area."}
n <- 20
x <- seq(-5, 5, by = 0.1)
plot(x, dt(x, df = n - 1),
  type = "l", main = "t(n-1)",
  ylab = "density",
  yaxs = "i",
  xlab = "t-values"
)

lower <- qt(0.025, df = n - 1)
upper <- qt(0.975, df = n - 1)
abline(v = lower)
abline(v = upper)

x1 <- seq(upper, 20, 0.001)
y1 <- dt(x1, df = n - 1)
polygon(c(x1, rev(x1)),
  c(rep(0, length(x1)), rev(y1)),
  col = gray(0.8)
)

x1 <- seq(-20, lower, 0.001)
y1 <- dt(x1, df = n - 1)
polygon(c(x1, rev(x1)),
  c(rep(0, length(x1)), rev(y1)),
  col = gray(0.8)
)

points(1.8, 0.005, col = "black", pch = 20)
text(x = 1.8, y = 0.04, label = "observed t-value", col = "black")
text(0, .2, "fail-to-reject \n region")
text(-3.5, .2, "rejection \n region")
text(3.5, .2, "rejection \n region")
```

The location of the vertical lines is determined by the so-called *absolute critical t-value* along the x-axis of the t-distribution.  This is the value such that the area under the curve in the tails to the left or right of the tails is 0.025. As discussed in chapter 1, this area under the curve represents the probability of observing a value as extreme as the critical t-value, or some value that is more extreme. Notice that if we ask ourselves what the probability is of observing some particular t-value (a point value), the answer must necessarily be $0$ (if you are unclear about why, re-read chapter 1). But we can ask the question: what is the absolute t-value, written $|t|$, such that $P(T>|t|)=0.05$?  That's the critical t-value. We call it the critical t-value because it demarcates the rejection region shown in Figure \@ref(fig:tails): we are adopting the convention that any observed t-value larger than this critical t-value allows us to reject the null hypothesis.

For a given sample size $n$, we can identify the rejection region by using the `qt` function, whose usage is analogous to the `qnorm` function discussed in chapter 1.

Because the shape of the t-distribution depends on the degrees of freedom (n-1), the absolute critical t-value beyond which we reject the null hypothesis will change depending on sample size. For large sample sizes, say $n>50$, the rejection point is about 2.

```{r}
abs(qt(0.025, df = 15))
abs(qt(0.025, df = 50))
```
Consider the observed t-value from our sample in our running example:

```{r}
## null hypothesis mean:
mu <- 450
(t_value <- (y_bar - mu) / SE)
```

This observed t-value is huge and is  telling you the distance of the sample mean from the null hypothesis mean $\mu$ in standard error units.

\begin{equation}
t=\frac{\bar{y}-\mu_0}{s/\sqrt{n}} \hbox{ or } t\frac{s}{\sqrt{n}}=\bar{y}-\mu_0
\end{equation}

For large sample sizes, if the absolute t-value $|t|$ is greater than  $2$, we will reject the null hypothesis. 

For a smaller sample size $n$, you can compute the exact critical t-value:

```{r}
qt(0.025, df = n - 1)
```

Why is this critical t-value negative in sign? That is because it is on the left-hand side of the t-distribution, which is symmetric.
The corresponding value on the right-hand side is:

```{r}
qt(0.975, df = n - 1)
```

These values are of course identical if we ignore the sign. This is why we always frame our discussion around the absolute t-value.

In R, the  built-in function `t.test` delivers the observed t-value. Given our running example, with the null hypothesis $\mu=450$, R returns the following:

```{r}
## observed t-value from t-test function:
t.test(y, mu = 450)$statistic
```

The default value for the null hypothesis mean $\mu$ in this function is 0; so if one doesn't define a null hypothesis mean, the statistical test is done with  reference to a null hypothesis that $\mu=0$. That is why this t-value does not match our calculation above: 

```{r}
t.test(y)$statistic
```

In the most common usage of the t-test, the null hypothesis mean will be $0$, because usually one is comparing a difference in means between two conditions or two sets of conditions. So the above line of code will work out correctly in those cases; but if you ever have a different null hypothesis mean than $0$, then you   have to specify it in  the `t.test` function.

So, the way that the t-test is used in psychology and related areas is to implement a *decision*: either reject the null hypothesis or fail to reject it. Whenever we do an experiment and carry out a t-test, we use the  t-test to make this binary decision: reject or fail to reject the null hypothesis. 

Recall that behind the t-test lies the  assumption  that the observed t-value  is coming from a random variable, $T\sim t(n-1)$. The particular t-value we observe from a particular data-set belongs to a distribution of t-values under hypothetical repeated sampling. Thus, implicit in the logic of the t-test---and indeed every frequentist statistical test---is the assumption that the experiment is in principle repeatable: the experiment can in principle be re-run as many times as we want, assuming we have the necessary resources and time. 

A quick simulation of t-values under repeated sampling makes this clear. Suppose that our null hypothesis mean is $450$, and our sample size  $n=100$. Assume that the data come from $Normal(\mu=450,\sigma=100)$. Thus, in this case the null hypothesis is in fact true. Let's do $10000$ simulations, compute the sample mean each time, and then store the observed t-value. The t-distribution that results is shown in Figure \@ref(fig:simt).

```{r simt,fig.cap="The distribution of t-values under repeated sampling. The null hypothesis is true."}
n <- 100
nsim <- 10000
tvals <- rep(NA, nsim)
for (i in 1:nsim) {
  y <- rnorm(n, mean = 450, sd = 100)
  SE <- sd(y) / sqrt(n)
  tvals[i] <- (mean(y) - 450) / SE
}
plot(density(tvals),
  main = "Simulated t-distribution",
  xlab = "t-values under repeated sampling"
)
```

What would the t-distribution look like if the null hypothesis were false? Assume now that the null hypothesis is that $\mu=450$ as before, but that in fact the true $\mu$ is 470. Now the null hypothesis is false. Figure \@ref(fig:simtnullfalse) shows the t-distribution under repeated sampling. The t-distribution is now centered around 2; why? This is because if we plug in the hypothesized mean (450) and the true mean (470) and the standard error ($100/\sqrt(100)=10$) into the equation for computing the t-value, the expected value of the t-distribution (its mean) is $2$.

\begin{equation}
\frac{470-450}{10} = 2
\end{equation}

```{r simtnullfalse,fig.cap="The distribution of t-values under repeated sampling. The null hypothesis is false, with the true mean being 470 (the null hypothesis is that the true mean is 450)."}
n <- 100
nsim <- 10000
tvals <- rep(NA, nsim)
for (i in 1:nsim) {
  y <- rnorm(n, mean = 470, sd = 100)
  SE <- sd(y) / sqrt(n)
  tvals[i] <- (mean(y) - 450) / SE
}
plot(density(tvals),
  main = "Simulated t-distribution",
  xlab = "t-values under repeated sampling"
)
```

This implicit idea of the experiment's repeatability leads to an important aspect of the t-test: once certain assumptions about the null hypothesis and the alternative hypothesis are fixed, we can using simulation to compute the proportion of times that the null hypothesis would be rejected under repeated sampling. One has to consider two alternative possible scenarios: the null is either true, or it is  false. In other words, this simulation-based approach allows us to study the t-test's ability (at least in theory) to lead the researchers to the correct decision under (hypothetical) repeated sampling. We turn to this issue next.

### Type I, II error, and power

When we do a hypothesis test using the t-test, the observed t-value will either fall in the rejection region, leading  us to reject the null hypothesis,
or it will land in the non-rejection region, leading us to fail to reject the null. For a particular experiment, that is a single, one-time event. 

So suppose  we have made our decision based on the observed t-value. Now, the null hypothesis can be either true or not true;  we don't know which of those two possibilities is the reality.
When we decide (based on  the observed t-value) that the  null  is true, we are asserting  that the parameter $\mu$ actually does have the  hypothesized value $\mu_0$; when we decide that the null is false, we are asserting that the parameter $\mu$ has some *specific* value $\mu_{alt}$ other than $\mu_0$.
We can represent these two alternative possible realities in a tabular form, as shown in Table \@ref(tab:type12). The two columns show the two possible worlds, one in which the null is true, and the other in  which it is false. The two rows show the two possible decisions we can take based on the observed t-value: reject the null or fail to reject it.

\begin{table}
\begin{tabular}{ccc}
        & \textbf{Possible world 1}      & \textbf{Possible world 2} \\  
\hline
 & $H_0$ TRUE: $\mu=\mu_0$  & $H_0$ FALSE $\mu=\mu_{alt}$ \\
\hline
Decision: `reject': & $\alpha$ & $1~-~\beta$ \\
                                     & Type I error                         & Power \\                                      
                                     & & \\
\hline
Decision: `fail to reject': & $1 - \alpha$ & $\beta$ \\                                    &                                 & Type II error\\
\hline
\end{tabular}
\caption{The possible realities (null is true or null is false) and the possible decisions (accept or reject null) we can take based on our observed t-value.} \label{tab:type12}
\end{table}

As the table shows, we can make two kinds of mistakes:

- Type I error or $\alpha$: Reject the null when it's true.
- Type II error or $\beta$: Accept the null when it's false.

In psychology and related areas, Type I error is usually fixed a priori at 0.05. This stipulated Type I error value is why the absolute critical t-value is kept at approximately $2$; if, following recommendations  from @benjamin2018redefine, we were to stipulate that the Type I  error be 0.005, then the critical t-value would have had to be set at:

```{r}
abs(qt(0.0025, df = n - 1))
```

This suggested change in convention hasn't been taken up yet in cognitive science, but this could well change one day.

Type II error, the probability of incorrectly accepting the null hypothesis when it is false with some particular value for the parameter $\mu$, is conventionally recommended [@powerbookcohen] to be kept at 0.20 or lower. This implies that the probability of correctly  rejecting a null hypothesis for some particular true value of $\mu$ is 1-Type II error. This probability, called statistical power, or just power, should then obviously be larger than 0.80. Again, there is nothing special about these stipulations; they are conventions that became the norm over time. 

Next, we will consider the trade-off between Type I and II error. For simplicity, assume that the standard error is 1, and the null hypothesis is that $\mu=0$. This means that the observed t-value is really the sample mean.

Consider the concrete situation where, in reality, the true value of $\mu$ is $2$. Let's assume for simplicity that the standard error is 1; this means that the true $\mu$ is also the t-value. This allows us to talk about the hypothesis test in terms of the t-distribution.

As mentioned above, the null hypothesis $H_0$ is that $\mu=0$. Now the $H_0$ is false because $\mu=2$ and not $0$. Type I  and II  error can be visualized graphically as shown in  Figure \@ref(fig:type12). 

```{r type12,echo=FALSE,fig.cap="A visualization of Type I  and II error. The dark-shaded tails of the left-hand side distribution represent Type I error; and  the light-colored shaded region of the right-hand side distribution represents Type II error. Power is the unshaded area under the curve in  the right-hand side distribution."}
## function for plotting the area under the curve:
plot.prob <- function(x,
                      x.min,
                      x.max,
                      prob,
                      mean,
                      sd,
                      gray.level, main) {
  plot(x, dnorm(x, mean, sd),
    type = "l", xlab = "",
    ylab = "", main = main
  )
  abline(h = 0)

  ## shade X<x
  x1 <- seq(x.min, qnorm(prob), abs(prob) / 5)
  y1 <- dnorm(x1, mean, sd)

  polygon(c(x1, rev(x1)),
    c(rep(0, length(x1)), rev(y1)),
    col = gray.level
  )
}

shadenormal <-
  function(prob = 0.5,
           gray1 = gray(0.3),
           x.min = -6,
           x.max = abs(x.min),
           x = seq(x.min, x.max, 0.01),
           mean = 0,
           sd = 1, main = "P(X<0)") {
    plot.prob(
      x = x, x.min = x.min, x.max = x.max,
      prob = prob,
      mean = mean, sd = sd,
      gray.level = gray1, main = main
    )
  }

shadenormal(prob = 0.025, main = "Type I, II error")

x1 <- seq(qnorm(0.975), 6, abs(0.975) / 5)
y1 <- dnorm(x1)

polygon(c(x1, rev(x1)),
  c(rep(0, length(x1)), rev(y1)),
  col = gray(0.3)
)

x <- seq(-6, 6, by = 0.1)
lines(x, dnorm(x, mean = 2), col = "black", lwd = 2)
abline(v = 2)
abline(v = -2)

x1 <- seq(-2, 2, 0.01)
y1 <- dnorm(x1, mean = 2)

polygon(c(x1, rev(x1)),
  c(rep(0, length(x1)), rev(y1)),
  col = gray(0.8)
)
```

To understand Figure \@ref(fig:type12), one has to consider two distributions side by side. First, consider the null hypothesis distribution, centered at 0. Under the null hypothesis distribution, the rejection region lies below the dark colored tails of the distributions. The area under the curve in these dark-colored tails is the Type I error (conventionally set at 0.05) that we decide on even before we conduct the experiment and collect the data. Because the Type I error is set at 0.05, and because the t-distribution is symmetric, the area under the curve in each tail is 0.025. The absolute critical t-value helps us demarcate the boundaries of the rejection regions through the vertical lines shown in the figure. These vertical lines play a crucial role in helping us understand Type II error and power. This becomes clear when we consider the distribution representing the alternative possible value of $\mu$, the distribution centered around 2. In this second distribution,  consider now the area under the curve  between the vertical lines demarcating the rejection region  under the  null. This area under the curve is the probability of accepting the null hypothesis when  the null hypothesis is false with some specific value (here, when $\mu$ has value 2). 

Some interesting observations follow. Suppose that the true mean is in fact $\mu=2$, as in the above illustration. Then,

- Simply decreasing Type I error to a smaller value like 0.005 will increase Type II error, which means that power (1-Type II error) will fall.
- Increasing sample size will squeeze the vertical lines closer to  each other because standard error will go down with increasing sample size. This will reduce Type II error, and therefore increase power. Decreasing sample size will have the opposite effect.
- If we design an experiment with a larger effect size, e.g., by setting up a stronger manipulation (concete examples will be discussed in this book later on), our Type II error will go down, and therefore power will go up. Figure \@ref(fig:highpower) shows a graphical visualization of a situation where the true mean is $\mu=4$. Here, Type II error is much smaller compared to Figure \@ref(fig:type12), where $\mu=2$.


```{r highpower,echo=FALSE,fig.cap="The change in Type II error if the true mean is 4."}
## function for plotting the area under the curve:
plot.prob <- function(x,
                      x.min,
                      x.max,
                      prob,
                      mean,
                      sd,
                      gray.level, main) {
  plot(x, dnorm(x, mean, sd),
    type = "l", xlab = "",
    ylab = "", main = main
  )
  abline(h = 0)

  ## shade X<x
  x1 <- seq(x.min, qnorm(prob), abs(prob) / 5)
  y1 <- dnorm(x1, mean, sd)

  polygon(c(x1, rev(x1)),
    c(rep(0, length(x1)), rev(y1)),
    col = gray.level
  )
}

shadenormal <-
  function(prob = 0.5,
           gray1 = gray(0.3),
           x.min = -6,
           x.max = abs(x.min),
           x = seq(x.min, x.max, 0.01),
           mean = 0,
           sd = 1, main = "P(X<0)") {
    plot.prob(
      x = x, x.min = x.min, x.max = x.max,
      prob = prob,
      mean = mean, sd = sd,
      gray.level = gray1, main = main
    )
  }

shadenormal(prob = 0.025, main = "Type I, II error")

x1 <- seq(qnorm(0.975), 6, abs(0.975) / 5)
y1 <- dnorm(x1)

polygon(c(x1, rev(x1)),
  c(rep(0, length(x1)), rev(y1)),
  col = gray(0.3)
)

x <- seq(-6, 6, by = 0.1)
lines(x, dnorm(x, mean = 4), col = "black", lwd = 2)
abline(v = 4)
abline(v = 2)
abline(v = -2)

x1 <- seq(-2, 2, 0.01)
y1 <- dnorm(x1, mean = 4)

polygon(c(x1, rev(x1)),
  c(rep(0, length(x1)), rev(y1)),
  col = gray(0.8)
)
```

In summary, when we plan out an experiment, we are also required to specify the Type I and II error associated with the design. Both sources of error are within our control, at least to some extent.  The Type I error we decide to use will determine our critical t-value  and therefore our decision criterion for rejecting, failing to reject, or even (under certain conditions, to be discussed below) accepting the null hypothesis.  

The Type II error we decide on will determine the long-run probability of incorrectly ``accepting'' the null hypothesis; its inverse (1-Type II error), statistical power, will determine the long-run probability of correctly rejecting the null hypothesis under the assumption that the $\mu$ has some particular value other than the null hypothesis mean.

That's the theory anyway. In practice, researchers only rarely consider the power properties of their experiment design; the focus is almost exclusively on Type I error. The neglect of power in experiment design has had interesting consequences for theory development, as we will see later in this book. For a case study in psycholinguistics, see @VasishthMertzenJaegerGelman2018.

### How to compute power for the one-sample t-test

Power (which is 1-Type II error) is a function of three variables:

- the effect size
- the standard deviation
- the sample size.

There are two ways that one can compute power in connection with the t-test: either one can use the built-in R function, `power.t.test`, or one can use simulation. 


#### Power calculation using the `power.t.test` function

Suppose that we have an expectation that an effect size is 15 ms $\pm 5$ ms (this could be based on the predictions of a theoretical model, or prior data); suppose also that prior experiments show standard deviations ranging from 100 to 300 ms. Given a particular sample size, this is enough information to compute a power curve as a function of effect size and standard deviation. See Figure \@ref(fig:powercurve) and the associated code below.

```{r powercurve,fig.cap="An illustration of a power curve for 10 participants, as a function of standard deviation, and three estimates of the effect: 15, 10, and 20."}
sds <- seq(100, 300, by = 1)
lower <- power.t.test(
  delta = 15 - 5,
  sd = sds, n = 10,
  strict = TRUE
)$power
upper <- power.t.test(
  delta = 15 + 5,
  sd = sds, n = 10,
  strict = TRUE
)$power
meanval <- power.t.test(
  delta = 15,
  sd = sds, n = 10,
  strict = TRUE
)$power

plot(sds, meanval,
  type = "l",
  main = "Power curve (n=10)\n
     using power.t.test",
  xlab = "standard deviation",
  ylab = "power",
  yaxs = "i",
  ylim = c(0.05, 0.12)
)
lines(sds, lower, lty = 2)
lines(sds, upper, lty = 2)
text(125, 0.053, "10")
text(150, 0.054, "15")
text(175, 0.056, "20")
```

#### Power calculations using simulation

An analogous calculation as the one shown above using the  `power.t.test` function can also be done using simulated data.
First, generate simulated data repeatedly for each possible combination of parameter values (here, effect size and standard deviation), and compute the proportion of significant effects for each parameter combination. This can be done by defining a function that takes as input the number of simulations, sample size, effect size, and standard deviation:

```{r}
compute_power <- function(nsim = 100000, n = 10,
                          effect = NULL,
                          stddev = NULL) {
  crit_t <- abs(qt(0.025, df = n - 1))
  temp_power <- rep(NA, nsim)
  for (i in 1:nsim) {
    y <- rnorm(n, mean = effect, sd = stddev)
    temp_power[i] <- ifelse(abs(t.test(y)$statistic)
    > crit_t, 1, 0)
  }
  ## return power calculation:
  mean(temp_power)
}
```

Then, plot the power curves as a function of effect size and standard deviation, exactly as in Figure \@ref(fig:powercurve). Power calculations using simulations are shown in Figure \@ref(fig:powercurve2). It is clear that simulation-based power estimation is going to be noisy; this is because each  time we are generating simulated data and then carrying out a statistical  test on it. This is no longer a closed-form mathematical calculation as done in `power.t.test` (this function simply implements a formula for power calculation specified for this simple case). Because the power estimates will be noisy, we show a smoothed lowess line for each effect size estimate. 

```{r powercurve2,cache=TRUE,echo=FALSE,fig.cap="An illustration of a power curve using simulation, for 10 participants, as a function of standard deviation, and three estimates of the effect: 15, 10, and 20. The power curves are lowess-smoothed."}
sds <- seq(100, 300, by = 50)
nsim <- 100000
n <- 10
power_meanval <- power_lowerval <- power_upperval <- rep(NA, length(sds))
for (s in 1:length(sds)) {
  power_meanval[s] <- compute_power(
    nsim = nsim, n = n,
    effect = 15,
    stddev = sds[s]
  )

  power_lowerval[s] <- compute_power(
    nsim = nsim, n = n,
    effect = 10,
    stddev = sds[s]
  )

  power_upperval[s] <- compute_power(
    nsim = nsim, n = n,
    effect = 20,
    stddev = sds[s]
  )
}
plot(sds, lowess(power_upperval)$y,
  type = "l", lty = 2,
  xlab = "standard deviation",
  ylab = "power",
  main = "Power curve (n=10) \n using simulation",
  ylim = c(0.05, 0.12),
  yaxs = "i"
)
lines(sds, lowess(power_lowerval)$y, lty = 2)
lines(sds, lowess(power_meanval)$y, lty = 1)
```

In the above example, simulation-based power calculation is overkill, and completely unnecessary because we have `power.t.test`. However, the technique shown above will be extended and will become our bread-and-butter method once we switch to power calculations for complicated linear mixed models. There, no closed form calculation can be done to compute power, at least not without oversimplifying the model; simulation will be the only practical way to calculate power.

It is important to appreciate the fact that power is a *function*; it isn't a single number. Because we can never be sure what the true effect size is, or what the true standard deviation is, power functions (power as a function of plausible values for the relevant parameters) are much more useful than single numbers.  

### The p-value

Continuing with our t-test example, the `t.test` function in R will not only print out a t-value as shown above, but also a probability known as a *p-value*. This is the probability of obtaining the observed t-value that we obtained, or some value more extreme than that, conditional on the assumption that the null hypothesis is true. 

We can compute the p-value "by hand". This can be computed, as done earlier, simply by calculating the area under the curve that lies beyond the absolute observed t-value on either side of the t-distribution. It is standard practice to take the tail probability on both sides of the t-distribution. 

```{r}
(abs_t_value <- abs(t.test(y, mu = 450)$statistic))

2 * pt(abs_t_value, df = n - 1, lower.tail = FALSE)
```

The area from both sides of the tail is taken because it is conventional to do a so-called *two-sided t-test*: our null hypothesis is that $\mu=450$, and our alternative hypothesis is two-sided: $\mu$ is either less than $450$ or $\mu$ is larger than $450$. When we reject the null hypothesis, we are accepting this alternative, that  $\mu$ could be some value other than $450$. Notice that this alternative hypothesis is remarkably vague; we would reject the null hypothesis regardless of whether the sample mean turns out to be $600$ or $-600$, for example. The  practical implication is that the p-value gives us  the  strength of the evidence against the null hypothesis; it doesn't give us  evidence in favor of  a specific alternative. The p-value will reject the null hypothesis regardless of whether our sample mean is positive or negative in sign. In psychology and allied disciplines, whenever the p-value falls below $0.05$, it is common practice to write something along the lines that "there was reliable evidence for the predicted effect." This statement  is incorrect! We only ever have evidence against the null. By looking at the sample mean and its sign, we are making a very big leap that we  have evidence for the specific sample mean we happened to get. As we will see below, when power is low, the sample mean can be wildly far from the true mean that produced the data. 

One need not have a two-sided alternative; one could have defined the alternative to be one-sided (for example, that $mu>450$). In that case, one would compute only one side of the area under the curve. This kind of one-sided test is not normally done, but one can imagine a situation where a one-sided test is justified (for example, when only one sign of the effect is possible, or if there is a strong theoretical reason to expect only one particular sign---positive or negative---on an effect). That said, in their scientific career, only one of the authors of this book has ever had occasion to use a one-sided test. In this book, we will not use one-sided t-tests.

The p-value is always interpreted with reference to the pre-defined Type I error. Conventionally, we reject the null if $p<0.05$. This is because we set the Type I error probability at 0.05. Keep in mind that Type I error probability and the p-value are two distinct things. 
The Type I error probability is the probability of your incorrectly rejecting the null under repeated sampling. This is not the same thing as your p-value. The latter probability will be obtained from a particular experiment, and will vary from experiment to experiment; it is a random variable. By contrast, the Type I error probability is a value we fix in advance.

### The distribution of the p-value under the null hypothesis

We have been talking about a continuous random variable as a dependent measure, and have learnt about the standard two-sided t-test, with a point null hypothesis. When we do such a test, we usually use the p-value to decide whether to reject the null hypothesis or not.

Sometimes, you will hear  statisticians (e.g., Andrew Gelman on his blog) criticize p-values by saying that the null hypothesis significance test is a "specific random number generator".  For example, a blog post from May 5, 2016 (shorturl.at/irHS9) quotes Daniel Lakeland: ``A p-value is the probability of seeing data as extreme or more extreme than the result, under the assumption that the result was produced by a specific random number generator (called the null hypothesis).''

What does that sentence mean? We explain this point here because it is very helpful in understanding the p-value.

Suppose that the null hypothesis is in fact true. We will now simulate the distribution of the p-value under repeated sampling, given this assumption. See Figure \@ref(fig:pvalnulltrue).

```{r pvalnulltrue,cache=TRUE,echo=FALSE,fig.cap="The p-value has a uniform distribution when the null hypothesis is true; a demonstration using simulation."}
nsim <- 100000
pvals <- rep(NA, nsim)
for (i in 1:nsim) {
  ## Here, the Null Hypothesis is that mu = 0
  y <- rnorm(100)
  pvals[i] <- t.test(y)$p.value
}

hist(pvals,
  freq = FALSE,
  main = "The distribution of p-values\n when null is true",
  xlab = "p-values",
  ylab = "density"
)
```

When the null hypothesis is actually true, the distribution of the p-value is uniform---every value between 0 and 1 is equally likely. The practical implication of this criticism of p-values is that when we do a single experiment and obtain a p-value under the assumption that the null is true, if the null were in fact true, then we are just using a random number generator (generating a random number between 0 and 1, $Z\sim uniform(0,1)$) to make a decision that the effect is present or absent. 

A broader implication is that we should not place our theory development exclusively at the feet of the p-value. As we discuss in this book, other considerations (such as replicability, uncertainty of the estimates, and power) are more important than the p-value.


:::: {.blackbox data-latex=""}
::: {data-latex=""}
**Proof that the p-value is uniformly distributed under the null: The probability integral transform**
:::


The fact that the p-value comes from a uniform distribution with bounds 0 and 1 when the null hypothesis is true can be formally derived using the random variable theory in chapter 1.

Consider the fact that the p-value is a random variable; call it $Z$. The p-value is the cumulative distribution function (CDF) of the random variable $T$, which itself is a transformation of the random variable that represents the mean of the data $\bar{Y}$: 

$T=(\bar{Y}-\mu)/(\sigma/\sqrt{n})$

This random variable $T$ itself has some CDF $F(T)$. It is possible to show that if a random variable $Z=F(T)$, i.e., if $Z$ is the CDF for the random variable $T$, then $Z$ has a uniform distribution ranging from 0 to 1, $Z \sim Uniform(0,1)$.

This is an amazing fact. To get a grip on this, let's first think about the fact that when a random variable $Z$ comes from a $Uniform(0,1)$ distribution, then $P(Z<z)=z$. Consider some examples:

- when $z=0$, then $P(Z<0)=0$;
- when $z=0.25$, then $P(Z<0.25)=0.25$; 
- when $z=0.5$, then $P(Z<0.5)=0.5$; 
- when $z=0.75$, then $P(Z<0.75)=0.75$; 
- when $z=1$, then $P(Z<1)=1$. 

You can verify this by typing:

```{r}
z <- c(0, 0.25, 0.5, 0.75, 1)
punif(z, min = 0, max = 1)
```

Next, we will prove the above statement, that if a random variable $Z=F(T)$, i.e., if $Z$ is the 
CDF for a random variable $T$, then $Z \sim Uniform(0,1)$.
The proof is actually quite astonishing and even has a name:  it's called the *probability integral transform*.

Suppose that $Z$ is the CDF of a random variable $T$: $Z=F(T)$. Then, it follows that $P(Z\leq z)$ can be rewritten in terms of the CDF of T: $P(F(T)\leq z)$. Now, if we apply the inverse of the CDF ($F^{-1}$) to both the left and right sides of the inequality, we get $P(F^{-1}F(T)\leq F^{-1}(z))$.
But $F^{-1}F(T)$ gives us back $T$; this holds because if we have a one-to-one onto function $f(x)$, then applying the inverse $f^{-1}$ to this function gives us back $x$. 

The fact that $F^{-1}F(T)$ gives us back $T$ means that we can rewrite $P(F^{-1}F(T)\leq F^{-1}(z))$ as $P(T\leq F^{-1}(z) )$. But this probabilityy is the CDF $F(F^{-1}(z))$, which simplifies to $z$. This shows that $P(Z\leq z) = z$; i.e., that the p-value has a uniform distribution under the null hypothesis. 

The above proof is restated below compactly:

\begin{equation}
\begin{split}
P(Z\leq z) =& P(F(T)\leq z)\\
=& P(F^{-1}F(T)\leq F^{-1}(z))\\
=& P(T\leq F^{-1}(z) ) \\
=& F(F^{-1} (z))\\
=& z\\
\end{split}
\end{equation}

It is for this reason that statisticians like Andrew Gelman periodically point out that "the null hypothesis significance test is a specific random number generator". 

::::

### Type M and S error in the face of low power

Beyond Type I and II error, there are also two other kinds of error to be aware of. These are Type M(agnitude) and S(ign) error; both sources of error are closely related to statistical power.

The terms Type M and S error were introduced by @Gelman14, but the ideas have been in existence for some time [@hedges1984estimation],[@lane1978estimating]. @powerfailure refer to Type M and S error as the "winner's curse" and "the vibration of effects." In related work, @ioannidis2008most refers to the vibration ratio in the context of epidemiology.

Type S and M error can be illustrated with the following example.
Suppose your true effect size is believed to be $D=15$, 
then we can compute (apart from statistical power) the following error rates, which are defined as follows:

- **Type S error**: the probability that the sign of the effect is incorrect, given that the result is statistically significant.
- **Type M error**: the expectation of the ratio of the absolute magnitude of the effect to the hypothesized true effect size, given that the result is significant. 
Gelman and Carlin also call this the exaggeration ratio, which is perhaps more descriptive than "Type M error".

Suppose that a particular study has standard error $46$, and sample size $37$. And suppose that the true $\mu=15$, as in the example discussed above. Then, we can compute statistical power, Type S and M error through simulation in the following manner:

```{r}
## probable effect size, derived from past studies:
D <- 15
## SE from the study of interest:
se <- 46
stddev <- se * sqrt(37)
nsim <- 10000
drep <- rep(NA, nsim)
for (i in 1:nsim) {
  samp <- rnorm(37, mean = D, sd = stddev)
  drep[i] <- mean(samp)
}
```

Power can be computed by simply determining the proportion of times that the absolute  observed t-value  is larger than 2:

```{r}
## power: the proportion of cases where
## we reject the null hypothesis correctly:
(pow <- mean(ifelse(abs(drep / se) > 2, 1, 0)))
```

Power is quite low here (we deliberately chose an example with  low power to illustrate Type S and M error). 

Next, we figure out which of the samples are statistically significant (which simulated values yield $p<0.05$). As a criterion, use a t-value of 2 to reject the null; this could have been done more precisely by working out an exact critical t-value for the given sample size.

```{r}
## which results in drep are significant at alpha=0.05?
signif <- which(abs(drep / se) > 2)
```

Type S error is the proportion of significant cases with the wrong sign (sign error), and Type M error is the ratio by which the true effect (of $\mu=15$) is exaggerated in those simulations that  happened to come out significant. 

```{r}
## Type S error rate | signif:
(types_sig <- mean(drep[signif] < 0))

## Type M error rate | signif:
(typem_sig <- mean(abs(drep[signif]) / D))
```

In this scenario, when power is approximately 6%, whenever we get a significant effect, the probability of obtaining the wrong  sign is a  whopping `r round(types_sig*100)`% and the effect is likely to be `r round(typem_sig,2)` times larger than its true magnitude. The practical implication is as follows.

When power is low, relying on the p-value (statistical significance) to declare an effect as being present will be misleading **even if the result is statistically significant**. This is because the significant effect will be  based on an overestimate of the effect (Type M error), and even the sign of the effect could be wrong. This isn't just a theoretical point; it has real-world  consequences for theory  development. For an  example from psycholinguistics regarding this point, see @VasishthMertzenJaegerGelman2018, @JaegerMertzenVanDykeVasishth2019, and @MertzenEtAl2021. In all these studies, an attempt was made to re-estimate published estimates of effects using larger sample sizes; in all cases, the larger sample sizes showed smaller estimates. In other words, the original published estimates, which were all based on low-powered studies, were over-estimates that did not replicate. A practical implication of Type M error is that "statistically significant effects" will not be "reliable" in any meaningful sense if power is low. 

Another useful way to  visualize Type M and S error  is through the so-called funnel plot. As shown in Figure \@ref(fig:funnel), estimates obtained from low-powered studies will tend to be exaggerated (the lower part of the funnel), and as power goes up, the effect estimates start to cluster tightly around the true value of the effect. 

```{r funnel,echo=FALSE,fig.cap="An illustration of a funnel plot. Shown are repeated samples of an effect estimate under different values of power, where the true value  of the effect is 15 (marked by the vertical line). Significant effects are shaded gray. The lower the power, the wider the fluctuation of the effect; under low power, it is the exaggerated effects that end up statistically significant, even though they are very biased relative to the true value. As power goes up, the effect estimates start to cluster around the true value, and significant effects are also accurate estimates of the effect. Thus, low power leads to exaggerated estimates of the effect, especially if the data are filtered by statistical significance."}
## funnel plot:
truemu <- 15
sampsize <- seq(10, 10000, by = 10)
n_expts <- length(sampsize)
means <- power <- sig <- rep(NA, n_expts)
for (i in 1:n_expts) {
  y <- rnorm(sampsize[i],
    mean = truemu,
    sd = 250
  )
  sig[i] <- ifelse(t.test(y)$p.value < 0.05, 1, 0)
  means[i] <- mean(y)
  power[i] <- power.t.test(
    d = truemu, sd = 250,
    n = sampsize[i]
  )$power
}

means_df <- data.frame(means, power, sig)

plot(jitter(means), jitter(power),
  main = "Funnel plot",
  xlim = range(c(min(means), max(means))),
  xlab = "effect", ylab = "power",
  # ylim=c(0,0.003),
  cex.lab = 1.8,
  cex.axis = 1.5, cex.main = 1.5
)
abline(v = 15)

sig_effects <- subset(means_df, sig == 1)

points(sig_effects$means, sig_effects$pow,
  pch = 21,
  bg = "#CACACA"
)
```

What is important to appreciate here is the fact that significant effects "point to the truth" just in case power is high; when power is low, either null results will frequently be found **even if the null is false**, and those results that turn out significant will be based on Type M error. Thus, when power is low, **all** possible outcomes (significant or non-significant) from a statistical analysis based on a p-value will be meaningless in the sense that non-significant effects don't allow us to accept the null hypothesis (due to low power), and significant effects are going to be based on exaggerated estimates that don't reflect the true value.

In many fields, it is practically impossible to conduct a high-powered study. What should one do in this situation? When reporting results that are likely based on an underpowered study, the best approach is to (i) simply report estimates with confidence intervals and not make binary decisions like "effect present/absent", (ii) openly acknowledge the power limitation, (iii) attempt to conduct a direct replication of the effect to establish robustness, and (iv) attempt to synthesize the evidence from existing knowledge [@cumming2014new].  

One can focus on reporting estimates in a paper as follows. For example, one can state that "the estimate of the mean effect was 50 ms, with 95% confidence interval [-10,110] ms. This estimate is consistent with the effect being present." Such a wording does not make any discovery claim; it just reports that the estimate is consistent with the predicted direction. Such a reported estimate can be used in meta-analyses, facilitating cumulative acquisition of knowledge.

By direct replication, we mean that the study should be run multiple times with the same materials and design but new participants, to establish whether effect estimates in the original study and the replication study are consistent with each other. Direct replications stand in contrast to so-called conceptual replications, which are not exact repetitions of the original design, but involve some further or slightly different but related experimental manipulations. Conceptual replications are also a very useful tool for cross-validating the existence of an effect, but direct replications should be a standard way to validate the consistency of an effect.

Of course, truly direct  replications are impossible to conduct because repeating a study will always differ from the original one in some way or another---the lab may differ, the protocols might differ slightly, the experimenter may be different, etc. Such between-study variability is obviously unavoidable in direct-replication attempts, but they are still worthwhile for establishing the existence of an effect. To make clearer the idea of establishing robustness through replication attempts, detailed examples of different kinds of replication  attempts of published studies will be presented in this book's example data sets.

Finally, meta-analysis is a very important tool for developing a good understanding of what has been learned in a field about a particular phenomenon. Meta-analysis has been largely neglected in psycholinguistics, with researchers classifying previous work using a voting method: researchers routinely count the number of studies that showed a significant vs. non-significant effect.  As an example, @hammerly2019grammaticality summarize the literature on a particular psycholinguistic phenomenon in this binary manner: "In our review, only 11 of the 22 studies that tested for the interaction indicative of the grammaticality asymmetry found a significant effect. Of the studies that ran contrasts to test for an effect of attractor number in grammatical sentences, 7 of the 20 studies found a significant effect, while all of the 16 studies that tested for effects of attractor number in ungrammatical sentences found a significant effect." As discussed above, this kind of summary is quite meaningless if the power of the experiments is not known to be high. It is much better to summarize the literatuee using a meta-analysis, which reports an estimate based on all existing studies, along with an uncertainty interval. Later in the book, concrete examples of meta-analyses will be discussed.

### Searching for significance

The NHST procedure is essentially a decision procedure: if $p<0.05$, we reject the null hypothesis; otherwise, we fail to reject the null. Because significant results are easier to publish than non-significant results, a common approach taken by researchers (including the first author of this book, when he was a graduate student) is to run the experiment and periodically check if statistical significance has been reached. The procedure can be described as follows:

- The experimenter gathers $n$ data points, then checks for significance (is $p<0.05$ or not?). 
- If the result is not significant, they get more data (say, $n$ more data points). Then they check for significance again.

Since time and money (and patience) are limited, the researcher might decide to stop collecting data after some multiple of $n$ have been collected. 

One can simulate different scenarios here. Suppose that $n$ is initially $15$ subjects.  
Under the standard assumptions, set Type I error probability to be $0.05$. Suppose that the null hypothesis that $\mu=0$ is in fact true, and that the standard deviation is $250$ ms (assuming a reading study). 

```{r}
## Standard properties of the t-test:
pvals <- NULL
tstat_standard <- NULL
n <- 15
nsim <- 10000
## assume a standard dev of 250:
stddev <- 250
mn <- 0
for (i in 1:nsim) {
  samp <- rnorm(n, mean = mn, sd = stddev)
  pvals[i] <- t.test(samp)$p.value
  tstat_standard[i] <- t.test(samp)$statistic
}
```

Type I error rate is about 5%, consistent with our expectations:

```{r}
round(mean(pvals < 0.05), 2)
```

But the situation quickly deteriorates as soon as we adopt the strategy outlined above. Below, we will also track the distribution of the t-statistic.

```{r}
pvals <- NULL
tstat <- NULL
## how many subjects can I run?
upper_bound <- n * 6

for (i in 1:nsim) {
  significant <- FALSE
  x <- rnorm(n, mean = mn, sd = stddev) ## take sample
  while (!significant & length(x) < upper_bound) {
    ## if not significant:
    if (t.test(x)$p.value > 0.05) {
      ## get more data
      x <- append(x, rnorm(n, mean = mn, sd = stddev))
    } else {
      significant <- TRUE
    } ## otherwise stop:
  }
  pvals[i] <- t.test(x)$p.value
  tstat[i] <- t.test(x)$statistic
}
```

Now, Type I error rate is much higher than 5%:

```{r}
round(mean(pvals < 0.05), 2)
```

Figure \@ref(fig:stoppingrule) shows the distributions of the t-statistic in the standard case vs/ with the above stopping rule:

```{r stoppingrule,echo=FALSE,fig.cap="A comparison of the distribution of t-values with an a priori fixed stopping rule (solid line), versus a flexible stopping rule conditional on finding significance (broken line)."}
plot(density(tstat_standard), main = "", xlab = "t-value")
lines(density(tstat), lty = 2)
```

What is important to realize here is that the inflation in the Type I error probability observed above was due to the fact that the t-distribution is no longer a t-distribution:  there are bumps in the tails when we use the flexible stopping rule, and these raise the Type I error. This demonstrates why one should fix one's sample size in advance, based on a power analysis. One should not deploy a stopping rule like the one above; if one uses such a stopping rule, there is a higher than 5\% probability of incorrectly declaring a result as statistically significant than the original Type I error rate of 0.05. 

There can be compelling reasons to adopt the above peek-and-run strategy; e.g., if one wants to avoid exposing patients to a treatment that might turn out to be harmful. In such situations, one can run an adaptive experimental trial by correcting for Type I error inflation [@pocock2013clinical]. 

In this book, we will aim to develop a workflow whereby the sample size is fixed through power analysis, in advance of running an experiment. 

## The two-sample t-test vs. the paired t-test

In our running example above, we examined the case where we have a single vector of data $y$. This led to the one-sample t-test. 

Next, we consider a case where we have two vectors of data. The data-set below is from @johnson2011quantitative.  Shown below are F1 formant data (in Hertz) for different vowels produced by  male and female speakers of different languages. (In a speech wave, different bands of energy centered around particular frequencies are called formants.)

```{r}
library(lingpsych)
data("df_F1data")
F1data <- df_F1data
F1data
```

Notice that each row belongs to the same vowel and language, and there are repeated instances of each vowel and language. However, males' and females' F1 frequencies can be seen as independent, completely ignoring the repeated instance of vowel and language. The t-test does not "know" whether the rows in the data are repeated or not---it is the researcher's job to make sure that model assumptions are met. In this case, when analyzing the male vs. female data, the assumption of the t-test is that each number in the male and female vector is independent of the others. 

So, we will treat the male and female vectors as independent. Suppose that our null hypothesis is that there is no difference between the mean F1's for males ($\mu_m$) and females ($\mu_f$).
Now, our null hypothesis is $H_0: \mu_m = \mu_f$ or $H_0: \mu_m - \mu_f = \delta = 0$.

This kind of design calls for a two-sample t-test. The two-sample t-test stands in contrast to the paired t-test, discussed below.

Because the formatting of the t-test in R is somewhat verbose, here is a function that extracts the essential information from a t-test:

```{r}
summary_ttest <- function(res, paired = TRUE, units = "ms") {
  obs_t <- round(res$statistic, 2)
  dfs <- round(res$parameter)
  pval <- round(res$p.value, 3)
  ci <- round(res$conf.int, 2)
  est <- round(res$estimate, 2)
  if (paired == TRUE) {
    print(paste(
      paste("t(", dfs, ")=",
        obs_t,
        sep = ""
      ),
      paste("p=", pval, sep = "")))
      print(paste("est.: ", est, " [", 
                  ci[1], ",", ci[2], "] ", 
                  units, sep = "")
    )
  } else {
    print(paste(
      paste("t(", dfs, ")=",
        obs_t,
        sep = ""
      ),
      paste("p=", pval, sep = "")))
      print(paste(paste("est. 1: ", est[1], sep = ""),
      paste("est. 2: ", est[2], sep = ""),
      paste("CI of diff. in means: [", ci[1], ",", ci[2], "]", sep = "")))
  }
}
```

The function call in R for a two-sample t-test is shown below. The assumption is that both the male and female F1 scores have equal variance.

```{r}
res<-t.test(F1data$female, F1data$male,
  paired = FALSE,
  var.equal = TRUE
)
summary_ttest(res,paired=FALSE)
```

This t-test is computing the following t-statistic:

\begin{equation}
t=\frac{d-(\mu_m - \mu_f)}{SE} = \frac{d-0}{SE} 
\end{equation}

\noindent 
where $d$ is the difference between the two sample means; the rest of the terms we are familiar with. SE is the estimated standard error of the sampling distribution of the difference between the means.

We will now do this calculation "by hand." The only new things are the formula for the SE calculation, and the degrees of freedom for t-distribution $(2\times n - 2)=36$.

The standard error for the difference in the means in the two-sample t-test is computed using this formula:

\begin{equation}
SE_\delta 
= \sqrt{\frac{\hat\sigma_m^2}{n_m} + \frac{\hat\sigma_f^2}{n_f}}
\end{equation}
  
Here, $\hat\sigma_m$ is an estimate of the standard deviation for males, and $\hat\sigma_f$  for the females; the $n$ are the respective sample sizes for males ($n_m$) and females ($n_f$).
  
```{r}
n_m <- n_f <- 19
## difference of sample means:
d <- mean(F1data$female) - mean(F1data$male)
(SE <- sqrt(var(F1data$male) / n_m + var(F1data$female) / n_f))
(observed_t <- (d - 0) / SE)
## p-value:
2 * (1 - pt(observed_t, df = n_m + n_f - 2))
```
  
The output of the two-sample t-test and the hand-calculation above match up.

Now consider what will change once we analyze the data focusing this time on the paired nature of the data (same vowel and same language). The two-sample  t-test now becomes a so-called paired t-test. 

For such  paired data,  the null hypothesis is that the F1 format value produced for a given vowel-and-language combination is the same for males and females: $H_0: \mu_f-\mu_m=\delta=0$. But since each row in the data-frame is paired (from the same vowel+language), we subtract the vector row-wise, and get a new *vector* $d$ (not a single number $d$ as in the two-sample t-test) with the row-wise differences. Then, we just do the familiar one-sample test we saw earlier:
  
```{r}
d <- F1data$female - F1data$male
res<-t.test(d)
summary_ttest(res)
```

An alternative syntax for the paired t-test explicitly feeds the two paired vectors into the function, but one must explicitly specify that they are paired, otherwise the test is a two-sample (i.e., unpaired) t-test:

```{r}
res<-t.test(F1data$female, F1data$male, paired = TRUE)
summary_ttest(res)
```
  
Incidentally, if one flips the order in which the vectors are entered into the `t.test` function, the sign of the t-value and of the estimate will of course flip. 
  
```{r}
res<-t.test(F1data$male, F1data$female, paired = TRUE)
summary_ttest(res)
```

Generally, one should ensure that the order in which one enters the vectors leads to an estimate that has an easy-to-interpret sign. For example, if object relatives (ORs) are expected to take longer to read than subject relatives (SRs), it would be better to place the vectors of reading times in the order OR,vSR. Of course, no harm can come from reversing the orders; it's just that the meaning of the estimate could be misinterpreted by the reader. 
  
A crucial assumption in the above paired t-test is that all the vowel-and-language combinations are independent of each other. Whether this assumption is correct or not (or approximately correct) depends on domain knowledge.
  
Incidentally, the p-value in the paired t-test is statistically significant, unlike the two-sample t-test above. The null hypothesis is the same in both tests, but the significance level leads to different conclusions. Which analysis is correct, the two-sample t-test or the paired t-test? It all depends on your assumptions about what the data represent. If you consider the data to be paired by vowel-and-language, for the reasons given above, then a paired test is called for. If it is more reasonable to assume that each data point from the 19 males is independent of the others and from the 19 females, we would treat this as unpaired data. Students are often tempted to choose the test that yields a significant effect, just because a p-value below 0.05 would render the analysis publishable. The reasoning should be based on what the model assumptions are, and whether the model makes sense, even in the simple case of the t-test.

In the data sets we will encounter in the remainder of this book, we will never use one or two-sample or paired t-tests. Instead, we will be using linear models or linear mixed models. The different varieties of t-test are discussed here because they are frequently used (and misused) in published papers, and because (as shown later) t-tests are in fact simplified linear mixed models in disguise.

Next, we look at some applications of the t-test in more complex settings than simple two-condition designs, and some subtle points relating to this statistical test. The essential point to take away from the following sections is that although the t-test has the wonderful property of simplicity, hidden dangers lurk which can mislead even highly experienced researchers.  


## Using paired t-tests in complex factorial designs

In psychology and psycholinguistics, it is fairly common to run relatively complex factorial designs. An example is the $2\times 2$ repeated measures design in @levy2013expectation. Even more complex designs, like $2\times 2\times 2$ design, are widely used; an example is @fedorenko2006nature. The data from these two papers illustrate some of the more common problems with the use and abuse of the t-test.

### Analyzing a $2\times 2$ repeated measures design using paired t-tests

The @levy2013expectation paper was a reading study on German that used eye-tracking and had two experiments, each with a $2\times 2$ design. In Experiment 1, the four conditions are as shown in Figure \@ref(fig:lk13E1). The four conditions can be described as follows:

- Condition a: adjunct in subordinate clause, dative noun in subordinate clause.
- Condition b: adjunct in main clause, dative noun in subordinate clause.
- Condition c: adjunct in subordinate clause, dative noun in main clause.
- Condition d: adjunct in main clause, dative clause in main clause.

The research questions in this paper can be characterized in terms of so-called main effects and interactions: Is there 

- a main effect of dative-noun location (main or subordinate clause)
- a main effect of adjunct location (main or subordinate clause)
- an interaction between the two factors?

(ref:lk13E1) The experiment design in Experiment 1 of @levy2013expectation. The figure is re-used with permission from @VasishthMertzenJaegerGelman2018 (License number 5211790221849).

```{r lk13E1, fig.cap = "(ref:lk13E1)", out.width = "80%", echo = FALSE, fig.align = "center"}
knitr::include_graphics("figures/lk13E1.png", dpi = 1000)
```

Some pre-processing has to be carried out to do the t-tests needed to answer these questions. First, load the data, create a condition column, and then examine the resulting structure of the data frame:

```{r}
data("df_levykeller13E1")
head(df_levykeller13E1)
```

As will become clear below, it is convenient to recode the $2\times 2$ design in terms of the four conditions labeled a-d.

```{r}
## create a condition column:
df_levykeller13E1$cond <-
  factor(ifelse(df_levykeller13E1$dat == "sub" & 
                  df_levykeller13E1$adj == "sub", "a",
    ifelse(df_levykeller13E1$dat == "sub" & 
             df_levykeller13E1$adj == "main", "b",
      ifelse(df_levykeller13E1$dat == "main" & 
               df_levykeller13E1$adj == "sub", "c",
        ifelse(df_levykeller13E1$dat == "main" & 
                 df_levykeller13E1$adj == "main", "d", "NA")
      )
    )
  ))
## sanity check:
xtabs(~ cond + adj, df_levykeller13E1)
xtabs(~ cond + dat, df_levykeller13E1)
```

Notice that each of the 28 subjects sees each condition six times. This is a Latin square design, which means that there should be $6\times 4=24$ items. You must check this!

```{r}
t(xtabs(~subj+cond,df_levykeller13E1))
```

The repeated measurements from each subject in each condition is problematic for the t-test: the t-test assumes that each subject delivers exactly one data point per condition. Mathematically, what's assumed in the t-test is that for $i=1,\dots,28$ subjects, we have four vectors of length 28, one for each condition: 

- $x_{a,1},\dots,x_{a,28}$
- $x_{b,1},\dots,x_{b,28}$
- $x_{c,1},\dots,x_{c,28}$
- $x_{d,1},\dots,x_{d,28}$

Instead, what we have is four vectors, each of length $28\times 6$. For example, look at condition a's data:

```{r}
length(subset(df_levykeller13E1,cond=="a")$TFT)
28*6
```

What is necessary here is to force the data into the format that t-test expects. The data be aggregated: restructure the data so that each subject's response for each of the four conditions is the mean of the six repetitions. This is often called a by-subject analysis:

```{r}
bysubj <- aggregate(TFT ~ subj + cond,
  mean,
  data = df_levykeller13E1
)
head(bysubj)
```

Now, each subject delivers exactly one data point per condition, as the t-test demands. 

```{r}
length(subset(bysubj,cond=="a")$TFT)
```

Notice that now we have no column any more indicating which items the subjects saw---this information has disappeared because for each subject, we took the average over all the six items for each condition. We have lost information; as discussed later, this loss of information can have far-reaching, adverse consequences for statistical inference.

```{r}
t(xtabs(~subj+cond,bysubj))
```


The main effects and interactions can be visualized as in Figure \@ref(fig:lk13E1plot). 

```{r lk13E1plot,echo=FALSE, fig.cap="Visualizing the means from the four conditions in the Levy and Keller 2013 Experiment 1 design allows us to graphically summarize the statistical comparisons of interest: the main effects of the two factors, and their interactions."}
meansE1<-with(bysubj,tapply(TFT,cond,mean))
names(meansE1)<-c("adj:sub, \n dat:sub",
                  "adj:main, \n dat:sub",
                  "adj:sub, \n dat:main",
                  "adj:main, \n dat:sub")
barplot(meansE1,ylab="Total fixation time (ms)",xlab="conditions",main="Main effects and interactions\n Levy and Keller, 2013, Expt 1")

```

The main effect of dative is asking whether the average of the two dat:main conditions is different from the average of the two dat:sub conditions. In terms of the means of the four conditions labeled a-d, the null hypothesis is then:

\begin{equation}
H_{0,MEdat}: \frac{\mu_a + \mu_b}{2} = \frac{\mu_c + \mu_d}{2}
\end{equation}

The paired t-test can be used to test this main effect. It will be convenient to obtain the reading times for each condition, and just do paired t-tests. (Each of the main effects and the interaction are going to be paired t-tests.)

```{r}
cond_a<-subset(bysubj,cond=="a")$TFT
cond_b<-subset(bysubj,cond=="b")$TFT
cond_c<-subset(bysubj,cond=="c")$TFT
cond_d<-subset(bysubj,cond=="d")$TFT
```

Here is the computation for the main effect of the dative noun:

```{r}
## main effect of dative:
mean_ab<-(cond_a+cond_b)/2
mean_cd<-(cond_c+cond_d)/2

MEdat_res<-t.test(mean_ab,mean_cd,paired=TRUE)
summary_ttest(MEdat_res)
```

@levy2013expectation report (their Table 6) a main effect of dative as having an estimate of $102.48$ ms (no confidence interval is reported), with a p-value below $0.01$. The small difference between their estimate and ours is because they analyzed unaggregated data with a linear mixed model. Their analysis is actually the better way to analyze these data, as we will discuss later in this book.

The main effect of adjunct is asking whether the average of the two adj:main conditions is different from the average of the two adj:sub conditions. 

\begin{equation}
H_{0,MEadj}: \frac{\mu_a + \mu_c}{2} = \frac{\mu_b + \mu_d}{2}
\end{equation}

This main effect can be tested as follows:

```{r}
mean_ac<-(cond_a+cond_c)/2
mean_bd<-(cond_b+cond_d)/2
MEadj_res<-t.test(mean_bd,mean_ac,paired=TRUE)
summary_ttest(MEadj_res)
```

@levy2013expectation also report a non-significant effect estimate of 16.28 ms, so this approximately matches our estimate. 

Finally, the interaction of the two factors (call it $dat\times adj$) is asking whether the adjunct effect within dat:main has the same value as the adjunct effect in dat:sub. In other words, the interaction is testing for no difference between two differences:

\begin{equation}
H_{0,dat\times adj}: (\mu_a - \mu_b) = (\mu_c - \mu_d)
\end{equation}

Another way to write this null hypothesis is as testing whether the difference between the two differences  is 0:

\begin{equation}
H_{0,dat\times adj}: (\mu_a - \mu_b) - (\mu_c - \mu_d) = 0
\end{equation}

The implementation of the t-test works like this:

```{r}
diff_ab<-cond_a-cond_b
diff_cd<-cond_c-cond_d
INTdatxadj_res<-t.test(diff_ab,diff_cd,paired=TRUE)
summary_ttest(INTdatxadj_res)
```

@levy2013expectation report a non-significant estimate of $-96.82$ ms for the interaction. This also approximately matches our calculations. 

The broader lesson here is that one can break down a design with a factorial design to a series of paired t-tests. Although it is extremely easy to analyze factorial designs like these with paired t-tests, there is an important price to be paid. The price is inflation of Type I error probability. 

### A complication with multiple t-tests: Inflation of Type I error probability 

Each of the three hypothesis tests has a Type I error probability set at $0.05$. Once the three hypothesis tests are carried out, the probability space is as shown in Figure \@ref(fig:probtree). The way to interpret the probability space is as follows: Each time that a hypothesis is tested, if the null hypothesis is in fact true, we can reject it incorrectly with probability 0.05, and accept it correctly with probability 0.95. Each successive hypothesis test takes us further down the tree, and the binary branches represent all the possible paths that could be taken. Thus, if we want to compute the probability of correctly failing to reject the null in all three hypothesis tests assuming that the null is true, we simply multiply the three probabilities 0.95 to get `r 0.95^3`. Subtracting one from this quantity gives us the probability of rejecting *at least one* null hypothesis incorrectly.  

```{r probtree, fig.cap = "The probability space representing three successive independent hypothesis tests when all null hypotheses are true, and Type I error probability is 0.05. Each hypothesis can either be rejected (rej.) or fail to be rejected (fail).", out.width = "80%", echo = FALSE, fig.align = "center"}
knitr::include_graphics("figures/probtree.png", dpi = 1000)
```

The probability of rejecting *at least one* of the three hypotheses incorrectly is no longer $0.05$, but rather:

\begin{equation}
1-0.95^3 = 0.1426
\end{equation}

This inflated Type I error probability can become very serious; for example, @levy2013expectation carried out 32 hypothesis tests; this results in an inflated Type I error probability of 

\begin{equation}
1-0.95^{32} = 0.8063
\end{equation}

That is an 80% probability of incorrectly rejecting at least one of the hypotheses tested! It is not uncommon to run 250 or even more hypothesis tests in a single paper. That would be a Type I error probability of `r round(1-0.95^{250},3)` of rejecting at least one null hypothesis incorrectly; you are essentially sure to get something come out significant incorrectly.

A standard approach to correcting this inflation is to use the Bonferroni correction: divide the standard Type I error by the number of tests, and use this corrected probability as the new Type I error. For example, in @levy2013expectation, the corrected $\alpha$ for 32 hypothesis tests would be 0.05/32=0.0016. One should use this corrected $\alpha$ to interpret multiple comparisons. 

### Analyzing a $2\times 2\times 2$ repeated measures design using paired t-tests

Even more complex designs can be analyzed using paired t-tests. For example, @fedorenko2006nature report a self-paced reading study with eight conditions. 

- Factor: One noun in memory set or three nouns
easy: Joel 
hard: Joel-Greg-Andy 
- Factor: Noun type, either proper name or occupation
name: Joel-Greg-Andy 
occ: poet-cartoonist-voter
- Factor: Relative clause type: subject vs. object relatives (in English)

The relative clause sentences are the following; the reading time is measured in the relative clause regions *who consulted the cardiologist* and *who the cardiologist consulted*. 

(1)  Subject-extracted:
The physician | who consulted the cardiologist | checked the files | in the office.

(2) Object-extracted:
The physician | who the cardiologist consulted | checked the files | in the office.

The authors had two versions of each of the two relative clause types, but we ignore that detail here. Another problem in the data is that subject 1 delivers twice as much data as the other subjects; this could be due to two subjects being misclassified as subject 1. But we ignore this problem in the data as well.

First, load and examine the data:

```{r}
data("df_fedorenko06")
head(df_fedorenko06)
```

As explained above, we have a $2\times 2\times 2$ design: rctype [obj,subj] $\times$ nountype [name, occupation] $\times$ load [hard,easy]. Region 2 is the critical region, the entire relative clause, and RT is the reading time for this region. Subject and item columns are self-explanatory.

The paired t-test can be used repeatedly to compute the main effects and interactions. This will be assigned as an exercise at the end of this chapter. Here, we want to focus on one interesting analysis in the original paper. The result from that paper is summarized in the quote below (emphasis ours):

"*The most interesting result presented here is **an interaction between syntactic complexity and the memory- noun/sentence-noun similarity during the critical region of the linguistic materials in the hard-load (three memory-nouns) conditions**: people processed object-extracted relative clauses more slowly when they had to maintain a set of nouns that were similar to the nouns in the sentence than when they had to maintain a set of nouns that were dissimilar from the nouns in the sentence; in contrast, for the less complex subject-extracted relative clauses, there was no reading time difference between the similar and dissimilar memory load conditions. In the easy-load (one memory-noun) conditions, no interaction between syntactic complexity and memory-noun/sentence-noun similarity was observed.
**These results provide evidence against the hypothesis whereby there is a pool of domain-specific verbal working memory resources for sentence comprehension, contra Caplan and Waters (1999).**
*"

If the analyses described in the quote above are to be carried out using t-tests (or repeated measures ANOVA), the approach that must be taken is to subset the data such that there are two $2\times 2$ subsetted data sets: one for the hard memory conditions and the other for the easy memory conditions. Then, one can investigate the interaction between the relative clause type (syntactic complexity) and the noun type (proper name vs. occupation) by computing aggregated differences between (i) the relative clause type in Noun Type occupation, and (i) the relative clause type in Noun Type proper name.

```{r}
fed06hard<-subset(df_fedorenko06,load=="hard")

## Compute difference between OR and SR 
## in Noun Type occupation:
RCocc<-aggregate(RT~subj+rctype,
                 mean,data=subset(fed06hard,
                                  nountype=="occ"))
diff_occ_hard<-subset(RCocc,rctype=="obj")$RT-
  subset(RCocc,rctype=="subj")$RT

## Compute difference between OR and SR 
## in Noun Type proper name:
RCname<-aggregate(RT~subj+rctype,
                  mean,data=subset(fed06hard,
                                   nountype=="name"))
diff_name_hard<-subset(RCname,rctype=="obj")$RT-
  subset(RCname,rctype=="subj")$RT

## by-subject interaction:
hardINTres<-t.test(diff_name_hard,
                   diff_occ_hard,paired=TRUE)
summary_ttest(hardINTres)
```

One can also investigate the interaction between RC type and Noun Type in the easy conditions:

```{r}
fed06easy <- subset(df_fedorenko06, 
                    load == "easy")

## Compute difference between OR and SR 
## in Noun Type occupation:
RCocc <- aggregate(RT ~ subj + rctype, 
                   mean, data = subset(fed06easy, 
                                       nountype == "occ"))
diff_occ_easy <- subset(RCocc, rctype == "obj")$RT - 
  subset(RCocc, rctype == "subj")$RT

## Compute difference between OR and SR 
## in Noun Type proper name:
RCname <- aggregate(RT ~ subj + rctype,
                    mean, data = subset(fed06easy, 
                                        nountype == "name"))
diff_name_easy <- subset(RCname, 
                         rctype == "obj")$RT - 
  subset(RCname, rctype == "subj")$RT

## by-subject interaction:
easyINTres <- t.test(diff_name_easy, 
                     diff_occ_easy, 
                     paired = TRUE)
summary_ttest(easyINTres)
```

Based on these aggregated by-subjects analyses, it would be tempting to conclude, as @fedorenko2006nature did, that there is a significant interaction in the hard conditions but not in the easy conditions. However, there are at least two problems here. First, 
the paired t-test (or the identical test, the repeated measures ANOVA) has a serious limitation: it is ignoring the by-item variability (because that is averaged out); later we will see that once one takes all sources of variance into account simultaneously, the picture changes dramatically. Second, the authors implicitly assume that the difference between a significant effect in the hard conditions and the non-significant effect in the easy conditions is itself significant---we will see below that that this conclusion is not warranted in the present case.

In summary, the (paired) t-test is a quick and easy tool for comparing means from two  conditions, and even relatively complex designs can be analyzed. In fact, a well-known mathematical psychologist once told the first author of this book: "If you need to run anything more complex than a t-test, you are asking the wrong question." Indeed, many scientists have had long and successful careers with just the t-test as their entire statistical tool. However, as they say, if all you have is a hammer, everything starts to look like a nail. As we show in this book, much important information is lost when one attempts to boil the data down to fit the constraints of the t-test. 

A further point is that the t-test is often used incorrectly. Some of the most common mistakes are discussed next.

## Common mistakes involving the (paired) t-test

### Ignoring the independence assumption

As also discussed above, the paired t-test assumes that each pair of data points that goes into the t-test is independent of the other pairs of data points. This implies that the data-frame cannot have more than one row for a particular pair. In other words, the data-frame cannot have repeated measurements spread out across rows.  

For example, doing a paired t-test on this hypothetical data-frame would be incorrect:
  
\begin{table}[ht]
\centering
\begin{tabular}{rrrll}
\hline
female & male & vowel & language \\ 
\hline
391 & 339 & i & W.Apache \\ 
400 & 320 & i & W.Apache \\ 
$\vdots$ & $\vdots$ & $\vdots$ & $\vdots$\\
\hline
\end{tabular}
\end{table}

Why? Because the assumption is that each row is independent of the others. This assumption is violated here (this is assuming that repeating the vowel from the same language will lead to some commonalities between the two repetitions).

Consider another hypothetical example. In the table below, from subject 1 we see two  data points each for condition a and for condition b. 
  
  \begin{table}[ht]
\centering
\begin{tabular}{rrrll}
\hline
condition a & condition b & subject & item \\ 
\hline
391 & 339 & 1 & 1 \\ 
400 & 320 & 1 & 2 \\ 
$\vdots$ & $\vdots$ & $\vdots$ & $\vdots$\\
\hline
\end{tabular}
\end{table}

Here, we again have repeated measurements from subject 1. The independence assumption is violated. Erroneous analyses using t-tests abound in experimental science; for examples of these kinds of misuse of the t-test in phonetics, see @NicenboimRoettgeretal and the references cited there.

How to proceed when we have repeated measurements from each subject or each item?
The solution is to aggregate the data so that each subject (or item) has only *one* value for each condition.

This aggregation allows us to meet the independence assumption of the t-test, but it has a potentially huge drawback: it pretends we have one measurement from each subject for each condition. 
Later on we will learn how to analyze unaggregated data, but if we want to do a paired t-test, we have no choice but to aggregate the data in this way.

Although an example of aggregation was already discussed above, it may be helpful to revisit the approach that is necessary using a fully worked example. Consider the following repeated measures data on subject versus object relative clauses in English; the data are from a self-paced reading study, Experiment 1 of @grodner. 

Subject relative clauses are sentences like *The man who was standing near the doorway laughed*. Here, the phrase (called a relative clause) *who was standing near the doorway* modifies the noun phrase *man*; it is called a subject relative because the noun phrase *man* is the subject of the relative clause. By contrast, object relative clauses are sentences like *The man who was the woman was talking to near the doorway laughed*; here, the *man* is the grammatical object object of the relative clause *who was the woman was talking to near the doorway*.

A theoretical prediction is that in English, object relatives are harder to read than subject relatives, in the relative clause verb region. We want to test this prediction.

First, load the data containing reading times from the region of interest (the relative clause verb):

```{r}
## From the library lingpsych:
data("df_gg05e1")
head(df_gg05e1)
```

We have repeated measurements for each condition from 42 subjects, and from 16 items arranged in a Latin square design. You can establish this by using the `xtabs` command.  There are no missing data points:

```{r}
t(xtabs(~ subject + condition, df_gg05e1))
t(xtabs(~ item + condition, df_gg05e1))
```
  
  
It is important to stress once more that it is the researcher's responsibility to make sure that the t-test's assumptions are  met. For example, one could fit a two-sample t-test to the data as provided. The two-sample t-test can be implemented using the syntax shown below:

```{r}
## incorrect t-test!
gg05incorrectres1<-t.test(rawRT ~ condition, df_gg05e1)
summary_ttest(gg05incorrectres1)
```

This t-test is incorrect for several reasons, but the most egregious error here is that the data are paired (each  subject delivers data for both  conditions), and that property of the data is being ignored. 

Another common mistake is to do a paired t-test on the data without checking that the data are independent in the sense discussed above. This kind of mistake happens when researchers neglect to aggregate the data. Again, the `t.test` function will happily return a meaningless result:
  
```{r}
## Incorrect paired t-test!
gg05incorrectres2<-t.test(rawRT ~ condition, 
                          paired = TRUE, df_gg05e1)
summary_ttest(gg05incorrectres2)
```
  
Here, the degrees of freedom indicate that we have fit the incorrect model. As mentioned above, there are 42 subjects and 16 items, and the presentation of items to subjects uses a Latin square design (each subject sees only one condition per item). The 335 degrees of freedom come from $42\times 8=336$ data points, minus one. Why do we say $42\times 8$ and not $42\times 16$? That is because each subject will return eight differences in reading time for each condition: each subject gives us eight subject-relative data points and eight object-relative data points.  

For each of the 42 subjects, the t-test function internally creates a vector of eight data points of subject relatives and subtracts the vector of eight data points of object relatives. That is how we end up with  $42\times 8=336$ data points. 

The 336/2=168 data points in each condition are assumed by the t-test to be independent of each other; but this cannot be the case because each subject delivers eight data points for each condition; these are obviously dependent (correlated) because they come from the same subject. 

What is needed is a single data point, the mean reading time for each subject and condition. That is the by-subjects t-test: the averaging is done over the items for each subject. Analogously, it is also common to compute a by-items t-test; this time, the averaging is done over the subjects for each item. In order to conduct the t-test correctly, aggregation of the data by subjects and by items is necessary. This necessity of analyzing data both by subjects and by items is discussed by @clark73 in a famous article, and more recently by @yarkonigeneralizability and @westfall2017.

Consider the by-subjects aggregation procedure below. Once the code below is run, we have only one data point for each condition and subject:

```{r}
bysubj <- aggregate(rawRT ~ subject + condition,
  mean,
  data = df_gg05e1
)
t(xtabs(~ subject + condition, bysubj))
```

Returning to the t-test, by aggregating the data the  independence assumption of the t-test is met, and the degrees of freedom for this by-subjects analysis are now correct ($42-1=41$):

```{r}
gg05res<-t.test(rawRT ~ condition, bysubj, 
                paired = TRUE)
summary_ttest(gg05res)
```

One unsurprising property of the averaged data is that the longer the average subject relative clause reading time for a particular subject, the longer that subject's average object relative clause reading time (Figure \@ref(fig:averageorsr)). This is not surprising because the mean reading times for each condition are coming from the same subject.

```{r averageorsr,fig.cap="The correlation between average object and subject relative clause reading times for the 42 subjects in the Grodner and Gibson (2005) experiment."}
SRdata <- subset(bysubj, condition == "subjgap")$rawRT
ORdata <- subset(bysubj, condition == "objgap")$rawRT
plot(SRdata, ORdata)
abline(lm(ORdata ~ SRdata))
```

The correlation is:

```{r}
cor(SRdata, ORdata)
```

Similar to the by-subjects aggregation done above, one could do a by-items aggregation and then a by-items t-test (What should be the degrees of freedom for the by-items analysis? There are 16 items in this data set). 

```{r}
byitem <- aggregate(rawRT ~ item + condition,
  mean,
  data = df_gg05e1
)
t(xtabs(~ item + condition, byitem))

gg05itemres<-t.test(rawRT ~ condition, 
                    byitem, paired = TRUE)
summary_ttest(gg05itemres)
```

As in the by-subjects analysis, there seems to be a slight correlation between the average object and subject reading times among the items, but the sparse data (16 items) leads to a lot of variability around the fitted line (Figure \@ref(fig:averageorsritems)).

```{r averageorsritems,fig.cap="The correlation between average object and subject relative clause reading times for the 42 subjects in the Grodner and Gibson (2005) experiment."}
SRdatabyitem <- subset(
  byitem,
  condition == "subjgap"
)$rawRT
ORdatabyitem <- subset(
  byitem,
  condition == "objgap"
)$rawRT
plot(SRdatabyitem, ORdatabyitem)
abline(lm(ORdatabyitem ~ SRdatabyitem))
cor(SRdatabyitem, ORdatabyitem)
```

### Doing a by-subjects and by-items paired t-test is generally dangerous

The paired t-test illustrated above is in general quite a dangerous way to analyze this data-set, because it ignores the fact that each subject  delivers not one but eight data points per condition. Each subject's repeated measurements will introduce a source of variance, but this source of variance is being suppressed in the by-subjects t-test, leading to a possibly over-enthusiastic t-value. Similarly, each item also delivers multiple data points, and therefore introduce a potential source of variance within each item that will be artificially ignored when we aggregate data.

```{r echo=FALSE,message=FALSE}
## set up contrast coding:
df_fedorenko06$ld<-ifelse(df_fedorenko06$load=="hard",1,-1)
df_fedorenko06$nn_easy<-ifelse(df_fedorenko06$load=="easy" & df_fedorenko06$nountype=="occ",-1,
                               ifelse(df_fedorenko06$load=="easy" & df_fedorenko06$nountype=="name",1,0))
df_fedorenko06$nn_hard<-ifelse(df_fedorenko06$load=="hard" & df_fedorenko06$nountype=="occ",-1,
                               ifelse(df_fedorenko06$load=="hard" & df_fedorenko06$nountype=="name",1,0))

df_fedorenko06$rc_easy<-ifelse(df_fedorenko06$load=="easy" & df_fedorenko06$rctype=="subj",-1,
                               ifelse(df_fedorenko06$load=="easy" & df_fedorenko06$rctype=="obj",1,0))
df_fedorenko06$rc_hard<-ifelse(df_fedorenko06$load=="hard" & df_fedorenko06$rctype=="subj",-1,
                               ifelse(df_fedorenko06$load=="hard" & df_fedorenko06$rctype=="obj",1,0))
library(lme4)
m1<-lmer(RT~ld + nn_hard*rc_hard + nn_easy*rc_easy +
           (1+ld + nn_hard:rc_hard ||subj) + 
           (1+ld + nn_hard:rc_hard + nn_easy||item),df_fedorenko06)
#summary(m1)
lmer_tval_INThard<-round(summary(m1)$coefficients[7,3],3)
est_INThard<-round(summary(m1)$coefficients[7,1],3)
SE_INThard<-round(summary(m1)$coefficients[7,2],3)

```

In order to take this variability into account simultaneously, we must switch to the linear mixed model.  For example, in the @fedorenko2006nature data, the interaction between Noun Type and RC type is significant when we aggregate over items; a linear mixed model that takes the item level variability into account yields a smaller t-value of `r lmer_tval_INThard`, with estimate `r est_INThard`, 95% CIs [`r est_INThard - 2*SE_INThard`, `r est_INThard + 2*SE_INThard`] ms. Compare this with the t-value we obtained with the paired t-test earlier:

```{r}
summary_ttest(hardINTres)
```

Thus, the conclusion can change quite dramatically depending on whether one ignores sources of variance in one's analysis or not.

The next chapter explains how to obtain the more realistic estimates from the linear mixed model by taking both subject and item variability into account simultaneously.

### The difference between a significant and a non-significant result need not itself be significant

A very common error in psychology and psycholinguistics is to find a significant effect in one study, then find a non-significant effect in another study that changes one variable from the first study. The conclusion then drawn is that the variable that differs between the two studies is  "significant."  @gelmanhill07 summarize this mistake as follows: "the difference between significant and non-significant is itself not significant." Indeed, @nieuwenhuis2011erroneous show that this mistake is widespread in areas like neuroscience. 

A real-life example will illustrate the problem. Recall that in the @fedorenko2006nature design, the hard conditions showed a significant interaction but the easy conditions showed a non-significant interaction:

```{r}
summary_ttest(hardINTres)
summary_ttest(easyINTres)
```

Is this difference between the easy and hard conditions itself significant? Not necessarily; one has to test for that by carrying out a higher-order interaction: the difference between (i) the RC vs. Noun-Type interaction in the hard condition, and (ii)  the RC vs. Noun-Type interaction in the easy condition. 
In the present case, the higher-order interaction is not even remotely significant---the main claim in the @fedorenko2006nature is not warranted.

```{r}
diff_hard<-diff_name_hard-diff_occ_hard
diff_easy<-diff_name_easy-diff_occ_easy
RCxNTxLD_INTres<-t.test(diff_hard,diff_easy,paired=TRUE)
summary_ttest(RCxNTxLD_INTres)
```


```{r echo=FALSE}
df_fedorenko06$noun<-ifelse(df_fedorenko06$nountype=="name",1/2,-1/2)
df_fedorenko06$ld<-ifelse(df_fedorenko06$load=="hard",1/2,-1/2)
df_fedorenko06$rc<-ifelse(df_fedorenko06$rctype=="obj",1/2,-1/2)
df_fedorenko06$nounxld<-df_fedorenko06$noun*df_fedorenko06$ld*2
df_fedorenko06$nounxrc<-df_fedorenko06$noun*df_fedorenko06$rc*2
df_fedorenko06$ldxrc<-df_fedorenko06$ld*df_fedorenko06$rc*2
df_fedorenko06$nounxldxrc<-df_fedorenko06$noun*df_fedorenko06$ld*df_fedorenko06$rc*4
m2<-lmer(RT~noun+ld+rc+nounxld + nounxrc + ldxrc + nounxldxrc+(1+ld+rc+ nounxrc + nounxldxrc||subj) + (1+ld+rc+ nounxrc + nounxldxrc||item),df_fedorenko06,
         control = lmerControl(calc.derivs = FALSE))
est_3wayint<-round(summary(m2)$coefficients[8,1],3)
SE_3wayint<-round(summary(m2)$coefficients[8,2],3)
t_3wayint<-round(summary(m2)$coefficients[8,3],3)
```

Incidentally, as shown in later chapters, the above analysis can be done with a linear mixed model as well. Such an analysis shows that the t-value is `r t_3wayint`, with estimate `r est_3wayint`, [`r est_3wayint-2*SE_3wayint`, `r est_3wayint+2*SE_3wayint`] ms.
This mistake---not explicitly checking for the interaction---is very common in psychology and psycholinguistics.

## Summary

This chapter explained the basis for the hypothesis testing in frequentist statistics: the central limit theorem. This led to the t-test, specifically, the one-sample t-test (equivalent to the paired t-test) and the two-sample t-test (for unpaired data). The key summary statistics, the t-test and the p-value were discussed, along with the limitations of these statisics in determining whether an effect is significant or not. Specifically, under low statistical power, Type M and S error have the consequence that neither significant nor non-significant results are interpretable. We learned how to compute statistical power using the ` power.t.test` function and using simulation. Important applications of the t-test in factorial designs was discussed, along with the limitations imposed by the t-test; these limitations can lead to misleading conclusions. Finally, some common mistakes in the usage of t-tests were exemplified.

## Further readings

The logic of the hypothesis test and the t-test are discussed in every introductory textbook in statistics. Because there is so much confusion about the basic logic of the hypothesis test, we suggest generally avoiding textbooks written by non-statisticians. We especially recommend @millermiller and @casellaberger.  

## Exercises {#sec:SamplingDistrnexercises}

```{exercise, SamplingDistrnexercisesqt}
Practice using the `qt` function
```

```{r echo=FALSE}
n <- 142
mu <- 123
sigma <- 70
sample.sd <- 50.885
sample.mean <- 145.242
```

Take an independent random sample of size ```r n``` from a normal distribution
with mean ```r mu```, and standard deviation ```r sigma```. Next, we are going to pretend we don't know the population parameters (the mean and standard deviation). We compute the MLEs of the mean and standard deviation using the data and get the sample mean ```r sample.mean``` and the sample standard deviation ```r sample.sd```. 

- Compute the estimated standard error using the sample standard deviation provided above. 
- What are your degrees of freedom for the relevant t-distribution?
- Calculate the **absolute** critical t-value for a 95\% confidence interval using the relevant degrees of freedom you just wrote above.
- Next, compute the lower bound of the 95\% confidence interval using the estimated standard error and the critical t-value.
- Finally, compute the upper bound of the 95\% confidence interval using the estimated standard error and the critical t-value.


```{exercise, SamplingDistrnexercisespvalue}
Computing the p-value
```

A paired t-test is done with data from 10 participants. The t-value from the test is 2.1. What is the p-value associated with a two-sided null hypothesis test? 


```{exercise, SamplingDistrnexercisestvalue}
Computing the t-value
```

 If the p-value from a two-sided null hypothesis test had been 0.09, what  would be the associated absolute t-value (i.e., ignoring the sign on the t-value)?  The number of participants is 10, as above.

```{exercise, SamplingDistrnexercisestype1type2}
Type I and II error
```

Given that Type I error is 0.01; what is the highest value possible for Type II error?  

```{exercise, SamplingDistrnexercisespairedttest}
Practice with the paired t-test
```

In a self-paced reading study, @grodner investigated subjects vs. object relative clauses. They analyzed the reading times  at the relative clause verb. However, a reviewer objects that the whole sentence's reading times (total reading times) should be used to evaluate the difference between the two conditions, because one cannot know where the difficulty might arise. It isn't clear whether one should use mean reading times over the entire sentence, or total reading times (summing up all the reading times over the entire sentence). 
Carry out a by-subjects paired t-test on (a) the critical relative clause verb, versus (b) mean reading time over all words in the two sentence types, and (c) total reading times over all words in the two sentence types. Compare the t-value across the three tests, and decide what the appropriate dependent variable might be (Note: there is no correct answer here).

The data are loaded and pre-processed as follows. The code below gives you the reading times for the data at the relative clause verb. You will have to work out how to obtain mean or total reading times for the whole sentence in each condition.

```{r}
## load data from lingpsych package:
data("df_gg05e1_full")
## get data from relative clause verb:
df_gge1crit <- subset(
  df_gg05e1_full,
  (condition == "objgap" &
    word_position == 6) |
    (condition == "subjgap" & word_position == 4)
)
```

```{exercise, SamplingDistrnexercisespairedttestfedorenko}
Using the paired t-test to test for main effects and interactions
```

Using the data from @fedorenko2006nature discussed in this chapter, carry out all seven paired t-tests to investigate all main effects and interactions.

```{exercise, SamplingDistrnexercisesinteraction}
Explicitly testing for a claimed interaction
```

```{r echo=FALSE}
minf <- function(f1,f2,n1,n2){
fprime <- (f1*f2)/(f1+f2)
n <- round(((f1+f2)*(f1+f2))/(((f1*f1)/n2)+((f2*f2)/n1)))
pval<-pf(fprime,1,n,lower.tail=FALSE)
return(paste("minF(1,",n,")=",round(fprime,digits=2),"p-value=",round(pval,2),"crit=",round(qf(.95,1,n))))
}

#E2 F1(1,54) = 6.36, p = 0.02; F2(1,23) = 4.86, p = 0.04)
#F2(1,23) = 0.10, p = 0.8).
minf(f1=6.36,f2=4.86,n1=54,n2=23)
## t-value from minF':
tE2<-sqrt(2.74)
## approx diff in means (from figs)
dE2<-200
dfE2<-57

#E3 F1(1,43) = 0.16, p = 0.7; F2(1,23) = 0.10, p = 0.8). 
# F2(1,23) = 4.86, p = 0.04). `{r}
minf(f1=0.16,f2=0.10,n1=43,n2=23)
tE3<-sqrt(0.06)
dE3<-100
SE1<-200/tE2
SE2<-100/tE3
dfE3<-50

BetwSE<-sqrt(SE1^2/(dfE2+1) + SE2^2/(dfE3+1))
t_between<-(200-100)/BetwSE
```

@vasishthlewisLanguage05 present two self-paced reading experiments in Hindi, their Experiments 2 and 3. In Experiment 2, the distance  between a grammatical subject and a verb was manipulated by inserting an intervening inanimate noun between the two words; the expectation was that the intervener would slow  processing at the verb. Surprisingly, a faster reading time was seen at the verb when the intervener was present. What was reported in the paper was a linear mixed-effects model-based ANOVA (analysis of variance), which translates to 
a near-significant speedup effect of the distance manipulation. The approximate statistics extracted from the paper are  $t(`r dfE2`) = `r round(-tE2,3)`$. The approximate difference in means, guessed at from their Figures 7 and 8, is about $200$ ms. By contrast, in Experiment 3, new subjects who hadn't participated in Experiment 2 were shown sentences with the same distance manipulation as in Experiment 2 but with the difference that the intervening noun was animate. The hypothesis was that the animacy status of the intervening noun would either reduce or neutralize the speedup, leading to a smaller speedup effect. Rhe reported statistics show that the distance manipulation was not statistically significant.  The approximate t-value is $t(`r dfE3`) = `r round(-tE3,3)`$. Figures 12 and 13 in the paper suggest that the approximate difference between the conditions is about $100$ ms. 

Based on these t-values and the estimates of the differences in means, the authors effectively claim that there is a significant difference between the two experiments. Given the above information, is the conclusion justified? Show the results of an appropriate statistical test and use these to argue whether there is support or no support for their conclusion. (Hint: the two-sample t-test will be useful here.)

```{exercise, SamplingDistrnexercisespowerttest}
Using the power.t.test function
```

```{r echo=FALSE}
n <- 163
mu <- 24
lowermu <- 19
uppermu <- 29
lowersigma <- 119
uppersigma <- 300
```

[In this exercise, assume that Type I error probability is 0.05 unless otherwise stated, and that we are doing a one-sample t-test, with a two-sided hypothesis test.]

You are given that the effect size is `r mu`, with 95\% confidence intervals `r lowermu` and `r uppermu`. The standard deviation can range from `r lowersigma` to `r uppersigma`. 

First, using the power.t.test function, draw for sample size `r n` three power curves for the three effect sizes (`r mu`, `r lowermu` and `r uppermu`), assuming that the standard deviation ranges from `r lowersigma` to `r uppersigma`. Draw a single plot showing all three curves, with the standard deviation on the x-axis and power on the y-axis.  

Then, redo the power analysis using simulation instead of the t-test. Do the following 10000 times: sample data repeatedly with sample size `r n` from a normal distribution with each of the three means above (`r mu`, `r lowermu` and `r uppermu`), and assuming that the standard deviation is between `r lowersigma` and  `r uppersigma`. The result of the simulation should be similar to the plot in the chapter: on the x-axis there will be standard deviations ranging from `r lowersigma` to `r uppersigma` and the y-axis will show power. Three lines should be drawn to represent the power for the three means considered. (Hint: this exercise just asks you to reuse code from the chapter, with some minimal changes.)

Now answer the following questions:

1. What will be the statistical power if the effect size is `r mu`, sample size is `r n` and standard deviation is `r uppersigma`?
2. What sample size is needed to obtain a statistical power of 0.80 when the effect size is `r mu`,  and standard deviation is `r uppersigma`? 
3. Suppose now that Type I error probability is changed to 0.005. What sample size is needed to obtain a statistical power of 0.80 when the effect size is `r mu`, and standard deviation is `r uppersigma`? 
