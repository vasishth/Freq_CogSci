\mainmatter

# (PART) Foundational ideas {-}

# Some important facts about distributions

In linguistics and psychology, typical data-sets involve either *discrete* dependent measures such as acceptability ratings on a Likert scale (for example, ranging from 1 to 7), and binary grammaticality judgements, or *continuous* dependent measures such as reading times or reaction times in milliseconds and EEG signals in microvolts. 

Whenever we fit a model using one of these types of dependent measures, we make some assumptions about how these measurements were generated. In particular, we usually assume that our observed measurements are coming from a particular *distribution*. The normal distribution is an example that may be familiar to the reader.  

In this chapter, we will learn how to make explicit the assumptions about the distribution associated with our data; we will also learn to visualize distributions. In order to do this, we need to understand the concept of a random variable, which presupposes some basic knowledge of probability theory (such as the sum and product rules). As will become apparent in this chapter, it is extremely useful to be able to think about data in terms of the underlying random variable producing the data. We consider the two cases, discrete and continuous, separately.

We will explain the terms *random variable* and *distribution* below through examples. But it is useful to define the notion of random variable formally.

A random variable, which will be denoted by a variable such as $Y$, is defined as a function from a sample space of possible outcomes $S$ to the real number system: 

\begin{equation}
Y : S \rightarrow \mathbb{R}
\end{equation}

The random variable associates to each outcome $\omega$ in the sample space $S$ ($\omega \in S$) exactly one number $Y(\omega) = y$. $S_Y$ will represent a set that contains all the $y$'s (all the possible values of $Y$, which we call the support of $Y$). We can compactly write: $y \in S_Y$. 

Every random variable $Y$ has associated with it a probability mass (density)  function (PMF, PDF). The term PMF is used for discrete distributions, and PDF for continuous distributions. One distinction between the PMF and PDF is crucial: the PMF maps every element of $S_Y$ to a value between 0 and 1, whereas the PDF maps a range of values $r\in S_Y$ to a value between 0 and 1 (examples are coming up). For both PMFs and PDFs, we will express this as follows:

\begin{equation}
p_Y : S_Y \rightarrow [0, 1] 
\end{equation}

Probability mass functions (discrete case) and probability density functions (continuous case) are functions that assign probabilities (discrete case)  to discrete events (discrete case) or a continuous range of values (continuous case) in a sample space.

The meanings of the terms PMF, PDF, etc., will become clearer as we discuss examples below. The reader to should revisit the above definition themselves when we present some concrete examples of discrete and continuous random variables below.



## Discrete random variables: An example using the Binomial distribution

Imagine that our data come from a grammaticality judgement task (participants see or hear sentences and have to decide whether these are grammatical or ungrammatical), and that the responses from participants are  a sequence of 1's and 0's, where 1 represents the judgment "grammatical", and 0 represents the judgement "ungrammatical". Assume also that each response, coded as 1 or 0, is generated independently from the others. We can simulate the outcome of such an experiment (i.e., a sequence of 1s and 0s) in R. Let's generate the outcome of 20 such experiments with a sample size of 10 (i.e., each experiment provides us with 10 responses). For each experiment, we count the number of 1s (or number of "successes"). The R code that generated this output will be explained soon.

```{r echo=FALSE}
rbinom(10, n = 20, prob = 0.5)
```

Outcomes such as this one (i.e., number of successes for a variable with two possible outcomes)  follow a probability distribution p(Y). In more technical terms, we can say that the number of successes in each of the $20$ simulated experiments above is being generated by a *discrete random variable* $Y$ which has associated with it a probability distribution $p(Y)$ called the **Binomial distribution**.^[When an experiment consists of only a single trial (i.e., we can have a total number of only 0 or 1 successes), $p(Y)$ is called a **Bernoulli distribution**.]   

As mentioned above, for a discrete random variable, the probability distribution $p(Y)$ is called a **probability mass function** (PMF). The PMF defines the probability of each possible outcome. In the above example, with $n=10$ trials, there are 11 possible outcomes:  $0,\dots,10$ successes. Which of these outcomes is most probable depends on a numerical value called a *parameter* in the Binomial distribution that represents the probability of success. We will call this parameter $\theta$; because it represents a probability, it must have a value between 0 and 1.  The left-hand side plot in Figure \@ref(fig:binomplot) shows an example of a Binomial PMF with $10$ trials and the parameter $\theta$ fixed at value $0.5$. Setting $\theta$ to 0.5 leads to a PMF where the most probable outcome is 5 successes out of 10. If we had set $\theta$ to, say 0.1, then the most probable outcome would be 1 success out of 10; and if we had set $\theta$ to 0.9, then the most probable outcome would be 9 successes out of 10.

As we will see later, when we analyze data, a primary goal will be to compute a so-called *estimate* of the parameter (here, $\theta$). In real-life situations, the value of the $\theta$ parameter will be unknown and unknowable; the data will allow us to compute a "guess" about the unknown value of the parameter. We will call this guess the estimate of the parameter. 

```{r, binomplot,echo=FALSE,fig.cap="Probability mass functions of a binomial distribution assuming 10 trials, with 50%, 10%, and 90% probability of success." }
op <- par(mfrow = c(1, 3), pty = "s")
plot(0:10, dbinom(0:10, size = 10, prob = 0.5),
  xlab = "possible outcomes",
  ylab = "probability",
  main = expression(paste(theta, "=0.5", sep = ""))
)
plot(0:10, dbinom(0:10, size = 10, prob = 0.1),
  xlab = "possible outcomes",
  ylab = "probability", main = expression(paste(theta, "=0.1", sep = ""))
)
plot(0:10, dbinom(0:10, size = 10, prob = 0.9),
  xlab = "possible outcomes",
  ylab = "probability",
  main = expression(paste(theta, "=0.9", sep = ""))
)
```

The probability mass function for the binomial is written as follows. 

\begin{equation}
\hbox{Binomial}(k|n,\theta) = 
\binom{n}{k} \theta^{k} (1-\theta)^{n-k}
\end{equation}

Here, $n$ represents the total number of trials, $k$ the number of successes, and $\theta$ the probability of success. The term $\binom{n}{k}$, pronounced n-choose-k, represents the number of ways in which one can choose $k$ successes out of $n$ trials. For example, 1 success out of 10 can occur in 10 possible ways: the very first trial could be a 1, the second trial could be a 1, etc.
The term $\binom{n}{k}$ expands to $\frac{n!}{k!(n-k)!}$; the exclamation mark is the factorial (e.g., $3!=3\times 2\times 1$). In `R`, $\binom{n}{k}$ is computed using the function `choose(n,k)`, with $n$ and $k$ representing positive integer values.

### The mean and variance of the Binomial distribution

It is possible to analytically compute the mean and variance of the PMF associated with the Binomial random variable $Y$. Without getting into the details of how these are derived mathematically, we just state here that the mean of $Y$ (also called the expectation, conventionally written $E[Y]$) and variance of $Y$ (written $Var(Y)$) of a Binomial distribution with parameter $\theta$ and $n$ trials are $E[Y] = n\theta$ and $Var(Y) = n\theta (1-\theta)$. 

Of course, we always know $n$ (because we decide on the number of trials ourselves), but in real experimental situations we never know the true value of $\theta$. But $\theta$ can be estimated from the data. From the observed data, we can compute the estimate of $\theta$, $\hat \theta=k/n$. The quantity $\hat \theta$ is the observed proportion of successes, and is called the **maximum likelihood estimate** of the true (but unknown mean). Once we have estimated $\theta$ in this way, we can also obtain an estimate (also a maximum likelihood estimate) of the variance by computing $n\hat\theta (1-\hat\theta)$. These estimates are then used for statistical inference. 

What does the term "maximum likelihood estimate" mean? The term **likelihood** refers to the value of the Binomial distribution function  for a particular value of $\theta$, once we have observed some data. For example, suppose you record $n=10$ trials, and observe $k=7$ successes. What is the probability of observing $7$ successes out of $10$? We need the binomial distribution to compute this value:

\begin{equation}
\hbox{Binomial}(k=7|n=10,\theta) = 
\binom{10}{7} \theta^{7} (1-\theta)^{10-7}
\end{equation}

Once we have observed the data, both $n$ and $k$ are fixed. The only variable in the above equation now is $\theta$: the above function is now only dependent on the value of $\theta$. When the data are fixed, the probability mass function is only dependent on the value of the parameter $\theta$, and is called a **likelihood function**. It is therefore often expressed as a function of $\theta$:

$p( y | \theta ) = p( k=7, n=10 | \theta) = \mathcal{L}(\theta)$

The vertical bar notation above should be read as saying that, given some data $y$ (which in  the binomial case will be $k$ "successes" in $n$ trials), the function returns a value for different values of $\theta$. 

If we now plot this function for all possible values of $\theta$, we get the plot shown in Figure \@ref(fig:binomlik). We will show below how to compute this function for different values of $\theta$.

```{r, binomlik,echo=FALSE,fig.cap="The likelihood function for 7 successes out of 10." }
theta <- seq(0, 1, by = 0.001)
plot(theta, dbinom(7, size = 10, prob = theta),
  xlab = expression("theta"), ylab = "probability",
  main = "Likelihood function", type = "l"
)
abline(v = 0.7)
text(0.7, 0.1, "Max. value at: \n 0.7")
```

What is important about this plot is that it shows that, given the data, the maximum point is at the point $0.7$, which corresponds to the estimated mean using the formula shown above: $k/n = 7/10$. Thus, the maximum likelihood estimate (MLE) gives us the most likely value of the parameter $\theta$ given the data. It is crucial to note here that the phrase "most likely" does not mean that the MLE from a *particular* sample of data invariably gives us an accurate estimate of $\theta$. For example, if we run our experiment for $10$ trials and get $1$ success out of $10$, the MLE is $0.10$. We could have happened to observe only one success out of ten even if the true $\theta$ were $0.5$. The MLE would however give an accurate estimate of the true parameter $\theta$ as $n$ approaches infinity.

### What information does a probability distribution provide?

What good is a probability mass function? We consider this question next. A lot of important information can be extracted from a PMF.

#### Compute the probability of a particular outcome (discrete case only)

The Binomial distribution shown in Figure \@ref(fig:binomplot) already displays the probability of each possible outcome under a different value for $\theta$. In `R`, there is a built-in function that allows us to calculate $P(Y=k)$, the probability in the random variable $Y$ of $k$ successes out of $n$, given a particular value of $k$ (this number constitutes our data), the number of trials $n$, and given a particular value of $\theta$; this is the ```dbinom``` function. For example, the probability of 5 successes out of 10 when $\theta$ is $0.5$ is:

```{r dnormexample1,echo=TRUE}
(prob <- dbinom(5, size = 10, prob = 0.5))
```

To be completely explicit, we can write this probability as $P(k=5|n=10,\theta=0.5)=`r round(prob,3)`$; when $n$ and $\theta$ are clear from context, we can also write simply $P(k=5)$.

The probabilities of success when $\theta$ is 0.1 or 0.7 can be computed by replacing 0.5 above by each of these probabilities:

```{r}
dbinom(5, size = 10, prob = 0.1)
dbinom(5, size = 10, prob = 0.7)
```

One can alternatively run the above command in one shot; one can give a range of probabilities, and get the probability of 5 successes out of 10 for these different probabilities:

```{r}
dbinom(5, size = 10, prob = c(0.1, 0.7))
```

The above command prints out the probability of 5 successes out of 10, when $\theta=0.1$ and $\theta=0.7$.

#### Compute the cumulative probability of k or less (more) than k successes 

Instead of the probability of obtaining a given number of successes we could be interested in knowing the cumulative probability of obtaining 1 or less, or 2 or less successes. Formally, we will write this cumulative probability as $F(2)$, and more generally, as $F(k)$, for a particular value of $k$. Thus, $F(k)=P(Y\leq k)$.

We can compute this cumulative probability with the ```dbinom``` function, through a simple summation procedure:

```{r cdfbinom1,echo=TRUE}
## the cumulative probability of obtaining
## 0, 1, or 2 successes out of 10,
## with theta=0.5:
dbinom(0, size = 10, prob = 0.5) + dbinom(1, size = 10, prob = 0.5) +
  dbinom(2, size = 10, prob = 0.5)
```

Mathematically, we could write the above summation as: 

\begin{equation}
\sum_{k=0}^2 \binom{n}{k} \theta^{k} (1-\theta)^{n-k} 
\end{equation}

An alternative to the cumbersome addition in the R code above is this more compact statement, which closely mimics the above mathematical expression:

```{r}
sum(dbinom(0:2, size = 10, prob = 0.5))
```

`R` has a built-in function called ```pbinom``` that does this summation for us.  If we want to know the probability of $2$ or less successes as in the above example, we can write:

```{r pbinomexample1,echo=TRUE}
pbinom(2, size = 10, prob = 0.5, lower.tail = TRUE)
```

The specification ```lower.tail=TRUE``` ensures that the summation goes from $2$ to numbers smaller than $2$ (which lie in the lower tail of the distribution in Figure \@ref(fig:binomplot)). If we wanted to know what the probability is of obtaining $2$ or more successes out of $10$, we can set ```lower.tail``` to ```FALSE```:

```{r pbinomexample2,echo=TRUE}
pbinom(2, size = 10, prob = 0.5, lower.tail = FALSE)
```

The cumulative distribution function or CDF, $F(k)$, can be plotted by computing the cumulative probabilities for any value $k$ or less than $k$, where $k$ ranges from $0$ to $10$ in our running example. The CDF is shown in Figure \@ref(fig:binomcdf).


```{r, binomcdf,echo=FALSE,fig.cap="The cumulative distribution function for a binomial distribution assuming 10 trials, with 50% probability of success." }
# op<-par(mfrow=c(1,2),pty="s")
plot(0:10, pbinom(0:10, size = 10, prob = 0.5),
  xlab = "Possible outcomes k",
  ylab = "Prob. of k or less successes",
  main = "Cumulative distribution function"
)
```

#### Compute the inverse of the cumulative distribution function (the quantile function)

We can also find out the value of the variable $k$ (the quantile) such that the probability of obtaining $k$ or less than $k$ successes is some specific probability value $p$. If we switch the x and y axes of Figure \@ref(fig:binomcdf), we obtain another very useful function, the inverse CDF. 

```{r, eval=FALSE,binominvcdf,echo=FALSE,fig.cap="The inverse cumulative distribution function for a binomial distribution assuming 10 trials, with 50% probability of success." }

plot(0:10 ~ pbinom(0:10, size = 10, prob = 0.5),
  ylab = "Possible outcomes k",
  xlab = "Prob. of k or less successes",
  main = "Cumulative distribution function"
)
```

The inverse of the CDF (known as the quantile function in R because it returns the quantile, the value k) is available in R as the function ```qbinom```. The usage is as follows: to find out what the value $k$ of the outcome is such that the probability of obtaining $k$ or less successes is $0.37$, type:

```{r qbinomexample,echo=TRUE}
qbinom(0.37, size = 10, prob = 0.5)
```

#### Generate random data from a $\hbox{Binomial}(n,\theta)$ distribution

We can generate random simulated data from a Binomial distribution by specifying the number of trials and the probability of success $\theta$. In `R`, we do this in the following way:

```{r rbinomexample,echo=TRUE}
rbinom(n = 10, size = 1, prob = 0.5)
```

The above code generates a sequences of 10 randomly generated $1$'s and $0$'s. Each of the 1's and 0's is called an outcome coming from a Bernoulli distribution. In other words, a Bernoulli distribution is just a Binomial distribution with size=1, i.e., a single trial.

Repeatedly run the above code; you will get different sequences of 1's and 0's each time. For each generated sequence, one can calculate the number of successes by just summing up the vector, or computing its mean and multiplying by the number of trials, here $10$:


```{r rbinomexamplemean,echo=TRUE}
y <- rbinom(n = 10, size = 1, prob = 0.5)
mean(y) * 10
sum(y)
```

We can think of the above as 10 Bernoulli trials, or a single Binomial trial of size 10. When we see the data in this way,  the syntax and output changes: 

```{r}
(y <- rbinom(n = 1, size = 10, prob = 0.5))
```

Seen as a Binomial experiment with size 10, the function returns the number of successes in a particular experiment with size 10.

Next, let's take a look at an example of a continuous random variable.

## Continuous random variables: An example using the Normal distribution  

We will now revisit the idea of a random variable using a continuous distribution as an example. Imagine that you have reading time data, represented as a vector $y$. The data $y$ are measured in milliseconds and are assumed to come from (i.e., assumed to be generated by) a random variable $Y$ that has as PDF the Normal distribution (this is not a realistic assumption; we will come up with a more realistic assumption later). We will write the above statement compactly as follows:

\begin{equation}
Y \sim Normal(\mu,\sigma)
\end{equation}

This expression should be read as: the random variable $Y$ has PDF $Normal(\mu,\sigma)$. The random variable $Y$ refers to an abstract mathematical object; specific instances of observed (or simulated) data will be referred to with the lower-case equivalent of the random variable; here, $y$ can refer to a particular data-point or a vector of data (the context will make it clear whether we are talking about a single data point or a vector of data). A further important notational detail: in statistics textbooks, you will often see the Normal distribution represented in terms of the mean and the variance ($\sigma^2$) instead of the standard deviation $\sigma$ that we use here. We have consciously chosen to use the standard deviation in the Normal distribution; this is because `R` uses the standard deviation and not the variance to define a normal distribution. 

An important assumption we make here is that each data point in the vector of data $y$ is independent of the others. Often, we express this assumption by saying that we have "independent and identically distributed data"; this is abbreviated as "i.i.d". One can be very explicit about this and write:

\begin{equation}
Y \stackrel{iid}{\sim}  Normal(\mu,\sigma)
\end{equation}

The probability density function (PDF) of the Normal distribution is defined as follows:

\begin{equation}
Normal(y|\mu,\sigma)=f(y)= \frac{1}{\sqrt{2\pi \sigma^2}} \exp \left(-\frac{(y-\mu)^2}{2\sigma^2} \right)
\end{equation}

Here, $\mu$ is the mean, and $\sigma$ is the standard deviation of the Normal distribution that the reading times have been sampled from. To display this function, we would have to decide on specific values for $\mu$ and $\sigma$ and then input the $y$ value into the  function to plot it. It may be instructive to write this  function "by hand" and then plot it. We just choose some default values for $\mu$ and $\sigma$ here.

```{r mynormdistrn,echo=TRUE,fig.cap="Creating one's own Normal distribution function." }
my_normal <- function(y = NULL, mu = 500, sigma = 100) {
  (1 / sqrt(2 * pi * sigma^2)) * exp(-(y - mu)^2 / (2 * sigma^2))
}
y <- seq(100, 900, by = 0.01)
plot(y, my_normal(y = y), type = "l")
```

Just like the `dbinom` function we saw earlier, there is a  built-in function in `R` called `dnorm` that will allow us to display the Normal distribution.
Using ```dnorm```, we can visualize the Normal distribution for particular values of $\mu$ and $\sigma$ as a PDF. Analogously to the discrete example we saw earlier, we can also plot the corresponding CDF (using ```pnorm```), and the inverse CDF (using ```qnorm```). See Figure \@ref(fig:normdistrn). 

In the continuous case, the PDF gives us something called the *density* for each possible value of our data $y$; "density" here is not the probability, rather is it giving us a non-negative number that is the value of the function $f(y)$ in the equation immediately above. We will return to the concept of density when we do an exercise on *maximum likelihood estimation*.

As in the discrete case, the CDF tells us the probability of observing a value like y or some value less than that (written: $P(Y<y)$); and the inverse CDF gives us the quantile $y$ such that $P(Y<y)$ is some specific value between 0 and 1.   The PDF, CDF, and inverse CDF are three different ways of looking at the same information. 

```{r, normdistrn,echo=FALSE,fig.cap="The PDF, CDF, and inverse CDF for the $Normal(\\mu=0,\\sigma=1)$." }
op <- par(mfrow = c(1, 3), pty = "s")
plot(function(y) dnorm(y, mean = 0, sd = 1), -3, 3,
  main = "PDF of Y ~ Normal(0,1)",
  ylab = "density", xlab = "y"
)
plot(function(y) pnorm(y, mean = 0, sd = 1), -3, 3,
  main = "CDF of Y ~ Normal(0,1)",
  ylab = "Prob(Y<y)", xlab = "y"
)
plot(function(y) qnorm(y, mean = 0, sd = 1), 0, 1,
  main = "Inverse CDF of Y ~ Normal(0,1)",
  ylab = "y", xlab = "P(Y<y)"
)
```

One important fact about the normal distribution is that 95% of the probability mass is covered by approximately plus/minus 1.96 times the standard deviation about the mean. Thus,  the range $\mu\pm 1.96\times \sigma$ will cover approximately 95% of the area under the curve. We will approximate this by talking about $\mu\pm 2\times \sigma$.

As in the discrete example, the PDF, CDF, and inverse of the CDF allow us to ask questions like:

* **What is the probability of observing values between some range $a$ and $b$ from a Normal distribution with mean $\mu$ and standard deviation $\sigma$**? We can compute the probability of the random variable lying between 1 and minus infinity:

```{r pnormexample}
pnorm(1, mean = 0, sd = 1)
```

```{r echo=FALSE}
## function for plotting area under curve:
plot.prob <- function(x,
                      x.min,
                      x.max,
                      prob,
                      mean,
                      sd,
                      gray.level, main) {
  plot(x, dnorm(x, mean, sd),
    type = "l", xlab = "",
    ylab = "", main = main
  )
  abline(h = 0)

  ## shade X<x
  x1 <- seq(x.min, qnorm(prob), abs(prob) / 5)
  y1 <- dnorm(x1, mean, sd)

  polygon(c(x1, rev(x1)),
    c(rep(0, length(x1)), rev(y1)),
    col = gray.level
  )
}

shadenormal <-
  function(prob = 0.5,
           gray1 = "black",
           x.min = -6,
           x.max = abs(x.min),
           x = seq(x.min, x.max, 0.01),
           mean = 0,
           sd = 1, main = "P(X<0)") {
    plot.prob(
      x = x, x.min = x.min, x.max = x.max,
      prob = prob,
      mean = mean, sd = sd,
      gray.level = gray1, main = main
    )
  }
```

```{r,auc,echo=FALSE,fig.cap="The area under the  curve between 1 and minus infinity in a Normal distribution with mean 0 and standard deviation 1." }
shadenormal(prob = 0.84134, main = "X~Normal(0,1); P(X<1)")
```


Notice here that the probability of any point value in a PDF is always 0. This is because the probability in a continuous probability distribution is defined to be the area under the curve, and the area under the curve at any single point on the x-axis is always 0. The implication here is that we can only ask about probabilities between a range of values; e.g., the probability that $Y$ lies between $a$ and $b$, or $P(a<Y<b)$, where $a\neq b$. Also, notice that $P(a<Y<b)$ and $P(a\leq Y\leq b)$ will be the same probability, because of the fact that $P(Y=a)$ or $P(Y=b)$ both equal 0.

* **What is the quantile $q$ such that the probability is $p$ of observing that value $q$ or something less (or more) than it**? For example, we can work out the quantile $q$ such that the probability of observing $q$ or something less than it is 0.975, in the Normal(500,100) distribution. Formally, we would write this as $F(a)=P(Y<a)$.

```{r qnormexample}
qnorm(0.975, mean = 500, sd = 100)
```

The above output says that the probability that the random variable is less than $q=696$ is 97.5%.

* **Generating simulated data**. Given a vector of $n$ independent and identically distributed data $y$, i.e., given that each data point is being generated independently from  $Y \sim Normal(\mu,\sigma)$ for some values of the parameters $\mu,\sigma$, the maximum likelihood estimates for the expectation and variance are

\begin{equation}
\bar{y} =  \frac{\sum_{i=1}^n y_i}{n} 
\end{equation}

\begin{equation}
Var(y) = \frac{\sum_{i=1}^n (y_i-
\bar{y})^2}{n}
\end{equation}

As an aside, keep in mind that the formula for variance in `R` is slightly different: it divides by $n-1$ instead of $n$. 

\begin{equation}
Var(y) = \frac{\sum_{i=1}^n (y_i-
\bar{y})^2}{n-1}
\end{equation}

The reason for this difference is simply that division by $n-1$ leads to a so-called unbiased estimate of the variance. 

Let's simulate some sample data from a Normal distribution and compute estimates of the mean and variance.   
Let's generate $10$ data points using the ```rnorm``` function, and then compute the mean and variance from the simulated data:

```{r rnormexample}
y <- rnorm(10, mean = 500, sd = 100)
mean(y)
var(y)
```

As mentioned earlier, depending on the sample size, the sample mean and sample variance *from a particular sample* may or may not be close to the true values of the respective parameters, despite the fact that these are *maximum likelihood estimates*. The estimates of the mean and variance from a particular sample will be close to the true values of the parameters as the sample size goes to infinity.

## Other common distributions

Here, we briefly present some of the distributions that we will need or use in this book. Throughout, we assume independent and identically distributed data.

### The standard normal: $\mathit{normal}(\mu=0,\sigma=1)$

Although we have already seen this distribution, the standard normal distribution is so important for us that it deserves its own section.

The standard normal is a special case of the normal distribution and is written $\mathit{normal}(0,1)$.

One important fact that is relevant for us in this book is that if $X_1, \dots, X_n$ are independent and identically distributed random variables from a distribution with mean $\mu$ and variance $\sigma^2$, then, as $n$ approaches infinity, the distribution of the transformed random variable $Z$ is:

\begin{equation}
Z = \frac{\bar{X} - \mu}{\sigma/\sqrt{n}}
\end{equation}

$\bar{X}$ is the mean of the random variables $X_1,\dots, X_n$: $\bar{X}=\sum_{i=1}^n \frac{X_i}{n}$.

The pdf of the standard normal is just like the $\mathit{normal}(\mu,\sigma)$ we saw above, with $\mu = 0$ and $\sigma = 1$.

\begin{equation}
f(z)= \frac{1}{\sqrt{2\pi}} \exp \left(-\frac{z^2}{2} \right)
\end{equation}

### The uniform distribution

A continuous random variable $U$ has a uniform distribution if the pdf of $U$ is:

\begin{equation}
f(u; \alpha, \beta)=
\begin{cases}
\frac{1}{\beta-\alpha} &  for \alpha < u < \beta ,\\
0 & \hbox{otherwise}
\end{cases}
\end{equation}



The mean and variance of the $\mathit{uniform}(\alpha,\beta)$ are:

\begin{equation}
\mu = \frac{\alpha + \beta}{2} \hbox{  and  }  
\sigma^2 = \frac{1}{12} (\beta-\alpha)^2
\end{equation}

Figure \@ref(fig:uniformdistrn) shows a $\mathit{uniform}(\alpha=0,\beta=1)$  distribution.

```{r uniformdistrn,message="FALSE",warning=FALSE,results="asis",fig.cap="The pdf of the uniform(0,1) distribution."}
u<-seq(0,1,by=0.001)
plot(u,dunif(u,min=0,max=1),type="l",
     ylab="density",main="uniform(0,1)")
```

The cdf $F(U<u)$ has the property that $F(U<u)=u$:

```{r uniformcdf2}
punif(0.25,min=0,max=1)
punif(0.75,min=0,max=1)
```

Figure \@ref(fig:uniformcdf1) shows the cdf for the distribution uniform(0,1).

```{r uniformcdf1,echo=FALSE,fig.cap="The cumulative distribution function of the uniform distribution uniform(0,1)."}
plot(u,punif(u,min=0,max=1),type="l",
     ylab="probability",main="CDF of uniform(0,1)")
```


### The Chi-square distribution

A random variable $X$ that has a chi-square distribution has the following pdf:

\begin{equation}
f(u; \nu)=
\begin{cases}
\frac{1}{2^{\nu/2} \Gamma(\nu/2)}x^{\frac{\nu-2}{2}\exp(-\frac{x}{2})} &  for x>0 ,\\
0 & \hbox{otherwise}
\end{cases}
\end{equation}

The parameter $\nu$ is referred to as the degrees of freedom. $\Gamma()$ is the Gamma function: $\Gamma(n)=(n-1)!$. 

The mean and variance of $\chi^2_{\nu}$ are:

\begin{equation}
\mu = \nu \hbox{  and  }  
\sigma^2 = \nu^2
\end{equation}

Figure \@ref(fig:chisqdistrn) shows a $\chi^2_{\nu=1}$  distribution.

```{r chisqdistrn,message="FALSE",warning=FALSE,results="asis",fig.cap="The pdf of the Chi-square distribution with 1 degree of freedom."}
x<-seq(0,4,by=0.001)
plot(x,dchisq(x,df=1),type="l",
     ylab="density",main="Chi-square(df=1)")
```

An important fact about the $\chi^2_{\nu=1}$ distribution that we will use in chapter 4 is that the quantile $x$ such that $F(X>x)=0.05$ is $x=3.84$.

```{r}
qchisq(0.05,df=1,lower.tail=FALSE)
```


### The t-distribution

A random variable $T$ that has a t-distribution has the pdf:

\begin{equation}
f(t) = \frac{\Gamma(\frac{\nu+1}{2})}{\sqrt{\pi\nu}\Gamma(\frac{\nu}{2})}\left(1+\frac{t^2}{\nu}\right)^{-\frac{\nu + 1}{2}} \hbox{ for } -\infty < t < \infty
\end{equation}

This distribution is also called the Student's t-distribution; the creator of the t-distribution, Gosset, wrote papers under the name Student.

This continuous distribution, often written $t(n-1)$, takes as a parameter the degrees of freedom $\nu=n-1$, where $n$ is the sample size (what constitutes sample size will be made more precise in the next chapter).  As the degrees of freedom approaches infinity (as the sample size approaches infinity), the distribution approaches the Normal distribution with mean 0 and standard deviation 1.

The mean of the t-distribution is $0$ if $n>1$ and variance $\frac{n}{n-2}$ if $n>2$.

The t-distribution becomes the Cauchy distribution if $n=2$. The Cauchy distribution has no mean or variance defined for it.

The t-distribution is actually related to the chi-square and standard normal distributions.
If $Y$ and $Z$ are independent random variables and $Y$ has a chi-square distribution with $\nu$ degrees of freedom and $Z$ has the standard normal, then the t-distribution with $\nu$ has the following form:

\begin{equation}
T = \frac{Z}{\sqrt{Y/\nu}}
\end{equation}

There are four functions in `R` that serve the same purpose as the `dnorm`, `pnorm`, `qnorm`, `rnorm` functions for the Normal:
`dt`, `pt`, `qt`, `rt`. These functions, particularly `dt`, `pt`, `qt`, will play a very important role in chapter 2.

### The F distribution

If $U$ and $V$ are independent random variables with a chi-square distribution having degrees of freedom $\nu_1$ and $\nu_2$ respectively, then the following pdf is called an F-distribution:

\begin{equation}
F = \frac{U/\nu_1}{V/\nu_2}
\end{equation}

The pdf of the F-distribution is:

\begin{equation}
g(f) = \frac{\Gamma(\frac{\nu_1 + \nu_2}{2})}{\Gamma(\frac{\nu_1}{2}) \Gamma(\frac{\nu_2}{2}) } \left(\frac{\nu_1}{\nu_2}\right)^{\nu_1/2} f^{\nu_1/2 -1 } (1 + \frac{\nu_1}{\nu_2}f)^{-\frac{1}{2}(\nu_1 + \nu_2)}
\end{equation}


for $f>0$. The pdf $g(f)$ has value $0$ for all other values of $f$. 


## Bivariate and multivariate distributions

So far, we have only discussed univariate distributions. It is also possible to specify distributions with two or more dimensions. 

Understanding bivariate (and, more generally, multivariate) distributions, and knowing how to simulate data from such distributions, is vital for us because linear mixed models crucially depend on such distributions. If we want to understand linear mixed models, we have to understand multivariate distributions.

### Example 1: Discrete bivariate distributions

Starting with the discrete case, consider the discrete bivariate distribution shown below. These are data from an experiment  where, inter alia, in each trial a Likert acceptability rating and a response accuracy (to a yes-no question) were recorded (the data are from a study by @AnnaLphd, used with permission here).

Figure \@ref(fig:bivardiscrete) shows the *joint probability mass function* of two random variables X and Y. The random variable X consists of 7 possible values (this is the 1-7 Likert response scale), and the random variable Y is response accuracy, with 0 representing incorrect responses, and 1 representing correct responses. 

```{r bivardiscrete,message="FALSE",warning=FALSE,results="asis",fig.cap="Example of a discrete bivariate distribution. In these data, in every trial, two pieces of information were collected: Likert responses and yes-no question responses. The random variable X represents Likert scale responses on a scale of 1-7. and the random variable Y represents 0, 1 (incorrect, correct) responses to comprehension questions."}
library(bivariate)
library(lingpsych)
data("df_discreteagrmt")
rating0 <- table(subset(df_discreteagrmt, accuracy == 0)$rating)
rating1 <- table(subset(df_discreteagrmt, accuracy == 1)$rating)

ratingsbivar <- data.frame(
  rating0 = rating0,
  rating1 = rating1
)

ratingsbivar <- ratingsbivar[, c(2, 4)]
colnames(ratingsbivar) <- c(0, 1)
library(MASS)

## function from bivariate package:
f <- bivariate::gbvpmf(as.matrix(ratingsbivar))
plot(f, TRUE,
  arrows = FALSE
)
```

One can also display the figure as a table. 

```{r}
probs <- attr(f, "p")
t(probs)
```

For each possible value of X and Y, we have a joint probability. Given such a bivariate distribution, there are two useful quantities we can compute: the *marginal* distributions ($p_{X}$ and $p_Y$), and the *conditional* distributions ($p_{X|Y}$ and $p_{Y|X}$).  
The table below shows the joint probability mass function $p_{X,Y}(x,y)$.

\begin{table}[!htbp] 
\begin{center}
\begin{tabular}{c|ccccccc}
$p_{X,Y}$ & x=1 & x=2 & x=3 & x=4 & x=5 & x=6 & x=7\\
\hline
y = 0 & 0.018 & 0.023 & 0.040 & 0.043 & 0.063 & 0.049 & 0.055\\
y = 1 & 0.031 & 0.053 & 0.086 & 0.096 &  0.147 & 0.153 &  0.142\\
\end{tabular}
\end{center}
\caption{The joint PMF for two random variables $X$ and $Y$.}\label{discretebivartable}
\end{table}

The marginal  distribution $p_Y$ is defined as follows. $S_{X}$ is the support of X, i.e., all the possible values of X.

\begin{equation}
p_{Y}(y)=\sum_{x\in S_{X}}p_{X,Y}(x,y).\label{eq-marginal-pmf}
\end{equation}	

Similarly, the marginal distribution $p_X$ is defined as:

\begin{equation}
p_{X}(x)=\sum_{y\in S_{Y}}p_{X,Y}(x,y).\label{eq-marginal-pmf2}
\end{equation}	

$p_Y$ is easily computed, by summing up the values in each row; and $p_X$ by summing up the values in each column. You can see why this is called the marginal distribution; the result appears in the margins of the table.

```{r}
# P(Y)
(PY <- rowSums(t(probs)))
sum(PY) ## sums to 1
# P(X)
(PX <- colSums(t(probs)))
sum(PX) ## sums to 1
```

The marginal probabilities sum to 1, as they should. The table below shows the marginal probabilities.

\small
\begin{table}[!htbp]
\begin{center}
\begin{tabular}{c|ccccccc|c}
$p_{X,Y}$ & x=1 & x=2 & x=3 & x=4 & x=5 & x=6 & x=7 & P(Y)\\
\hline
y = 0 & 0.018 & 0.023 & 0.040 & 0.043 & 0.063 & 0.049 & 0.055 &  \textbf{0.291}\\
y = 1 & 0.031 & 0.053 & 0.086 & 0.096 &  0.147 & 0.153 &  0.142 &  \textbf{0.709}\\
\hline
P(X) & \textbf{0.049} & \textbf{0.077} & \textbf{0.126} & \textbf{0.139} & \textbf{0.210} & \textbf{0.202} & \textbf{0.197}\\
\end{tabular}
\end{center}
\caption{The joint and marginal distributions of X and Y.}\label{discretebivartable2}
\end{table}
\normalsize

Notice that to compute the marginal distribution of X, one is summing over all the Ys; and to compute the marginal distribution of Y, one sums over all the X's. We say that we are *marginalizing out* the random variable that we are summing over. One can visualize the two marginal distributions using barplots. 

```{r marginalprobs,fig.cap="Marginal distributions of X and Y.",echo=FALSE}
op <- par(mfrow = c(1, 2), pty = "s")
names(PX) <- 1:7
barplot(PX, main = "P(X)")
barplot(PY, main = "P(Y)")
```

For computing conditional distributions, recall that the  conditional distribution of a random variable $X$ given that $Y=y$, where $y$ is some specific (fixed) value, is:

\begin{equation}
p_{X\mid Y} (x\mid y) = \frac{p_{X,Y}(x,y)}{p_Y(y)}	\quad \hbox{provided } p_Y(y)=P(Y=y)>0
\end{equation}

As an example, let's consider how $p_{X\mid Y}$ would be computed.
The possible values of $y$ are $0,1$, and so we have to find the conditional distribution (defined above) for each of these values. I.e., we have to find $p_{X\mid Y}(x\mid y=0)$, and $p_{X\mid Y}(x\mid y=1)$.

Let's do the calculation for $p_{X\mid Y}(x\mid y=0)$.

\begin{equation}
\begin{split}
p_{X\mid Y} (1\mid 0) =& \frac{p_{X,Y}(1,0)}{p_Y(0)}\\
	=&  \frac{0.018}{0.291}\\
	=& `r fractions(0.018/0.291)`
\end{split}	
\end{equation}

This conditional probability value will occupy the cell X=1, Y=0 in the table below summarizing the conditional probability distribution $p_{X|Y}$. In this way, one can fill in the entire table, which will then represent the conditional distributions $p_{X|Y=0}$ and $p_{X|Y=1}$. The reader may want to take a few minutes to complete the table.  

\begin{table}[!htbp]
\begin{center}
\begin{tabular}{c|ccccccc}
	& x=1 & x=2 & x=3 & x=4 & x=5 & x=6 & x=7\\ 
\hline	
$p_{X\mid Y}(x\mid y=0)$  & `r fractions(0.018/0.291)` &  & & & & & \\
$p_{X\mid Y}(x\mid y=1)$  &  &  & & & & &  \\
\end{tabular}
\end{center}
\caption{The conditional probability distribution of X given Y.}
\label{XgivenY}
\end{table}

Similarly, one can construct a table that shows $p_{Y|X}$.

### Example 2: Continuous bivariate distributions

Consider now the continuous bivariate case; this time, we will use simulated data. Consider two normal random variables $X$ and $Y$, each of which coming from, for example, a Normal(0,1) distribution, with some correlation $\rho$ between the two random variables. 

A bivariate distribution for two random variables $X$ and $Y$, each of which comes from a normal distribution, is expressed in terms of the means and standard deviations of each of the two distributions, and the correlation $\rho$ between them. The standard deviations and correlation are expressed in a special form of a $2\times 2$ matrix called a variance-covariance matrix $\Sigma$. If $\rho_u$ is the correlation between the two random variables, and $\sigma _{x}$ and $\sigma _{y}$ the respective standard deviations, the variance-covariance matrix is written as:

\begin{equation}\label{eq:covmatfoundations}
\Sigma
=
\begin{pmatrix}
\sigma _{x}^2  & \rho_u\sigma _{x}\sigma _{y}\\
\rho_u\sigma _{x}\sigma _{y}    & \sigma _{y}^2\\
\end{pmatrix}
\end{equation}

The off-diagonals of this matrix contain the covariance between  $X$ and $Y$. 

The joint distribution of $X$ and $Y$ is defined as follows:

\begin{equation}\label{eq:jointpriordistfoundations}
\begin{pmatrix}
  X \\ 
  Y \\
\end{pmatrix}
\sim 
\mathcal{N}_2 \left(
\begin{pmatrix}
  0 \\
  0 \\
\end{pmatrix},
\Sigma
\right)
\end{equation}

The subscript on $\mathcal{N}_2$ refers to the number of dimensions; if we had a multivariate distribution with three random variables, say X, Y, Z, the distribution would be $\mathcal{N}_3$, and so on. The variance-covariance matrix for the three-dimensional distribution would be a $3\times 3$ matrix, not a $2\times 2$ matrix as above, and would contain three correlations ($\rho_{X,Y},\rho_{X,Z},\rho_{Y,Z}$).

Returning to the bivariate example, the joint PDF is written with reference to the two variables $f_{X,Y}(x,y)$. It has the property that the area under the curve sums to 1. Formally, we would write this as a double integral: we are summing up the area under the curve for both dimensions X and Y. The integral symbol ($\int$) is just the continuous equivalent of the discrete summation symbol ($\sum$).

\begin{equation}
\iint_{S_{X,Y}} f_{X,Y}(x,y)\, dx dy = 1
\end{equation}

Here, the terms $dx$ and $dy$ express the fact that we are summing the area under the curve along the X axis and the Y axis.

\begin{equation}
f_{X,Y}(x,y) = \frac{\exp(-\frac{1}{2(1-\rho^2)}\left[(\frac{x-\mu_1}{\sigma_1})^2 -2 \rho(\frac{x-\mu_1}{\sigma_1})(\frac{y-\mu_2}{\sigma_2}) + (\frac{y-\mu_2}{\sigma_2})^2  \right])}{2\pi \sigma_1\sigma_2\sqrt{1-\rho^2} }
\end{equation}

where $-\infty < x < \infty$ and $-\infty < y < \infty$. The parameters are constrained as follows: $\sigma_1, \sigma_2 > 0$, and $-1 < \rho < 1$.  

The joint CDF would be written as follows. The equation below gives us the probability of observing a value like $(u,v)$ or some value smaller than that (i.e., some $(u',v')$, such that $u'<u$ and $v'<v$.

\begin{equation}
\begin{split}
F_{X,Y}(u,v) =& P(X<u,Y<v)\\
             =& \int_{-\infty}^u \int_{-\infty}^v f_{X,Y}(x,y)\, dy dx \hbox{ for } (x,y)\in \mathbb{R}^2\\
\end{split}
\end{equation}

As an aside, notice that the support for the normal distribution ranges from minus infinity to plus infinity.  There can however be other PDFs with a more limited support; an example would be a normal distribution whose pdf $f(x)$ is such that the lower bound is truncated at, say, 0. In such a case, the area under the range $\int_{-\infty}^0 f(x) \, dx$ will be 0 because the range lies outside the support of the truncated normal distribution. 

A visualization will help. The figures below show a bivariate distribution with correlation zero (Figure \@ref(fig:zerocor)), a positive (Figure \@ref(fig:poscor)) and a negative correlation (Figure \@ref(fig:negcor)).

```{r zerocor,echo=FALSE,fig.cap="A bivariate Normal distribution with zero correlation. Shown are four plots: the top-right plot shows the three-dimensional bivariate density, the top-left plot the contour plot of the distribution (seen from above). The lower plots show the cumulative distribution function from two views, as a three-dimensional plot and as a contour plot."}
f1 <- nbvpdf(0, 0, 1, 1, 0)
# f1 %$% matrix.variances
plot(f1,
  all = TRUE, n = 20,
  main = "No correlation"
)
```

```{r poscor,echo=FALSE,fig.cap="A bivariate Normal distribution with a negative  correlation of -0.6. Shown are four plots: the top-right plot shows the three-dimensional bivariate density, the top-left plot the contour plot of the distribution (seen from above). The lower plots show the cumulative distribution function from two views, as a three-dimensional plot and as a contour plot."}
f3 <- nbvpdf(0, 0, 1, 1, -0.6)
# f1 %$% matrix.variances
plot(f3,
  all = TRUE, n = 20,
  main = "Correlation -0.6"
)
```


```{r negcor,echo=FALSE,fig.cap="A bivariate Normal distribution with a positive correlation of 0.6. Shown are four plots: the top-right plot shows the three-dimensional bivariate density, the top-left plot the contour plot of the distribution (seen from above). The lower plots show the cumulative distribution function from two views, as a three-dimensional plot and as a contour plot."}
f2 <- nbvpdf(0, 0, 1, 1, 0.6)
# f1 %$% matrix.variances
plot(f2,
  all = TRUE, n = 20,
  main = "Correlation 0.6"
)
```

In this book, we will make use of such multivariate distributions a lot, and it will soon become important to know how to generate simulated bivariate or multivariate data that is correlated. So let's look at how to generate simulated data next.

### Generate simulated bivariate (multivariate) data 

Suppose we want to generate 100 correlated pairs of data, with correlation $\rho=0.6$. The two random variables have mean 0, and standard deviations 5 and 10 respectively. 

Here is how we would generate such data. First, define a variance-covariance matrix; then, use the multivariate analog of the `rnorm` function, `mvrnorm`, to generate $100$ data points.

```{r}
library(MASS)
## define a variance-covariance matrix:
Sigma <- matrix(c(5^2, 5 * 10 * .6, 5 * 10 * .6, 10^2),
  byrow = FALSE, ncol = 2
)
## generate data:
u <- mvrnorm(
  n = 100,
  mu = c(0, 0),
  Sigma = Sigma
)
head(u)
```

A plot confirms that the simulated data are positively correlated.

```{r}
plot(u[, 1], u[, 2])
```

As an exercise, try changing the correlation to $0$ or to $-0.6$, and then plot the bivariate distribution that results.

One final useful thing to notice about the variance-covariance matrix is that it can be decomposed into the component standard deviations and an underlying correlation matrix. For example, consider the matrix above:

```{r}
Sigma
```

One can decompose the matrix as follows. The matrix can be seen as the product of a diagonal matrix of the standard deviations and the correlation matrix:

```{r}
## sds:
(sds <- c(5, 10))
## diagonal matrix:
(sd_diag <- diag(sds))
## correlation matrix:
(corrmatrix <- matrix(c(1, 0.6, 0.6, 1), ncol = 2))
```

Given these two matrices, one can reassemble the variance-covariance matrix:

```{r}
sd_diag %*% corrmatrix %*% sd_diag
```

There is a built-in convenience function, `sdcor2cov` in the `SIN` package that does this calculation, taking the vector of standard deviations (not the diagonal matrix) and the correlation matrix to yield the variance-covariance matrix:

```{r}
SIN::sdcor2cov(stddev = sds, corr = corrmatrix)
```

We will be using this function a lot when simulating data from hierarchical models.

## Likelihood and maximum likelihood estimation

We now turn to an important topic: the idea of likelihood, and of maximum likelihood estimation. Consider as a first example the discrete case, using the Binomial distribution.

Suppose we toss a fair coin 10 times, and count the number of heads; we do this experiment once. Notice below that we set the probability of success to be 0.5. This is because we are assuming that we tossed a fair coin.

```{r}
(x <- rbinom(1, size = 10, prob = 0.5))
```

The *probability* of obtaining this value depends on the parameter we set for $\theta$ in the PMF for the binomial distribution. Here are some possible values for $\theta$ and the resulting probabilities:

```{r}
dbinom(x, size = 10, prob = 0.1)
dbinom(x, size = 10, prob = 0.3)
dbinom(x, size = 10, prob = 0.5)
dbinom(x, size = 10, prob = 0.8)
dbinom(x, size = 10, prob = 1.0)
```

The value of $\theta$ that gives us the highest probability will be called the **maximum likelihood estimate**. The function ```dbinom```  (which is a function of $\theta$) is also called a likelihood function, and the maximum value of this function is called the maximum likelihood estimate. We can graphically figure out the maximal value of the ```dbinom``` likelihood function here by plotting the value of the function for all possible values of $\theta$, and checking which is the maximal value:

```{r}
theta <- seq(0, 1, by = 0.01)
plot(theta, dbinom(x, size = 10, prob = theta),
  type = "l",
  ylab = "likelihood/probability"
)
abline(v = x / 10)
```

It should be clear from the figure that the maximum value corresponds to the proportion of heads: `r  x`/10. This value is called the maximum likelihood estimate (MLE). We can obtain this MLE of $\theta$, which  maximizes the likelihood, by computing:

\begin{equation}
\hat \theta = \frac{x}{n}
\end{equation}

where $n$ is sample size, and $x$ is the number of successes. For the analytical derivation of this result, see the Linear Modeling lecture notes:
https://github.com/vasishth/LM



The likelihood function in a continuous case is similar to that of the discrete example above, but there is one crucial difference, which we will just get to below. 

Consider the following example. Suppose we have one data point from a Normal distribution, with mean 0 and standard deviation 1:

```{r}
(x <- rnorm(1, mean = 0, sd = 1))
```

The likelihood function for this data point is going to depend on two parameters, $\mu$ and $\sigma$. For simplicity, let's assume that $\sigma$ is known to be 1 and that only $\mu$ is unknown. In this situation, the value of $\mu$ that maximizes the likelihood will be the MLE. As before, we can graphically find the MLE by plotting the likelihood function:

```{r}
mu <- seq(-3, 3, by = 0.01)
plot(mu, dnorm(x,
  mean = mu,
  sd = 1
), type = "l")
abline(v = x)
```

The maximum point in this function will always be the sample mean from the data; the sample mean is the MLE. In the above case, the mean of the single data point `r x` is the number itself. If we had two data points from a Normal(0,1) distribution, then the likelihood function would be defined as follows. First, let us simulate two data points:

```{r}
(x1 <- rnorm(1))
(x2 <- rnorm(1))
```

These two data points are independent of each other. Hence, to obtain the joint likelihood, we will have to multiply the likelihoods of each of the numbers, given some value for $\mu$:

```{r eval=FALSE}
mu <- 1
dnorm(x1, mean = mu) * dnorm(x2, mean = mu)
```


In order to plot the joint likelihood, we need to write a function:

```{r}
normallik <- function(mu = NULL) {
  dnorm(x1, mean = mu) * dnorm(x2, mean = mu)
}
## test:
normallik(mu = 1)
```

Now, we can plot the likelihood function for these two data points:

```{r}
mu <- seq(-3, 3, by = 0.01)
plot(mu, normallik(mu), type = "l")
abline(v = mean(c(x1, x2)))
```

Notice that the maximum value of this joint likelihood is the mean of the two data points.

For the normal distribution, where $Y \sim N(\mu,\sigma)$, we can get MLEs of $\mu$ and $\sigma$ by computing:

\begin{equation}
  \hat \mu = \frac{1}{n}\sum y_i = \bar{y}  
\end{equation}

and

\begin{equation}
	\hat \sigma ^2 = \frac{1}{n}\sum (y_i-\bar{y})^2
\end{equation}

As mentioned earlier, as the formula for the variance, you will sometimes see the unbiased estimate (and this is what R computes) but for large sample sizes the difference is not important:

\begin{equation}
  \hat \sigma ^2 = \frac{1}{n-1}\sum (y_i-\bar{y})^2
\end{equation}

Now we come to a crucial difference between the discrete and continuous cases discussed above. The ```dbinom``` is the PMF, but it is also a likelihood function when seen as a function of $\theta$. Once we have fixed the $\theta$ parameter to a particular value, the ```dbinom``` function gives us the *probability* of a particular outcome.
Given some data $k$ from $n$ trials from a Binomial distribution, and treating $\theta$ as variable between 0 and 1, ```dbinom``` gives us the likelihood. 

The situation is slightly different in the continuous PDF. Taking the normal distribution as an example, the ```dnorm``` function $f(y|\mu,\sigma)$ doesn't give us the **probability** of a point value, but rather the **density**. When the function $f(y|\mu,\sigma)$ is treated as a function of the parameters, it gives us the **likelihood**. We need to make a careful distinction between the words probability and likelihood; in day-to-day usage the two words are used interchangeably, but here these two terms have different technical meanings.

A final point to note is that a likelihood function is not a PDF; the area under the curve does not need to sum to 1. By contrast, in a PDF, the area under the curve must sum to 1.

### The importance of the MLE 

One significance of the MLE is that, having assumed a particular underlying PMF/PDF, we can estimate the (unknown) parameters (the mean and variance) of the distribution that we assume to have generated our particular data. For example, in the Binomial case, we have a formula for computing the MLEs of the mean and variance; for the Normal distribution, we have a formula for computing the MLE of the mean and the variance.


What are the distributional properties of the mean **under repeated sampling**? This is a question that forms the basis for hypothesis testing in the frequentist paradigm. So we turn to this topic in the next chapter.

:::: {.blackbox data-latex=""}
::: {data-latex=""}
**Basic random variable theory: A formal statement**
:::

We can summarize the above informal concepts relating to random variables very compactly if we re-state them in mathematical form. A mathematical statement has the advantage not only of brevity but also of reducing ambiguity.  

Formally, a random variable $Y$ is defined as a function from a sample space of possible outcomes $S$ to the real number system: 

\begin{equation}
Y : S \rightarrow \mathbb{R}
\end{equation}

The random variable associates to each outcome $\omega \in S$ exactly one number $Y(\omega) = y$. $S_Y$ is all the $y$'s (all the possible values of $Y$, the support of $Y$). I.e., $y \in S_Y$. 

Every random variable $Y$ has associated with it a probability mass (distribution)  function (PMF, PDF). I.e., PMF is used for discrete distributions and PDF for continuous distributions. The PMF maps every element of $S_Y$ to a value between 0 and 1. The PDF maps a range of values in the support of $Y$ to a value between 0 and 1 (e.g., $P(a \leq Y\leq b) \rightarrow [0, 1]$).

\begin{equation}
p_Y : S_Y \rightarrow [0, 1] 
\end{equation}

Probability mass functions (discrete case) and probability density functions (continuous case) are functions that assign probabilities or relative frequencies to events in a sample space.

The expression 

\begin{equation}
 Y \sim f(\cdot)
\end{equation}

\noindent
will be used to mean that the random variable $Y$ has PDF/PMF $f(\cdot)$.
For example, if we say that $Y \sim Binomial(n,\theta)$, then we are asserting that the PMF is:

\begin{equation}
\hbox{Binomial}(k|n,\theta) = 
\binom{n}{k} \theta^{k} (1-\theta)^{n-k}
\end{equation}

If we say that $Y\sim Normal(\mu,\sigma)$, we are asserting that the PDF is

\begin{equation}
Normal(y|\mu,\sigma)= \frac{1}{\sqrt{2\pi \sigma^2}} \exp \left(-\frac{(y-\mu)^2}{2\sigma^2} \right)
\end{equation}

The **cumulative distribution function** or CDF is defined as follows: 

For discrete distributions, the probability that $Y$ is less than $a$ is written:

\begin{equation}
P(Y<a) = F(Y<a) =\sum_{-\infty}^{a} f(y)
\end{equation}

For continuous distributions, the summation symbol $\sum$ above becomes the summation symbol for the continuous case, which is the integral $\int$. The upper and lower bounds are marked by adding a subscript and a superscript on the integral. For example, if we want the area under the curve between points a and b for some function $f(y)$, we write $\int_b^a f(y)\, dy$. So, if we want the probability that $Y$ is less than $a$, we would write:

\begin{equation}
P(Y<a) = F(Y<a) =\int_{-\infty}^{a} f(y)\, dy
\end{equation}

The above integral is simply summing up the area under the curve between the points $-\infty$ and $a$; this gives us the probability of observing $a$ or a value smaller than $a$.

We can use the complementary cumulative distribution function to compute quantities like $P(Y>a)$ by computing $1-F(a)$, and the quantity $P(a\leq Y\leq b)$ by computing $F(b)-F(a)$, where $b>a$.

A final point here is that we can go back and forth between the PDF and the CDF. If the PDF is $f(y)$, then the CDF that allows us to compute quantities like $P(Y<b)$ is just the integral:

\begin{equation}
F(Y<b)=\int_{-\infty}^b f(y)\, dy
\end{equation}

The above is simply computing the area under the curve $f(y)$, ranging from $b$ to $-\infty$.

Because differentiation is the opposite of integration (this is called the Fundamental Theorem of Calculus), if we differentiate the CDF, we get the PDF back: 

\begin{equation}
d(F(y))/dy=f(y)
\end{equation}

In bivariate distributions, the joint CDF is written $F_{X,Y}(a,b)=P(X\leq a, Y\leq b)$, where $-\infty < a,b<\infty$. The *marginal distributions* of $F_X$ and $F_Y$ are the CDFs of each of the associated random variables. The CDF of $X$:

\begin{equation}
F_X(a) = P(X\leq a) = F_X(a,\infty)	
\end{equation}

The CDF of $Y$:

\begin{equation}
F_Y(b) = P(Y\leq b) = F_Y(\infty,b)	
\end{equation}

$f(x,y)$ is the *joint PDF* of $X$ and $Y$. Every joint PDF satisfies

\begin{equation}
f(x,y)\geq 0\mbox{ for all }(x,y)\in S_{X,Y},
\end{equation}
and
\begin{equation}
\int \int_{S_{X,Y}}f(x,y)\,\mathrm{d} x\,\mathrm{d} y=1.
\end{equation}
	
where $S_{X,Y}$ is the joint support of the two random variables.

If X and Y are jointly continuous, they are individually continuous, and their PDFs are:

\begin{equation}
\begin{split}
P(X\in A) = & P(X\in A, Y\in (-\infty,\infty))	\\
= & \int_A \int_{-\infty}^{\infty} f(x,y)\,dy\, dx\\
= & \int_A f_X(x)\, dx
\end{split}	
\end{equation}

\noindent
where

\begin{equation}
f_X(x) = \int_{-\infty}^{\infty} f(x,y)\, dy	
\end{equation}

Similarly:

\begin{equation}
f_Y(y) =  \int_{-\infty}^{\infty} f(x,y)\, dx		
\end{equation}
::::

## Useful R functions relating to univariate distributions

Table \@ref(tab:dpqrfunctions) summarizes the different functions relating to univariate PMFs and PDFs, using the Binomial and Normal as examples.

Table: (\#tab:dpqrfunctions) Important R functions relating to two univariate distributions, the Binomial and the Normal. In the table, prob represents probability and ranges from 0 to 1. $P(y)$ is the probability of observing $y$; $F(y)$ is the cumulative distribution function (CDF); and $F^{-1}(prob)$ is the inverse of the CDF.

                                              Discrete              Continuous
  ------------------------------------ ---------------------- ----------------------
  Example:                              Binomial(n,$\theta$)   Normal($\mu,\sigma$)
  Likelihood function                          dbinom                 dnorm
  Probability: $P(Y=y)$                 dbinom                always 0
  CDF, $F(y)=P(Y\geq y)=prob$           pbinom                 pnorm
  Inverse CDF, $F^{-1}(prob)=y$         qbinom                 qnorm
  Generate simulated data                      rbinom                 rnorm

Other distributions, such as the t-distribution, the Uniform, Exponential, Gamma, Beta, etc., have their own set of d-p-q-r functions in R. 

## Summary	
	
## Further reading

For readers interested in the mathematics needed for statistics, the books by @fox2009mathematical, @gill2006essential, and @moore2013mathematics are useful. The essential matrix algebra needed for statistics is discussed in @fieller.  Accessible introductions to probability theory are @morin2016probability and @blitzstein2014introduction. @kerns contains a very well-written and freely available general introduction to random variable theory and statistics, but assumes the reader knows the basics of calculus.

## Exercises {#sec:Foundationsexercises}

```{exercise, Foundationsexercisespnorm1}
Practice using the ```pnorm``` function 
```

Given a normal distribution with mean 500 and standard deviation 100, use the `pnorm` function to calculate the probability of obtaining values between 200 and 800 from this distribution.

```{exercise, Foundationsexercisespnorm2}
Practice using the ```pnorm``` function 
```

Calculate the following probabilities. 
Given a normal distribution with mean 800 and standard deviation 150, what is the probability of getting 

- a score of 700 or less
- a score of 900 or more
- a score of 800 or more


```{exercise, Foundationsexercisespnorm3}
Practice using the ```pnorm``` function 
```
Given a normal distribution with mean 600 and standard deviation 200, what is the probability of getting 

- a score of 550 or less.
- a score between 300 and 800.
- a score of 900 or more.


```{exercise, Foundationsexercisesqnorm1}
Practice using the ```qnorm``` function 
```

Consider a normal distribution with mean 1 and standard deviation 1.
Compute the lower and upper boundaries such that:

- the area (the probability) to the left of the lower boundary is 0.10. 
- the area (the probability) to the left of the upper boundary is 0.90.

```{exercise, Foundationsexercisesqnorm2}
Practice using the ```qnorm``` function 
```

Given a normal distribution with mean 650 and standard deviation 125. There exist two quantiles, the lower quantile q1 and the upper quantile q2, that are equidistant from the mean 650, such that the area under the curve of the Normal between q1 and q2  is 80%. Find q1 and q2. 


```{exercise, Foundationsexercisesmle1}
Maximum likelihood estimation
```

The function `dnorm` gives the likelihood given a data point (or multiple data) and a value for the mean and the standard deviation (sd). Using `dnorm`, compute 

- the likelihood of the data point 420 assuming a mean of 500 and standard deviation 100.
- the likelihood of the data point 420 assuming a mean of 420 and standard deviation 100.
- the likelihood of the data point 420 assuming a mean of 400 and standard deviation 100.

```{exercise, Foundationsexercisesmle2}
Maximum likelihood estimation
```

```{r echo=FALSE}
x <- round(rnorm(10, mean = 500, sd = 10), 0)
mn <- mean(x)
loglik <- round(sum(dnorm(x, mean = mn, sd = 5, log = TRUE)), 3)
loglik2 <- round(sum(dnorm(x, mean = mn - 2, sd = 5, log = TRUE)), 3)
```

You are given $10$ independent and identically distributed data points that are assumed to come from a Normal distribution with unknown mean and unknown standard deviation:

```{r}
x
```

The function ```dnorm``` gives the likelihood given  multiple data points and a value for the mean and the standard deviation. The log-likelihood can be computed by typing ```dnorm(...,log=TRUE)```.

The product of the likelihoods for two independent data points can be computed like this: Suppose we have two independent and identically distributed data points 5 and 10. Then, assuming that the Normal distribution they come from has mean 10 and standard deviation 5, the joint likelihood of these is:

```{r}
dnorm(5, mean = 10, sd = 2) * dnorm(10, mean = 10, sd = 5)
```

It is easier to do this on the log scale, because then one can add instead of multiplying. This is because $\log(x\times y)= \log(x) + \log(y)$.  For example:

```{r}
log(2 * 3)
log(2) + log(3)
```

So the joint log likelihood of the two data points is:

```{r}
dnorm(5, mean = 10, sd = 5, log = TRUE) +
  dnorm(10, mean = 10, sd = 5, log = TRUE)
```

Even more compactly:

```{r}
sum(dnorm(c(5, 10), mean = 10, sd = 5, log = TRUE))
```

- Given the 10 data points above, calculate the maximum likelihood estimate (MLE) of the expectation.
- The sum of the log-likelihoods of the data x, using as the mean the MLE from the sample, and standard deviation 5.
- What is the sum of the log-likelihood if the mean used to compute the log-likelihood is ```r mn-2``` (assume a standard deviation of 5)? 
- Which value for the mean, the MLE or ```r mn-2```, gives the higher log-likelihood? 

```{exercise, Foundationsexercisesbivariate}
Generating bivariate data
```

Generate 50 data points from two random variables X and Y, where $X\sim Normal(50,100)$ and $Y\sim Normal(100,20)$. The correlation between the random variables is  0.7. Plot the simulated data points from Y against those from X.

```{exercise, Foundationsexercisesmultivariate}
Generating multivariate data
```

The bivariate case can be generalized to more than two dimensions.
Generate 50 data points from three random variables X, Y, and Z, where $X\sim Normal(50,100)$, $Y\sim Normal(100,20)$, and $Z\sim Normal(200,50)$. The correlation between the random variables X and Y is  0.5, between X and Z is 0.2, an between Y and Z is 0.7. Here, you will have to define a $3\times 3$ variance covariance matrix, with the pairwise covariances in the off-diagonals. Plot the simulated data points as two-dimensional figures: Y against X, Y against Z, and X against Z.
