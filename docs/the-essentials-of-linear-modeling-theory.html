<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>5.2 The essentials of linear modeling theory | Linear Mixed Models in Linguistics and Psychology: A Comprehensive Introduction</title>
  <meta name="description" content="Linear Mixed Models for Linguistics and Psychology: A Comprehensive Introduction" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="5.2 The essentials of linear modeling theory | Linear Mixed Models in Linguistics and Psychology: A Comprehensive Introduction" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://github.com/vasishth/Freq_CogSci" />
  <meta property="og:image" content="https://github.com/vasishth/Freq_CogSci/images/temporarycover.jpg" />
  <meta property="og:description" content="Linear Mixed Models for Linguistics and Psychology: A Comprehensive Introduction" />
  <meta name="github-repo" content="https://github.com/vasishth/Freq_CogSci" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="5.2 The essentials of linear modeling theory | Linear Mixed Models in Linguistics and Psychology: A Comprehensive Introduction" />
  
  <meta name="twitter:description" content="Linear Mixed Models for Linguistics and Psychology: A Comprehensive Introduction" />
  <meta name="twitter:image" content="https://github.com/vasishth/Freq_CogSci/images/temporarycover.jpg" />

<meta name="author" content="Shravan Vasishth, Daniel Schad, Audrey Bürki, Reinhold Kliegl" />


<meta name="date" content="2021-12-19" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="a-quick-review-of-some-basic-concepts-in-matrix-algebra.html"/>
<link rel="next" href="summary-4.html"/>
<script src="libs/header-attrs/header-attrs.js"></script>
<script src="libs/jquery/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections/anchor-sections.js"></script>
<script src="libs/kePrint/kePrint.js"></script>
<link href="libs/lightable/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  background-color: #f8f8f8; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Linear Mixed Models in Linguistics and Psychology</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="prerequisites.html"><a href="prerequisites.html"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="" data-path="how-to-read-this-book.html"><a href="how-to-read-this-book.html"><i class="fa fa-check"></i>How to read this book</a></li>
<li class="chapter" data-level="" data-path="online-materials.html"><a href="online-materials.html"><i class="fa fa-check"></i>Online materials</a></li>
<li class="chapter" data-level="" data-path="software-needed.html"><a href="software-needed.html"><i class="fa fa-check"></i>Software needed</a></li>
<li class="chapter" data-level="" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="about-the-authors.html"><a href="about-the-authors.html"><i class="fa fa-check"></i>About the Authors</a></li>
<li class="part"><span><b>I Foundational ideas</b></span></li>
<li class="chapter" data-level="1" data-path="some-important-facts-about-distributions.html"><a href="some-important-facts-about-distributions.html"><i class="fa fa-check"></i><b>1</b> Some important facts about distributions</a>
<ul>
<li class="chapter" data-level="1.1" data-path="discrete-random-variables-an-example-using-the-binomial-distribution.html"><a href="discrete-random-variables-an-example-using-the-binomial-distribution.html"><i class="fa fa-check"></i><b>1.1</b> Discrete random variables: An example using the Binomial distribution</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="discrete-random-variables-an-example-using-the-binomial-distribution.html"><a href="discrete-random-variables-an-example-using-the-binomial-distribution.html#the-mean-and-variance-of-the-binomial-distribution"><i class="fa fa-check"></i><b>1.1.1</b> The mean and variance of the Binomial distribution</a></li>
<li class="chapter" data-level="1.1.2" data-path="discrete-random-variables-an-example-using-the-binomial-distribution.html"><a href="discrete-random-variables-an-example-using-the-binomial-distribution.html#what-information-does-a-probability-distribution-provide"><i class="fa fa-check"></i><b>1.1.2</b> What information does a probability distribution provide?</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="continuous-random-variables-an-example-using-the-normal-distribution.html"><a href="continuous-random-variables-an-example-using-the-normal-distribution.html"><i class="fa fa-check"></i><b>1.2</b> Continuous random variables: An example using the Normal distribution</a></li>
<li class="chapter" data-level="1.3" data-path="other-common-distributions.html"><a href="other-common-distributions.html"><i class="fa fa-check"></i><b>1.3</b> Other common distributions</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="other-common-distributions.html"><a href="other-common-distributions.html#the-standard-normal-mathitnormalmu0sigma1"><i class="fa fa-check"></i><b>1.3.1</b> The standard normal: <span class="math inline">\(\mathit{normal}(\mu=0,\sigma=1)\)</span></a></li>
<li class="chapter" data-level="1.3.2" data-path="other-common-distributions.html"><a href="other-common-distributions.html#the-uniform-distribution"><i class="fa fa-check"></i><b>1.3.2</b> The uniform distribution</a></li>
<li class="chapter" data-level="1.3.3" data-path="other-common-distributions.html"><a href="other-common-distributions.html#the-chi-square-distribution"><i class="fa fa-check"></i><b>1.3.3</b> The Chi-square distribution</a></li>
<li class="chapter" data-level="1.3.4" data-path="other-common-distributions.html"><a href="other-common-distributions.html#the-t-distribution"><i class="fa fa-check"></i><b>1.3.4</b> The t-distribution</a></li>
<li class="chapter" data-level="1.3.5" data-path="other-common-distributions.html"><a href="other-common-distributions.html#the-f-distribution"><i class="fa fa-check"></i><b>1.3.5</b> The F distribution</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="bivariate-and-multivariate-distributions.html"><a href="bivariate-and-multivariate-distributions.html"><i class="fa fa-check"></i><b>1.4</b> Bivariate and multivariate distributions</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="bivariate-and-multivariate-distributions.html"><a href="bivariate-and-multivariate-distributions.html#example-1-discrete-bivariate-distributions"><i class="fa fa-check"></i><b>1.4.1</b> Example 1: Discrete bivariate distributions</a></li>
<li class="chapter" data-level="1.4.2" data-path="bivariate-and-multivariate-distributions.html"><a href="bivariate-and-multivariate-distributions.html#example-2-continuous-bivariate-distributions"><i class="fa fa-check"></i><b>1.4.2</b> Example 2: Continuous bivariate distributions</a></li>
<li class="chapter" data-level="1.4.3" data-path="bivariate-and-multivariate-distributions.html"><a href="bivariate-and-multivariate-distributions.html#generate-simulated-bivariate-multivariate-data"><i class="fa fa-check"></i><b>1.4.3</b> Generate simulated bivariate (multivariate) data</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="likelihood-and-maximum-likelihood-estimation.html"><a href="likelihood-and-maximum-likelihood-estimation.html"><i class="fa fa-check"></i><b>1.5</b> Likelihood and maximum likelihood estimation</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="likelihood-and-maximum-likelihood-estimation.html"><a href="likelihood-and-maximum-likelihood-estimation.html#the-importance-of-the-mle"><i class="fa fa-check"></i><b>1.5.1</b> The importance of the MLE</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="useful-r-functions-relating-to-univariate-distributions.html"><a href="useful-r-functions-relating-to-univariate-distributions.html"><i class="fa fa-check"></i><b>1.6</b> Useful R functions relating to univariate distributions</a></li>
<li class="chapter" data-level="1.7" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i><b>1.7</b> Summary</a></li>
<li class="chapter" data-level="1.8" data-path="further-reading.html"><a href="further-reading.html"><i class="fa fa-check"></i><b>1.8</b> Further reading</a></li>
<li class="chapter" data-level="1.9" data-path="sec:Foundationsexercises.html"><a href="sec:Foundationsexercises.html"><i class="fa fa-check"></i><b>1.9</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="hypothetical-repeated-sampling-and-the-t-test.html"><a href="hypothetical-repeated-sampling-and-the-t-test.html"><i class="fa fa-check"></i><b>2</b> Hypothetical repeated sampling and the t-test</a>
<ul>
<li class="chapter" data-level="2.1" data-path="some-terminology-surrounding-typical-experiment-designs-in-linguistics-and-psychology.html"><a href="some-terminology-surrounding-typical-experiment-designs-in-linguistics-and-psychology.html"><i class="fa fa-check"></i><b>2.1</b> Some terminology surrounding typical experiment designs in linguistics and psychology</a></li>
<li class="chapter" data-level="2.2" data-path="the-central-limit-theorem-using-simulation.html"><a href="the-central-limit-theorem-using-simulation.html"><i class="fa fa-check"></i><b>2.2</b> The central limit theorem using simulation</a></li>
<li class="chapter" data-level="2.3" data-path="three-examples-of-the-sampling-distribution.html"><a href="three-examples-of-the-sampling-distribution.html"><i class="fa fa-check"></i><b>2.3</b> Three examples of the sampling distribution</a></li>
<li class="chapter" data-level="2.4" data-path="the-confidence-interval-and-what-its-good-for.html"><a href="the-confidence-interval-and-what-its-good-for.html"><i class="fa fa-check"></i><b>2.4</b> The confidence interval, and what it’s good for</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="the-confidence-interval-and-what-its-good-for.html"><a href="the-confidence-interval-and-what-its-good-for.html#confidence-interals-are-often-misinterpreted"><i class="fa fa-check"></i><b>2.4.1</b> Confidence interals are often misinterpreted</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="hypothesis-testing-the-one-sample-t-test.html"><a href="hypothesis-testing-the-one-sample-t-test.html"><i class="fa fa-check"></i><b>2.5</b> Hypothesis testing: The one sample t-test</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="hypothesis-testing-the-one-sample-t-test.html"><a href="hypothesis-testing-the-one-sample-t-test.html#the-one-sample-t-test"><i class="fa fa-check"></i><b>2.5.1</b> The one-sample t-test</a></li>
<li class="chapter" data-level="2.5.2" data-path="hypothesis-testing-the-one-sample-t-test.html"><a href="hypothesis-testing-the-one-sample-t-test.html#type-i-ii-error-and-power"><i class="fa fa-check"></i><b>2.5.2</b> Type I, II error, and power</a></li>
<li class="chapter" data-level="2.5.3" data-path="hypothesis-testing-the-one-sample-t-test.html"><a href="hypothesis-testing-the-one-sample-t-test.html#how-to-compute-power-for-the-one-sample-t-test"><i class="fa fa-check"></i><b>2.5.3</b> How to compute power for the one-sample t-test</a></li>
<li class="chapter" data-level="2.5.4" data-path="hypothesis-testing-the-one-sample-t-test.html"><a href="hypothesis-testing-the-one-sample-t-test.html#the-p-value"><i class="fa fa-check"></i><b>2.5.4</b> The p-value</a></li>
<li class="chapter" data-level="2.5.5" data-path="hypothesis-testing-the-one-sample-t-test.html"><a href="hypothesis-testing-the-one-sample-t-test.html#the-distribution-of-the-p-value-under-the-null-hypothesis"><i class="fa fa-check"></i><b>2.5.5</b> The distribution of the p-value under the null hypothesis</a></li>
<li class="chapter" data-level="2.5.6" data-path="hypothesis-testing-the-one-sample-t-test.html"><a href="hypothesis-testing-the-one-sample-t-test.html#type-m-and-s-error-in-the-face-of-low-power"><i class="fa fa-check"></i><b>2.5.6</b> Type M and S error in the face of low power</a></li>
<li class="chapter" data-level="2.5.7" data-path="hypothesis-testing-the-one-sample-t-test.html"><a href="hypothesis-testing-the-one-sample-t-test.html#searching-for-significance"><i class="fa fa-check"></i><b>2.5.7</b> Searching for significance</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="the-two-sample-t-test-vs.-the-paired-t-test.html"><a href="the-two-sample-t-test-vs.-the-paired-t-test.html"><i class="fa fa-check"></i><b>2.6</b> The two-sample t-test vs. the paired t-test</a></li>
<li class="chapter" data-level="2.7" data-path="using-paired-t-tests-in-complex-factorial-designs.html"><a href="using-paired-t-tests-in-complex-factorial-designs.html"><i class="fa fa-check"></i><b>2.7</b> Using paired t-tests in complex factorial designs</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="using-paired-t-tests-in-complex-factorial-designs.html"><a href="using-paired-t-tests-in-complex-factorial-designs.html#analyzing-a-2times-2-repeated-measures-design-using-paired-t-tests"><i class="fa fa-check"></i><b>2.7.1</b> Analyzing a <span class="math inline">\(2\times 2\)</span> repeated measures design using paired t-tests</a></li>
<li class="chapter" data-level="2.7.2" data-path="using-paired-t-tests-in-complex-factorial-designs.html"><a href="using-paired-t-tests-in-complex-factorial-designs.html#a-complication-with-multiple-t-tests-inflation-of-type-i-error-probability"><i class="fa fa-check"></i><b>2.7.2</b> A complication with multiple t-tests: Inflation of Type I error probability</a></li>
<li class="chapter" data-level="2.7.3" data-path="using-paired-t-tests-in-complex-factorial-designs.html"><a href="using-paired-t-tests-in-complex-factorial-designs.html#analyzing-a-2times-2times-2-repeated-measures-design-using-paired-t-tests"><i class="fa fa-check"></i><b>2.7.3</b> Analyzing a <span class="math inline">\(2\times 2\times 2\)</span> repeated measures design using paired t-tests</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="common-mistakes-involving-the-paired-t-test.html"><a href="common-mistakes-involving-the-paired-t-test.html"><i class="fa fa-check"></i><b>2.8</b> Common mistakes involving the (paired) t-test</a>
<ul>
<li class="chapter" data-level="2.8.1" data-path="common-mistakes-involving-the-paired-t-test.html"><a href="common-mistakes-involving-the-paired-t-test.html#ignoring-the-independence-assumption"><i class="fa fa-check"></i><b>2.8.1</b> Ignoring the independence assumption</a></li>
<li class="chapter" data-level="2.8.2" data-path="common-mistakes-involving-the-paired-t-test.html"><a href="common-mistakes-involving-the-paired-t-test.html#doing-a-by-subjects-and-by-items-paired-t-test-is-generally-dangerous"><i class="fa fa-check"></i><b>2.8.2</b> Doing a by-subjects and by-items paired t-test is generally dangerous</a></li>
<li class="chapter" data-level="2.8.3" data-path="common-mistakes-involving-the-paired-t-test.html"><a href="common-mistakes-involving-the-paired-t-test.html#the-difference-between-a-significant-and-a-non-significant-result-need-not-itself-be-significant"><i class="fa fa-check"></i><b>2.8.3</b> The difference between a significant and a non-significant result need not itself be significant</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="summary-1.html"><a href="summary-1.html"><i class="fa fa-check"></i><b>2.9</b> Summary</a></li>
<li class="chapter" data-level="2.10" data-path="further-readings.html"><a href="further-readings.html"><i class="fa fa-check"></i><b>2.10</b> Further readings</a></li>
<li class="chapter" data-level="2.11" data-path="sec:SamplingDistrnexercises.html"><a href="sec:SamplingDistrnexercises.html"><i class="fa fa-check"></i><b>2.11</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="linear-models-and-linear-mixed-models.html"><a href="linear-models-and-linear-mixed-models.html"><i class="fa fa-check"></i><b>3</b> Linear models and linear mixed models</a>
<ul>
<li class="chapter" data-level="3.1" data-path="from-the-t-test-to-the-linear-mixed-model.html"><a href="from-the-t-test-to-the-linear-mixed-model.html"><i class="fa fa-check"></i><b>3.1</b> From the t-test to the linear (mixed) model</a></li>
<li class="chapter" data-level="3.2" data-path="sum-coding.html"><a href="sum-coding.html"><i class="fa fa-check"></i><b>3.2</b> Sum coding</a></li>
<li class="chapter" data-level="3.3" data-path="checking-model-assumptions.html"><a href="checking-model-assumptions.html"><i class="fa fa-check"></i><b>3.3</b> Checking model assumptions</a></li>
<li class="chapter" data-level="3.4" data-path="from-the-paired-t-test-to-the-linear-mixed-model.html"><a href="from-the-paired-t-test-to-the-linear-mixed-model.html"><i class="fa fa-check"></i><b>3.4</b> From the paired t-test to the linear mixed model</a></li>
<li class="chapter" data-level="3.5" data-path="linear-mixed-models.html"><a href="linear-mixed-models.html"><i class="fa fa-check"></i><b>3.5</b> Linear mixed models</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="linear-mixed-models.html"><a href="linear-mixed-models.html#model-type-1-varying-intercepts"><i class="fa fa-check"></i><b>3.5.1</b> Model type 1: Varying intercepts</a></li>
<li class="chapter" data-level="3.5.2" data-path="linear-mixed-models.html"><a href="linear-mixed-models.html#the-formal-statement-of-the-varying-intercepts-model"><i class="fa fa-check"></i><b>3.5.2</b> The formal statement of the varying intercepts model</a></li>
<li class="chapter" data-level="3.5.3" data-path="linear-mixed-models.html"><a href="linear-mixed-models.html#model-type-2-varying-intercepts-and-slopes-without-a-correlation"><i class="fa fa-check"></i><b>3.5.3</b> Model type 2: Varying intercepts and slopes, without a correlation</a></li>
<li class="chapter" data-level="3.5.4" data-path="linear-mixed-models.html"><a href="linear-mixed-models.html#model-type-3-varying-intercepts-and-varying-slopes-with-correlation"><i class="fa fa-check"></i><b>3.5.4</b> Model type 3: Varying intercepts and varying slopes, with correlation</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="shrinkage-in-linear-mixed-models.html"><a href="shrinkage-in-linear-mixed-models.html"><i class="fa fa-check"></i><b>3.6</b> Shrinkage in linear mixed models</a></li>
<li class="chapter" data-level="3.7" data-path="summary-2.html"><a href="summary-2.html"><i class="fa fa-check"></i><b>3.7</b> Summary</a></li>
<li class="chapter" data-level="3.8" data-path="further-reading-1.html"><a href="further-reading-1.html"><i class="fa fa-check"></i><b>3.8</b> Further reading</a></li>
<li class="chapter" data-level="3.9" data-path="sec:LMExercises1.html"><a href="sec:LMExercises1.html"><i class="fa fa-check"></i><b>3.9</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="hypothesis-testing-using-the-likelihood-ratio-test.html"><a href="hypothesis-testing-using-the-likelihood-ratio-test.html"><i class="fa fa-check"></i><b>4</b> Hypothesis testing using the likelihood ratio test</a>
<ul>
<li class="chapter" data-level="4.1" data-path="the-likelihood-ratio-test-the-theory.html"><a href="the-likelihood-ratio-test-the-theory.html"><i class="fa fa-check"></i><b>4.1</b> The likelihood ratio test: The theory</a></li>
<li class="chapter" data-level="4.2" data-path="a-practical-example-using-simulated-data.html"><a href="a-practical-example-using-simulated-data.html"><i class="fa fa-check"></i><b>4.2</b> A practical example using simulated data</a></li>
<li class="chapter" data-level="4.3" data-path="a-real-life-example-the-english-relative-clause-data.html"><a href="a-real-life-example-the-english-relative-clause-data.html"><i class="fa fa-check"></i><b>4.3</b> A real-life example: The English relative clause data</a></li>
<li class="chapter" data-level="4.4" data-path="summary-3.html"><a href="summary-3.html"><i class="fa fa-check"></i><b>4.4</b> Summary</a></li>
<li class="chapter" data-level="4.5" data-path="further-reading-2.html"><a href="further-reading-2.html"><i class="fa fa-check"></i><b>4.5</b> Further reading</a></li>
<li class="chapter" data-level="4.6" data-path="sec:HypTestExercises.html"><a href="sec:HypTestExercises.html"><i class="fa fa-check"></i><b>4.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ch:LMtheory.html"><a href="ch:LMtheory.html"><i class="fa fa-check"></i><b>5</b> Linear modeling theory</a>
<ul>
<li class="chapter" data-level="5.1" data-path="a-quick-review-of-some-basic-concepts-in-matrix-algebra.html"><a href="a-quick-review-of-some-basic-concepts-in-matrix-algebra.html"><i class="fa fa-check"></i><b>5.1</b> A quick review of some basic concepts in matrix algebra</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="a-quick-review-of-some-basic-concepts-in-matrix-algebra.html"><a href="a-quick-review-of-some-basic-concepts-in-matrix-algebra.html#matrix-addition-subtraction-and-multiplication"><i class="fa fa-check"></i><b>5.1.1</b> Matrix addition, subtraction, and multiplication</a></li>
<li class="chapter" data-level="5.1.2" data-path="a-quick-review-of-some-basic-concepts-in-matrix-algebra.html"><a href="a-quick-review-of-some-basic-concepts-in-matrix-algebra.html#diagonal-matrix-and-identity-matrix"><i class="fa fa-check"></i><b>5.1.2</b> Diagonal matrix and identity matrix</a></li>
<li class="chapter" data-level="5.1.3" data-path="a-quick-review-of-some-basic-concepts-in-matrix-algebra.html"><a href="a-quick-review-of-some-basic-concepts-in-matrix-algebra.html#powers-of-matrices"><i class="fa fa-check"></i><b>5.1.3</b> Powers of matrices</a></li>
<li class="chapter" data-level="5.1.4" data-path="a-quick-review-of-some-basic-concepts-in-matrix-algebra.html"><a href="a-quick-review-of-some-basic-concepts-in-matrix-algebra.html#inverse-of-a-matrix"><i class="fa fa-check"></i><b>5.1.4</b> Inverse of a matrix</a></li>
<li class="chapter" data-level="5.1.5" data-path="a-quick-review-of-some-basic-concepts-in-matrix-algebra.html"><a href="a-quick-review-of-some-basic-concepts-in-matrix-algebra.html#linear-independence-and-rank"><i class="fa fa-check"></i><b>5.1.5</b> Linear independence, and rank</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="the-essentials-of-linear-modeling-theory.html"><a href="the-essentials-of-linear-modeling-theory.html"><i class="fa fa-check"></i><b>5.2</b> The essentials of linear modeling theory</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="the-essentials-of-linear-modeling-theory.html"><a href="the-essentials-of-linear-modeling-theory.html#least-squares-estimation-geometric-argument"><i class="fa fa-check"></i><b>5.2.1</b> Least squares estimation: Geometric argument</a></li>
<li class="chapter" data-level="5.2.2" data-path="the-essentials-of-linear-modeling-theory.html"><a href="the-essentials-of-linear-modeling-theory.html#the-expectation-and-variance-of-the-parameters-beta"><i class="fa fa-check"></i><b>5.2.2</b> The expectation and variance of the parameters beta</a></li>
<li class="chapter" data-level="5.2.3" data-path="the-essentials-of-linear-modeling-theory.html"><a href="the-essentials-of-linear-modeling-theory.html#hypothesis-testing-using-analysis-of-variance-anova"><i class="fa fa-check"></i><b>5.2.3</b> Hypothesis testing using Analysis of variance (ANOVA)</a></li>
<li class="chapter" data-level="5.2.4" data-path="the-essentials-of-linear-modeling-theory.html"><a href="the-essentials-of-linear-modeling-theory.html#some-further-important-topics-in-linear-modeling"><i class="fa fa-check"></i><b>5.2.4</b> Some further important topics in linear modeling</a></li>
<li class="chapter" data-level="5.2.5" data-path="the-essentials-of-linear-modeling-theory.html"><a href="the-essentials-of-linear-modeling-theory.html#generalized-linear-models"><i class="fa fa-check"></i><b>5.2.5</b> Generalized linear models</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="summary-4.html"><a href="summary-4.html"><i class="fa fa-check"></i><b>5.3</b> Summary</a></li>
<li class="chapter" data-level="5.4" data-path="further-reading-3.html"><a href="further-reading-3.html"><i class="fa fa-check"></i><b>5.4</b> Further reading</a></li>
<li class="chapter" data-level="5.5" data-path="sec:LMExercises2.html"><a href="sec:LMExercises2.html"><i class="fa fa-check"></i><b>5.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ch:contr.html"><a href="ch:contr.html"><i class="fa fa-check"></i><b>6</b> Contrast coding</a>
<ul>
<li class="chapter" data-level="6.1" data-path="basic-concepts-illustrated-using-a-two-level-factor.html"><a href="basic-concepts-illustrated-using-a-two-level-factor.html"><i class="fa fa-check"></i><b>6.1</b> Basic concepts illustrated using a two-level factor</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="basic-concepts-illustrated-using-a-two-level-factor.html"><a href="basic-concepts-illustrated-using-a-two-level-factor.html#treatmentcontrasts"><i class="fa fa-check"></i><b>6.1.1</b> Default contrast coding: Treatment contrasts</a></li>
<li class="chapter" data-level="6.1.2" data-path="basic-concepts-illustrated-using-a-two-level-factor.html"><a href="basic-concepts-illustrated-using-a-two-level-factor.html#inverseMatrix"><i class="fa fa-check"></i><b>6.1.2</b> Defining hypotheses</a></li>
<li class="chapter" data-level="6.1.3" data-path="basic-concepts-illustrated-using-a-two-level-factor.html"><a href="basic-concepts-illustrated-using-a-two-level-factor.html#effectcoding"><i class="fa fa-check"></i><b>6.1.3</b> Sum contrasts</a></li>
<li class="chapter" data-level="6.1.4" data-path="basic-concepts-illustrated-using-a-two-level-factor.html"><a href="basic-concepts-illustrated-using-a-two-level-factor.html#sec:cellMeans"><i class="fa fa-check"></i><b>6.1.4</b> Cell means parameterization and posterior comparisons</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="the-hypothesis-matrix-illustrated-with-a-three-level-factor.html"><a href="the-hypothesis-matrix-illustrated-with-a-three-level-factor.html"><i class="fa fa-check"></i><b>6.2</b> The hypothesis matrix illustrated with a three-level factor</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="the-hypothesis-matrix-illustrated-with-a-three-level-factor.html"><a href="the-hypothesis-matrix-illustrated-with-a-three-level-factor.html#sumcontrasts"><i class="fa fa-check"></i><b>6.2.1</b> Sum contrasts</a></li>
<li class="chapter" data-level="6.2.2" data-path="the-hypothesis-matrix-illustrated-with-a-three-level-factor.html"><a href="the-hypothesis-matrix-illustrated-with-a-three-level-factor.html#the-hypothesis-matrix"><i class="fa fa-check"></i><b>6.2.2</b> The hypothesis matrix</a></li>
<li class="chapter" data-level="6.2.3" data-path="the-hypothesis-matrix-illustrated-with-a-three-level-factor.html"><a href="the-hypothesis-matrix-illustrated-with-a-three-level-factor.html#generating-contrasts-the-hypr-package"><i class="fa fa-check"></i><b>6.2.3</b> Generating contrasts: The <code>hypr</code> package</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="sec:4levelFactor.html"><a href="sec:4levelFactor.html"><i class="fa fa-check"></i><b>6.3</b> Further examples of contrasts illustrated with a factor with four levels</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="sec:4levelFactor.html"><a href="sec:4levelFactor.html#repeatedcontrasts"><i class="fa fa-check"></i><b>6.3.1</b> Repeated contrasts</a></li>
<li class="chapter" data-level="6.3.2" data-path="sec:4levelFactor.html"><a href="sec:4levelFactor.html#contrasts-in-linear-regression-analysis-the-design-or-model-matrix"><i class="fa fa-check"></i><b>6.3.2</b> Contrasts in linear regression analysis: The design or model matrix</a></li>
<li class="chapter" data-level="6.3.3" data-path="sec:4levelFactor.html"><a href="sec:4levelFactor.html#polynomialContrasts"><i class="fa fa-check"></i><b>6.3.3</b> Polynomial contrasts</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="nonOrthogonal.html"><a href="nonOrthogonal.html"><i class="fa fa-check"></i><b>6.4</b> What makes a good set of contrasts?</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="nonOrthogonal.html"><a href="nonOrthogonal.html#centered-contrasts"><i class="fa fa-check"></i><b>6.4.1</b> Centered contrasts</a></li>
<li class="chapter" data-level="6.4.2" data-path="nonOrthogonal.html"><a href="nonOrthogonal.html#orthogonal-contrasts"><i class="fa fa-check"></i><b>6.4.2</b> Orthogonal contrasts</a></li>
<li class="chapter" data-level="6.4.3" data-path="nonOrthogonal.html"><a href="nonOrthogonal.html#the-role-of-the-intercept-in-non-centered-contrasts"><i class="fa fa-check"></i><b>6.4.3</b> The role of the intercept in non-centered contrasts</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="summary-5.html"><a href="summary-5.html"><i class="fa fa-check"></i><b>6.5</b> Summary</a></li>
<li class="chapter" data-level="6.6" data-path="further-reading-4.html"><a href="further-reading-4.html"><i class="fa fa-check"></i><b>6.6</b> Further reading</a></li>
<li class="chapter" data-level="6.7" data-path="sec:Contrastsexercises.html"><a href="sec:Contrastsexercises.html"><i class="fa fa-check"></i><b>6.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ch:coding2x2.html"><a href="ch:coding2x2.html"><i class="fa fa-check"></i><b>7</b> Contrast coding for designs with two predictor variables</a>
<ul>
<li class="chapter" data-level="7.1" data-path="sec:MR:ANOVA.html"><a href="sec:MR:ANOVA.html"><i class="fa fa-check"></i><b>7.1</b> Contrast coding in a factorial 2 x 2 design</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="sec:MR:ANOVA.html"><a href="sec:MR:ANOVA.html#the-difference-between-an-anova-and-a-multiple-regression"><i class="fa fa-check"></i><b>7.1.1</b> The difference between an ANOVA and a multiple regression</a></li>
<li class="chapter" data-level="7.1.2" data-path="sec:MR:ANOVA.html"><a href="sec:MR:ANOVA.html#nestedEffects"><i class="fa fa-check"></i><b>7.1.2</b> Nested effects</a></li>
<li class="chapter" data-level="7.1.3" data-path="sec:MR:ANOVA.html"><a href="sec:MR:ANOVA.html#interactions-between-contrasts"><i class="fa fa-check"></i><b>7.1.3</b> Interactions between contrasts</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="sec:contrast:covariate.html"><a href="sec:contrast:covariate.html"><i class="fa fa-check"></i><b>7.2</b> One factor and one covariate</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="sec:contrast:covariate.html"><a href="sec:contrast:covariate.html#estimating-a-group-difference-and-controlling-for-a-covariate"><i class="fa fa-check"></i><b>7.2.1</b> Estimating a group-difference and controlling for a covariate</a></li>
<li class="chapter" data-level="7.2.2" data-path="sec:contrast:covariate.html"><a href="sec:contrast:covariate.html#estimating-differences-in-slopes"><i class="fa fa-check"></i><b>7.2.2</b> Estimating differences in slopes</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="sec:interactions:NLM.html"><a href="sec:interactions:NLM.html"><i class="fa fa-check"></i><b>7.3</b> Interactions in generalized linear models (with non-linear link functions)</a></li>
<li class="chapter" data-level="7.4" data-path="summary-6.html"><a href="summary-6.html"><i class="fa fa-check"></i><b>7.4</b> Summary</a></li>
<li class="chapter" data-level="7.5" data-path="sec:Contrasts2x2exercises.html"><a href="sec:Contrasts2x2exercises.html"><i class="fa fa-check"></i><b>7.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="using-simulation-to-understand-your-model.html"><a href="using-simulation-to-understand-your-model.html"><i class="fa fa-check"></i><b>8</b> Using simulation to understand your model</a>
<ul>
<li class="chapter" data-level="8.1" data-path="a-reminder-the-maximal-linear-mixed-model.html"><a href="a-reminder-the-maximal-linear-mixed-model.html"><i class="fa fa-check"></i><b>8.1</b> A reminder: The maximal linear mixed model</a></li>
<li class="chapter" data-level="8.2" data-path="obtain-estimates-from-a-previous-study.html"><a href="obtain-estimates-from-a-previous-study.html"><i class="fa fa-check"></i><b>8.2</b> Obtain estimates from a previous study</a></li>
<li class="chapter" data-level="8.3" data-path="decide-on-a-range-of-plausible-values-of-the-effect-size.html"><a href="decide-on-a-range-of-plausible-values-of-the-effect-size.html"><i class="fa fa-check"></i><b>8.3</b> Decide on a range of plausible values of the effect size</a></li>
<li class="chapter" data-level="8.4" data-path="extract-parameter-estimates.html"><a href="extract-parameter-estimates.html"><i class="fa fa-check"></i><b>8.4</b> Extract parameter estimates</a></li>
<li class="chapter" data-level="8.5" data-path="define-a-function-for-generating-data.html"><a href="define-a-function-for-generating-data.html"><i class="fa fa-check"></i><b>8.5</b> Define a function for generating data</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="define-a-function-for-generating-data.html"><a href="define-a-function-for-generating-data.html#generate-a-latin-square-design"><i class="fa fa-check"></i><b>8.5.1</b> Generate a Latin-square design</a></li>
<li class="chapter" data-level="8.5.2" data-path="define-a-function-for-generating-data.html"><a href="define-a-function-for-generating-data.html#generate-data-row-by-row"><i class="fa fa-check"></i><b>8.5.2</b> Generate data row-by-row</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="repeated-generation-of-data-to-compute-power.html"><a href="repeated-generation-of-data-to-compute-power.html"><i class="fa fa-check"></i><b>8.6</b> Repeated generation of data to compute power</a></li>
<li class="chapter" data-level="8.7" data-path="what-you-can-now-do.html"><a href="what-you-can-now-do.html"><i class="fa fa-check"></i><b>8.7</b> What you can now do</a></li>
<li class="chapter" data-level="8.8" data-path="using-the-package-designr-to-simulate-data-and-compute-power.html"><a href="using-the-package-designr-to-simulate-data-and-compute-power.html"><i class="fa fa-check"></i><b>8.8</b> Using the package <code>designr</code> to simulate data and compute power</a>
<ul>
<li class="chapter" data-level="8.8.1" data-path="using-the-package-designr-to-simulate-data-and-compute-power.html"><a href="using-the-package-designr-to-simulate-data-and-compute-power.html#simulating-data-with-two-conditions"><i class="fa fa-check"></i><b>8.8.1</b> Simulating data with two conditions</a></li>
<li class="chapter" data-level="8.8.2" data-path="using-the-package-designr-to-simulate-data-and-compute-power.html"><a href="using-the-package-designr-to-simulate-data-and-compute-power.html#simulating-data-in-factorial-designs"><i class="fa fa-check"></i><b>8.8.2</b> Simulating data in factorial designs</a></li>
</ul></li>
<li class="chapter" data-level="8.9" data-path="sec:Simulationexercises.html"><a href="sec:Simulationexercises.html"><i class="fa fa-check"></i><b>8.9</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="ch:MultComp.html"><a href="ch:MultComp.html"><i class="fa fa-check"></i><b>9</b> Understanding Type I error inflation using simulation</a>
<ul>
<li class="chapter" data-level="9.1" data-path="overly-simple-random-effects-structure-in-lmms-inflate-type-i-error.html"><a href="overly-simple-random-effects-structure-in-lmms-inflate-type-i-error.html"><i class="fa fa-check"></i><b>9.1</b> Overly simple random effects structure in LMMs inflate Type I error</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="overly-simple-random-effects-structure-in-lmms-inflate-type-i-error.html"><a href="overly-simple-random-effects-structure-in-lmms-inflate-type-i-error.html#type-i-error-with-a-varying-intercepts-only-model"><i class="fa fa-check"></i><b>9.1.1</b> Type I error with a varying intercepts-only model</a></li>
<li class="chapter" data-level="9.1.2" data-path="overly-simple-random-effects-structure-in-lmms-inflate-type-i-error.html"><a href="overly-simple-random-effects-structure-in-lmms-inflate-type-i-error.html#type-i-error-with-a-varying-intercepts-and-varying-slopes-model"><i class="fa fa-check"></i><b>9.1.2</b> Type I error with a varying intercepts and varying slopes model</a></li>
<li class="chapter" data-level="9.1.3" data-path="overly-simple-random-effects-structure-in-lmms-inflate-type-i-error.html"><a href="overly-simple-random-effects-structure-in-lmms-inflate-type-i-error.html#type-i-error-inflation-due-to-model-mis-specification"><i class="fa fa-check"></i><b>9.1.3</b> Type I error inflation due to model mis-specification</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="type-i-error-inflation-due-to-multiple-comparisons.html"><a href="type-i-error-inflation-due-to-multiple-comparisons.html"><i class="fa fa-check"></i><b>9.2</b> Type I error inflation due to multiple comparisons</a></li>
<li class="chapter" data-level="9.3" data-path="the-practical-implications.html"><a href="the-practical-implications.html"><i class="fa fa-check"></i><b>9.3</b> The practical implications</a></li>
<li class="chapter" data-level="9.4" data-path="summary-7.html"><a href="summary-7.html"><i class="fa fa-check"></i><b>9.4</b> Summary</a></li>
<li class="chapter" data-level="9.5" data-path="further-reading-5.html"><a href="further-reading-5.html"><i class="fa fa-check"></i><b>9.5</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="ch:reproducible.html"><a href="ch:reproducible.html"><i class="fa fa-check"></i><b>10</b> Developing a reproducible workflow</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Linear Mixed Models in Linguistics and Psychology: A Comprehensive Introduction</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="the-essentials-of-linear-modeling-theory" class="section level2" number="5.2">
<h2><span class="header-section-number">5.2</span> The essentials of linear modeling theory</h2>
<p>Consider a deterministic function <span class="math inline">\(\phi(\mathbf{x},\beta)\)</span> which takes as input some variable values <span class="math inline">\(x\)</span> and some fixed values <span class="math inline">\(\beta\)</span>. A simple example would be some variable <span class="math inline">\(x\)</span> determining the value of another variable <span class="math inline">\(y\)</span> by multiplying <span class="math inline">\(x\)</span> with <span class="math inline">\(\beta\)</span>.</p>
<p><span class="math display">\[\begin{equation}
y = \beta x
\end{equation}\]</span></p>
<p>Another example with two fixed values <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> determining <span class="math inline">\(y\)</span> is:</p>
<p><span class="math display">\[\begin{equation}
y = \beta_0 + \beta_1 x
\end{equation}\]</span></p>
<p>We can rewrite the above equation in matrix form as follows.</p>
<p><span class="math display">\[\begin{equation}
\begin{split}
y=&amp; \beta_0 + \beta_1 x\\
=&amp; \beta_0\times 1 + \beta_1 x\\
=&amp; \begin{pmatrix}
1 &amp; x\\
\end{pmatrix}
\begin{pmatrix}
\beta_0 \\
\beta_1 \\
\end{pmatrix}
\end{split}
\end{equation}\]</span></p>
<p>Because <span class="math inline">\(y\)</span> is a function of <span class="math inline">\(x\)</span> and the <span class="math inline">\(2\times 1\)</span> matrix <span class="math inline">\(\beta=[\beta_0 \beta_1]^T\)</span>, the most general way to write the above function is as we did above:</p>
<p><span class="math display">\[\begin{equation}
y = \phi(x,\beta)
\end{equation}\]</span></p>
<p>In a statistical model, we don’t expect an equation like <span class="math inline">\(y=\phi(x,\beta)\)</span> to fit all the points exactly. For example, we could come up with an
equation that, given a word’s frequency, gives a prediction regarding that word’s reaction time:</p>
<p><span class="math display">\[\begin{equation}
\hbox{predicted reaction time} = \beta_0 + \beta_1 \hbox{frequency}
\end{equation}\]</span></p>
<p>Given any single value of the frequency of a word, we will not get a perfectly correct prediction of the reaction time for that word. As a concrete example, see this data-set from the <code>languageR</code> library, which allows us to visualize the effect of (log) word frequency on (log) reaction times. This model is technically incorrect, because we have repeated measures; a linear mixed model would be more appropriate. But a simple linear model is sufficient to make the point here:</p>
<div class="sourceCode" id="cb453"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb453-1"><a href="the-essentials-of-linear-modeling-theory.html#cb453-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(languageR)</span></code></pre></div>
<div class="sourceCode" id="cb454"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb454-1"><a href="the-essentials-of-linear-modeling-theory.html#cb454-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(<span class="st">&quot;lexdec&quot;</span>)</span>
<span id="cb454-2"><a href="the-essentials-of-linear-modeling-theory.html#cb454-2" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">lm</span>(RT <span class="sc">~</span> Frequency, lexdec)</span>
<span id="cb454-3"><a href="the-essentials-of-linear-modeling-theory.html#cb454-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m)<span class="sc">$</span>coefficients</span></code></pre></div>
<pre><code>##             Estimate Std. Error t value  Pr(&gt;|t|)
## (Intercept)  6.58878   0.022296 295.515 0.000e+00
## Frequency   -0.04287   0.004533  -9.459 1.027e-20</code></pre>
<div class="sourceCode" id="cb456"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb456-1"><a href="the-essentials-of-linear-modeling-theory.html#cb456-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(RT <span class="sc">~</span> Frequency, lexdec)</span>
<span id="cb456-2"><a href="the-essentials-of-linear-modeling-theory.html#cb456-2" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(m)</span></code></pre></div>
<p><img src="Freq_CogSci_files/figure-html/unnamed-chunk-194-1.svg" width="672" /></p>
<p>Because the predicted values from the linear model don’t exactly predict the observed vlues, we express the dependent variable <span class="math inline">\(y\)</span> as a non-deterministic function:</p>
<p><span class="math display">\[\begin{equation}
y=\phi(x,\beta,\epsilon)=\beta_0+\beta_1x+\epsilon
\end{equation}\]</span></p>
<p>Here, <span class="math inline">\(\epsilon\)</span> is an error random variable which is assumed to have some PDF (the normal distribution) associated with it.
It is assumed to have expectation (mean) 0, and some standard deviation (to be estimated from the data) <span class="math inline">\(\sigma\)</span>.
We can write this statement in compact form as <span class="math inline">\(\epsilon \sim N(0,\sigma)\)</span>.</p>
<p>The <strong>general linear model</strong> is a non-deterministic function like the one above:</p>
<p><span class="math display">\[\begin{equation}
Y=f(x)^T\beta +\epsilon 
\end{equation}\]</span></p>
<p>The matrix formulation will be written as below. <span class="math inline">\(n\)</span> refers to the number of data points (that is, <span class="math inline">\(Y_1,\dots,Y_n\)</span>), and the index <span class="math inline">\(j\)</span> ranges from <span class="math inline">\(1\)</span> to <span class="math inline">\(n\)</span>.</p>
<p><span class="math display">\[\begin{equation}
Y = X\beta + \epsilon \Leftrightarrow y_j = f(x_j)^T \beta + \epsilon_j, j=1,\dots,n
\end{equation}\]</span></p>
<p>To make this concrete, suppose we have three data points, i.e., <span class="math inline">\(n=3\)</span>. Then, the matrix formulation is</p>
<p><span class="math display">\[\begin{equation}
\begin{split}
\begin{pmatrix}
Y_1 \\
Y_2\\
Y_3 \\
\end{pmatrix}
=
\begin{pmatrix}
1 &amp; x_1 \\
1 &amp; x_2 \\
1 &amp; x_3 \\
\end{pmatrix}
\begin{pmatrix}
\beta_0 \\
\beta_1 \\
\end{pmatrix}+ 
\begin{pmatrix}
\epsilon_1 \\
\epsilon_2 \\
\epsilon_3 \\
\end{pmatrix}\\
\end{split}
\end{equation}\]</span></p>
<p>We can write this in compact form as follows:</p>
<p><span class="math display">\[\begin{equation}
Y = X \beta + \epsilon
\end{equation}\]</span></p>
<p>Y is a <span class="math inline">\(3\times 1\)</span> matrix, X is a <span class="math inline">\(3\times 2\)</span> matrix, <span class="math inline">\(\beta\)</span> <span class="math inline">\(2\times 1\)</span>, and <span class="math inline">\(\epsilon\)</span> <span class="math inline">\(3\times 1\)</span>.</p>
<p>Here, <span class="math inline">\(f(x_1)^T = (1~x_1)\)</span>, and is the first row of the matrix <span class="math inline">\(X\)</span>,
<span class="math inline">\(f(x_2)^T = (1~x_2)\)</span> is the second row, and
<span class="math inline">\(f(x_3)^T = (1~x_3)\)</span> is the third row.</p>
<p>Note that the expectation or mean of Y, written <span class="math inline">\(E[Y]\)</span>, is <span class="math inline">\(X\beta\)</span>. In a data-set with <span class="math inline">\(n\)</span> data points, when there are <span class="math inline">\(p\)</span> parameters, <span class="math inline">\(\beta\)</span> is a <span class="math inline">\(p\times 1\)</span> matrix, and <span class="math inline">\(X\)</span>, which is called the <strong>design matrix</strong> or <strong>model matrix</strong>, is <span class="math inline">\(n\times p\)</span>.</p>
<div id="least-squares-estimation-geometric-argument" class="section level3" number="5.2.1">
<h3><span class="header-section-number">5.2.1</span> Least squares estimation: Geometric argument</h3>
<p>The above excursion into the matrix formulation of the linear model gives us the ability to understand how the <span class="math inline">\(\beta\)</span> parameters are estimated.</p>
<p>When we have a deterministic model <span class="math inline">\(y=\phi(f(x)^T,\beta)=\beta_0+\beta_1x\)</span>, this implies a perfect fit to all data points.
This is like solving the equation <span class="math inline">\(Ax=b\)</span> in linear algebra: we solve for <span class="math inline">\(\beta\)</span> in <span class="math inline">\(X\beta=y\)</span> e.g., by pre-multiplying by <span class="math inline">\(X^{-1}\)</span>: <span class="math inline">\(X^{-1}X\beta=X^{-1}y\)</span>. For example, if we have:</p>
<div class="sourceCode" id="cb457"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb457-1"><a href="the-essentials-of-linear-modeling-theory.html#cb457-1" aria-hidden="true" tabindex="-1"></a>(X <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">6</span>, <span class="dv">8</span>, <span class="dv">3</span>, <span class="dv">6</span>, <span class="dv">8</span>, <span class="dv">10</span>), <span class="at">nrow =</span> <span class="dv">3</span>, <span class="at">byrow =</span> <span class="cn">TRUE</span>))</span></code></pre></div>
<pre><code>##      [,1] [,2] [,3] [,4]
## [1,]    1    2    2    2
## [2,]    2    4    6    8
## [3,]    3    6    8   10</code></pre>
<div class="sourceCode" id="cb459"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb459-1"><a href="the-essentials-of-linear-modeling-theory.html#cb459-1" aria-hidden="true" tabindex="-1"></a>(y <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">5</span>, <span class="dv">6</span>)))</span></code></pre></div>
<pre><code>##      [,1]
## [1,]    1
## [2,]    5
## [3,]    6</code></pre>
<p>We can solve for <span class="math inline">\(x\)</span> as follows (ginv is a function that computed a so-called generalized inverse for non-square matrices):</p>
<div class="sourceCode" id="cb461"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb461-1"><a href="the-essentials-of-linear-modeling-theory.html#cb461-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MASS)</span>
<span id="cb461-2"><a href="the-essentials-of-linear-modeling-theory.html#cb461-2" aria-hidden="true" tabindex="-1"></a><span class="do">## Solve for x in Ax=y:</span></span>
<span id="cb461-3"><a href="the-essentials-of-linear-modeling-theory.html#cb461-3" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">ginv</span>(X) <span class="sc">%*%</span> y</span>
<span id="cb461-4"><a href="the-essentials-of-linear-modeling-theory.html#cb461-4" aria-hidden="true" tabindex="-1"></a><span class="do">## confirm:</span></span>
<span id="cb461-5"><a href="the-essentials-of-linear-modeling-theory.html#cb461-5" aria-hidden="true" tabindex="-1"></a>X <span class="sc">%*%</span> x</span></code></pre></div>
<pre><code>##      [,1]
## [1,]    1
## [2,]    5
## [3,]    6</code></pre>
<p>But when we have a non-deterministic model
<span class="math inline">\(y=\phi(f(x)^T,\beta,\epsilon)\)</span>, there is no solution! Now, the best we can do in an equation like <span class="math inline">\(Ax=b\)</span> is to get <span class="math inline">\(Ax\)</span> to be as close an approximation as possible to <span class="math inline">\(b\)</span>. In other words, we try to minimize <span class="math inline">\(\mid b-Ax\mid\)</span>.</p>
<p>The goal is to estimate <span class="math inline">\(\beta\)</span>; we want to find a value of <span class="math inline">\(\beta\)</span> such that the observed Y is as close to its expected value <span class="math inline">\(X\beta\)</span>.
In order to be able to identify <span class="math inline">\(\beta\)</span> from <span class="math inline">\(X\beta\)</span>, the linear transformation <span class="math inline">\(\beta \rightarrow X\beta\)</span> should be one-to-one, so that every possible value of <span class="math inline">\(\beta\)</span> gives a different <span class="math inline">\(X\beta\)</span>. This in turn requires that X be of full rank <span class="math inline">\(p\)</span>. So, if a design matrix X is <span class="math inline">\(n\times p\)</span>, then it is necessary that <span class="math inline">\(n\geq p\)</span>. There must be at least as many observations as parameters. If this is not true, then the model is said to be <strong>over-parameterized</strong>.</p>
<p>Assuming that X is of full rank, and that <span class="math inline">\(n&gt;p\)</span>,
Y can be considered a point in n-dimensional space and the set of candidate <span class="math inline">\(X\beta\)</span> is a <span class="math inline">\(p\)</span>-dimensional subspace of this space; see Figure <a href="the-essentials-of-linear-modeling-theory.html#fig:leastsquares">5.1</a>. There will be one point in this subspace which is closest to Y in terms of Euclidean distance. The unique <span class="math inline">\(\beta\)</span> that corresponds to this point is the <strong>least squares estimator</strong> of <span class="math inline">\(\beta\)</span>; we will call this estimator <span class="math inline">\(\hat \beta\)</span>.</p>
<div class="figure"><span style="display:block;" id="fig:leastsquares"></span>
<img src="figures/leastsq.pdf" alt="Geometric visualization of the distance minimization procedure for estimating the parameters in a linear model."  />
<p class="caption">
FIGURE 5.1: Geometric visualization of the distance minimization procedure for estimating the parameters in a linear model.
</p>
</div>
<p>Notice that <span class="math inline">\(\epsilon=(Y - X\hat\beta)\)</span> and <span class="math inline">\(X\beta\)</span> are perpendicular to each other. Because the dot product of two perpendicular (orthogonal) vectors is 0, we get the result:</p>
<p><span class="math display">\[\begin{equation}
(Y- X\hat\beta)^T X \beta = 0 \Leftrightarrow (Y- X\hat\beta)^T X = 0 
\end{equation}\]</span></p>
<p>Multiplying out the terms, we proceed as follows. One result that we use here is that <span class="math inline">\((AB)^T = B^T A^T\)</span>.</p>
<p><span class="math display">\[\begin{equation}
\begin{split}
~&amp; (Y- X\hat\beta)^T X = 0  \\
~&amp; (Y^T- \hat\beta^T X^T)X = 0\\
\Leftrightarrow&amp; Y^T X - \hat\beta^TX^T X = 0 \quad  \\
\Leftrightarrow&amp; Y^T X = \hat\beta^TX^T X \\
\Leftrightarrow&amp; (Y^T X)^T = (\hat\beta^TX^T X)^T \\
\Leftrightarrow&amp; X^T Y = X^TX\hat\beta\\
\end{split}
\end{equation}\]</span></p>
<p>This gives us the important result:</p>
<p><span class="math display">\[\begin{equation}
\hat\beta = (X^TX)^{-1}X^T Y
\end{equation}\]</span></p>
<p>X is of full rank, therefore <span class="math inline">\(X^TX\)</span> is invertible. One crucial detail, whose significance will only become clear in the chapter on contrast coding, is that the matrix <span class="math inline">\((X^TX)^{-1}X^T\)</span>—the generalized matrix inverse—is closely related to the design matrix, which in turn determines what comparisons are implied by the <span class="math inline">\(\beta\)</span> parameters.</p>
<p>Let’s look at a concrete example:</p>
<div class="sourceCode" id="cb463"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb463-1"><a href="the-essentials-of-linear-modeling-theory.html#cb463-1" aria-hidden="true" tabindex="-1"></a>(X <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(</span>
<span id="cb463-2"><a href="the-essentials-of-linear-modeling-theory.html#cb463-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rep</span>(<span class="dv">1</span>, <span class="dv">8</span>), <span class="fu">rep</span>(<span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>), <span class="at">each =</span> <span class="dv">4</span>),</span>
<span id="cb463-3"><a href="the-essentials-of-linear-modeling-theory.html#cb463-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rep</span>(<span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>), <span class="at">each =</span> <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb463-4"><a href="the-essentials-of-linear-modeling-theory.html#cb463-4" aria-hidden="true" tabindex="-1"></a>), <span class="at">ncol =</span> <span class="dv">3</span>))</span></code></pre></div>
<pre><code>##      [,1] [,2] [,3]
## [1,]    1   -1   -1
## [2,]    1   -1   -1
## [3,]    1   -1    1
## [4,]    1   -1    1
## [5,]    1    1   -1
## [6,]    1    1   -1
## [7,]    1    1    1
## [8,]    1    1    1</code></pre>
<div class="sourceCode" id="cb465"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb465-1"><a href="the-essentials-of-linear-modeling-theory.html#cb465-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(Matrix)</span>
<span id="cb465-2"><a href="the-essentials-of-linear-modeling-theory.html#cb465-2" aria-hidden="true" tabindex="-1"></a><span class="do">## full rank:</span></span>
<span id="cb465-3"><a href="the-essentials-of-linear-modeling-theory.html#cb465-3" aria-hidden="true" tabindex="-1"></a><span class="fu">rankMatrix</span>(X)</span></code></pre></div>
<pre><code>## [1] 3
## attr(,&quot;method&quot;)
## [1] &quot;tolNorm2&quot;
## attr(,&quot;useGrad&quot;)
## [1] FALSE
## attr(,&quot;tol&quot;)
## [1] 1.776e-15</code></pre>
<p>Notice that the inverted matrix is also symmetric. We will use this fact soon.</p>
<p>The matrix <span class="math inline">\(V=X^T X\)</span> is a symmetric matrix, which means that <span class="math inline">\(V^T=V\)</span>.</p>
</div>
<div id="the-expectation-and-variance-of-the-parameters-beta" class="section level3" number="5.2.2">
<h3><span class="header-section-number">5.2.2</span> The expectation and variance of the parameters beta</h3>
<p>Our model now is:</p>
<p><span class="math display">\[\begin{equation}
Y = X\beta + \epsilon
\end{equation}\]</span></p>
<p>Let <span class="math inline">\(\epsilon\sim N(0,\sigma)\)</span>. In other words, we are assuming that each value generated by the random variable <span class="math inline">\(\epsilon\)</span> is independent and it has the same distribution, i.e., it is identically distributed. This is sometimes shortened to the iid assumption. So we should technically be writing:</p>
<p><span class="math display">\[\begin{equation}
Y = X\beta + \epsilon \quad  \epsilon\sim N(0,\sigma)
\end{equation}\]</span></p>
<p>and add that <span class="math inline">\(Y\)</span> are independent and identically distributed.</p>
<p>Some consequences of the above statements:</p>
<ul>
<li><span class="math inline">\(E[\epsilon]=0\)</span></li>
<li><span class="math inline">\(Var(\epsilon)=\sigma^2 I_n\)</span></li>
<li><span class="math inline">\(E[Y]=X\beta=\mu\)</span></li>
<li><span class="math inline">\(Var(Y)=\sigma^2 I_n\)</span></li>
</ul>
<p>We can now derive the expectation and variance of the estimators <span class="math inline">\(\hat\beta\)</span>. We need a fact about variances: when we want to know <span class="math inline">\(Var(a\times B)\)</span>, where a is a constant and B is a random variable, this variance is <span class="math inline">\(a^2 Var(B)\)</span>. In the matrix setting, Var(AB), where A is a conformable matrix consisting of some constant values, is <span class="math inline">\(A Var(B)A^T\)</span>.</p>
<p><span class="math display">\[\begin{equation}
E[\hat\beta] = E[(X^TX)^{-1}X^T Y] = (X^TX)^{-1}X^T X\beta = \beta
\end{equation}\]</span></p>
<p>Notice that the above shows that <span class="math inline">\(\hat\beta\)</span> is a so-called “unbiased estimator” of <span class="math inline">\(\beta\)</span>. The word unbiased doesn’t mean that every time you compute an estimate of <span class="math inline">\(\beta\)</span>, you are guaranteed to get an accurate estimate of the true <span class="math inline">\(\beta\)</span>! Think about Type M error.</p>
<p>Next, we compute the variance:</p>
<p><span class="math display">\[\begin{equation}
Var(\hat\beta) = Var([(X^TX)^{-1}X^T] Y)
\end{equation}\]</span></p>
<p>Expanding the right hand side out:</p>
<p><span class="math display">\[\begin{equation}
Var([(X^TX)^{-1}X^T] Y) = [(X^TX)^{-1}X^T] Var(Y)  [(X^TX)^{-1}X^T]^{T}
\end{equation}\]</span></p>
<p>Replacing Var(Y) with its variance written in matrix form <span class="math inline">\(\sigma^2 I\)</span>, and unpacking the transpose on the right-most expression <span class="math inline">\([(X^TX)^{-1}X^T]^{T}\)</span>:</p>
<p><span class="math display">\[\begin{equation}
Var(\hat\beta)= [(X^TX)^{-1}X^T] \sigma^2 I  X[(X^TX)^{-1}]^{T} 
\end{equation}\]</span></p>
<p>Since <span class="math inline">\(\sigma^2\)</span> is a scalar we can move it to the left, and any matrix multiplied by I is the matrix itself, so we ignore I, getting:</p>
<p><span class="math display">\[\begin{equation}
Var(\hat\beta)= \sigma^2 [(X^TX)^{-1}X^T X[(X^TX)^{-1}]^{T} 
\end{equation}\]</span></p>
<p>Since <span class="math inline">\((X^TX)^{-1}X^T X = I\)</span>, we can simplify to</p>
<p><span class="math display">\[\begin{equation}
Var(\hat\beta)= \sigma^2 [(X^TX)^{-1}]^{T} 
\end{equation}\]</span></p>
<p>Now, <span class="math inline">\((X^TX)^{-1}\)</span> is symmetric, so
<span class="math inline">\([(X^TX)^{-1}]^T=(X^TX)^{-1}\)</span>. This gives us:</p>
<p><span class="math display">\[\begin{equation}
Var(\hat\beta)= \sigma^2 (X^TX)^{-1} 
\end{equation}\]</span></p>
<p>Let’s make this concrete using the <code>lexdec</code> data-set as an example:</p>
<div class="sourceCode" id="cb467"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb467-1"><a href="the-essentials-of-linear-modeling-theory.html#cb467-1" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> lexdec<span class="sc">$</span>RT</span>
<span id="cb467-2"><a href="the-essentials-of-linear-modeling-theory.html#cb467-2" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> lexdec<span class="sc">$</span>Frequency</span>
<span id="cb467-3"><a href="the-essentials-of-linear-modeling-theory.html#cb467-3" aria-hidden="true" tabindex="-1"></a>m0 <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x)</span>
<span id="cb467-4"><a href="the-essentials-of-linear-modeling-theory.html#cb467-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb467-5"><a href="the-essentials-of-linear-modeling-theory.html#cb467-5" aria-hidden="true" tabindex="-1"></a><span class="do">## design matrix:</span></span>
<span id="cb467-6"><a href="the-essentials-of-linear-modeling-theory.html#cb467-6" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(m0)</span>
<span id="cb467-7"><a href="the-essentials-of-linear-modeling-theory.html#cb467-7" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(X, <span class="at">n =</span> <span class="dv">4</span>)</span></code></pre></div>
<pre><code>##   (Intercept)     x
## 1           1 4.860
## 2           1 4.605
## 3           1 4.997
## 4           1 4.727</code></pre>
<div class="sourceCode" id="cb469"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb469-1"><a href="the-essentials-of-linear-modeling-theory.html#cb469-1" aria-hidden="true" tabindex="-1"></a><span class="do">## (X^TX)^{-1}</span></span>
<span id="cb469-2"><a href="the-essentials-of-linear-modeling-theory.html#cb469-2" aria-hidden="true" tabindex="-1"></a>invXTX <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X)</span>
<span id="cb469-3"><a href="the-essentials-of-linear-modeling-theory.html#cb469-3" aria-hidden="true" tabindex="-1"></a><span class="do">## estimate of beta:</span></span>
<span id="cb469-4"><a href="the-essentials-of-linear-modeling-theory.html#cb469-4" aria-hidden="true" tabindex="-1"></a>(hat_beta <span class="ot">&lt;-</span> invXTX <span class="sc">%*%</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> y)</span></code></pre></div>
<pre><code>##                 [,1]
## (Intercept)  6.58878
## x           -0.04287</code></pre>
<div class="sourceCode" id="cb471"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb471-1"><a href="the-essentials-of-linear-modeling-theory.html#cb471-1" aria-hidden="true" tabindex="-1"></a><span class="do">## estimated variance (se^2) of the estimate of beta:</span></span>
<span id="cb471-2"><a href="the-essentials-of-linear-modeling-theory.html#cb471-2" aria-hidden="true" tabindex="-1"></a>(hat_sigma <span class="ot">&lt;-</span> <span class="fu">summary</span>(m0)<span class="sc">$</span>sigma)</span></code></pre></div>
<pre><code>## [1] 0.2353</code></pre>
<div class="sourceCode" id="cb473"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb473-1"><a href="the-essentials-of-linear-modeling-theory.html#cb473-1" aria-hidden="true" tabindex="-1"></a>(hat_var <span class="ot">&lt;-</span> hat_sigma<span class="sc">^</span><span class="dv">2</span> <span class="sc">*</span> invXTX)</span></code></pre></div>
<pre><code>##             (Intercept)          x
## (Intercept)   0.0004971 -9.760e-05
## x            -0.0000976  2.054e-05</code></pre>
<p>What we have here is a bivariate normal distribution as an estimate of the <span class="math inline">\(\beta\)</span> parameters:</p>
<p><span class="math display">\[\begin{equation}
\begin{pmatrix}
\hat\beta_0\\
\hat\beta_1\\
\end{pmatrix}
\sim 
N(\begin{pmatrix}
6.58878\\
-0.04287\\
\end{pmatrix},
\begin{pmatrix}
 0.000497 &amp; -0.0000976\\
-0.0000976 &amp; 2.054e-05\\
\end{pmatrix})
\end{equation}\]</span></p>
<p>The variance of a bivariate distribution has the variances along the diagonal, and the covariance between <span class="math inline">\(\hat\beta_0\)</span> and
<span class="math inline">\(\hat\beta_1\)</span> on the off-diagonals. Covariance is defined as:</p>
<p><span class="math display">\[\begin{equation}
Cov(\hat\beta_0,\hat\beta_1)=\hat\rho \hat\sigma_{\hat\beta_0}\hat\sigma_{\hat\beta_1}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\hat\rho\)</span> is the estimated correlation between <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>.</p>
<p>So</p>
<ul>
<li><span class="math inline">\(\hat\beta_0 \sim N(6.588778,0.022296)\)</span></li>
<li><span class="math inline">\(\hat\beta_1 \sim N(-0.042872,0.0045325)\)</span>, and - <span class="math inline">\(Cov(\hat\beta_0,\hat\beta_1)=-9.76\times 10^{-05}\)</span>.</li>
</ul>
<p>So the correlation between the <span class="math inline">\(\hat\beta\)</span> is</p>
<div class="sourceCode" id="cb475"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb475-1"><a href="the-essentials-of-linear-modeling-theory.html#cb475-1" aria-hidden="true" tabindex="-1"></a><span class="do">## hat rho:</span></span>
<span id="cb475-2"><a href="the-essentials-of-linear-modeling-theory.html#cb475-2" aria-hidden="true" tabindex="-1"></a>hat_var[<span class="dv">1</span>, <span class="dv">2</span>] <span class="sc">/</span> (<span class="fu">sqrt</span>(hat_var[<span class="dv">1</span>, <span class="dv">1</span>]) <span class="sc">*</span> <span class="fu">sqrt</span>(hat_var[<span class="dv">2</span>, <span class="dv">2</span>]))</span></code></pre></div>
<pre><code>## [1] -0.9658</code></pre>
<p>Notice what happens to this correlation when we center the predictor:</p>
<div class="sourceCode" id="cb477"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb477-1"><a href="the-essentials-of-linear-modeling-theory.html#cb477-1" aria-hidden="true" tabindex="-1"></a>x_c <span class="ot">&lt;-</span> <span class="fu">scale</span>(x, <span class="at">scale =</span> <span class="cn">FALSE</span>)</span>
<span id="cb477-2"><a href="the-essentials-of-linear-modeling-theory.html#cb477-2" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x_c)</span>
<span id="cb477-3"><a href="the-essentials-of-linear-modeling-theory.html#cb477-3" aria-hidden="true" tabindex="-1"></a><span class="do">## design matrix:</span></span>
<span id="cb477-4"><a href="the-essentials-of-linear-modeling-theory.html#cb477-4" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(m)</span>
<span id="cb477-5"><a href="the-essentials-of-linear-modeling-theory.html#cb477-5" aria-hidden="true" tabindex="-1"></a><span class="do">## (X^TX)^{-1}</span></span>
<span id="cb477-6"><a href="the-essentials-of-linear-modeling-theory.html#cb477-6" aria-hidden="true" tabindex="-1"></a>invXTX <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X)</span>
<span id="cb477-7"><a href="the-essentials-of-linear-modeling-theory.html#cb477-7" aria-hidden="true" tabindex="-1"></a><span class="do">## estimate of beta:</span></span>
<span id="cb477-8"><a href="the-essentials-of-linear-modeling-theory.html#cb477-8" aria-hidden="true" tabindex="-1"></a>(hat_beta <span class="ot">&lt;-</span> invXTX <span class="sc">%*%</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> y)</span></code></pre></div>
<pre><code>##                 [,1]
## (Intercept)  6.38509
## x_c         -0.04287</code></pre>
<div class="sourceCode" id="cb479"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb479-1"><a href="the-essentials-of-linear-modeling-theory.html#cb479-1" aria-hidden="true" tabindex="-1"></a><span class="do">## estimated variance (se^2) of the estimate of beta:</span></span>
<span id="cb479-2"><a href="the-essentials-of-linear-modeling-theory.html#cb479-2" aria-hidden="true" tabindex="-1"></a>(hat_sigma <span class="ot">&lt;-</span> <span class="fu">summary</span>(m0)<span class="sc">$</span>sigma)</span></code></pre></div>
<pre><code>## [1] 0.2353</code></pre>
<div class="sourceCode" id="cb481"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb481-1"><a href="the-essentials-of-linear-modeling-theory.html#cb481-1" aria-hidden="true" tabindex="-1"></a>(hat_var <span class="ot">&lt;-</span> hat_sigma<span class="sc">^</span><span class="dv">2</span> <span class="sc">*</span> invXTX)</span></code></pre></div>
<pre><code>##             (Intercept)       x_c
## (Intercept)   3.338e-05 7.204e-21
## x_c           7.204e-21 2.054e-05</code></pre>
<div class="sourceCode" id="cb483"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb483-1"><a href="the-essentials-of-linear-modeling-theory.html#cb483-1" aria-hidden="true" tabindex="-1"></a><span class="do">## correlation:</span></span>
<span id="cb483-2"><a href="the-essentials-of-linear-modeling-theory.html#cb483-2" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(hat_var[<span class="dv">1</span>, <span class="dv">2</span>] <span class="sc">/</span> (<span class="fu">sqrt</span>(hat_var[<span class="dv">1</span>, <span class="dv">1</span>]) <span class="sc">*</span> <span class="fu">sqrt</span>(hat_var[<span class="dv">2</span>, <span class="dv">2</span>])), <span class="dv">4</span>)</span></code></pre></div>
<pre><code>## [1] 0</code></pre>
<p>The correlation now is (effectively) zero. This is one of the consequences of centering your predictor: the intercept-slope sampling distributions become independent. The relevance of this fact will be discussed in the chapters on contrast coding.</p>
</div>
<div id="hypothesis-testing-using-analysis-of-variance-anova" class="section level3" number="5.2.3">
<h3><span class="header-section-number">5.2.3</span> Hypothesis testing using Analysis of variance (ANOVA)</h3>
<p>We can compare two models, one nested inside another, as follows:</p>
<div class="sourceCode" id="cb485"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb485-1"><a href="the-essentials-of-linear-modeling-theory.html#cb485-1" aria-hidden="true" tabindex="-1"></a>m1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x_c)</span>
<span id="cb485-2"><a href="the-essentials-of-linear-modeling-theory.html#cb485-2" aria-hidden="true" tabindex="-1"></a>m0 <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> <span class="dv">1</span>)</span>
<span id="cb485-3"><a href="the-essentials-of-linear-modeling-theory.html#cb485-3" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(m0, m1)</span></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Model 1: y ~ 1
## Model 2: y ~ x_c
##   Res.Df  RSS Df Sum of Sq    F Pr(&gt;F)    
## 1   1658 96.7                             
## 2   1657 91.8  1      4.95 89.5 &lt;2e-16 ***
## ---
## Signif. codes:  
## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The F-score you get here is actually the square of the t-value you get in the linear model summary:</p>
<div class="sourceCode" id="cb487"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb487-1"><a href="the-essentials-of-linear-modeling-theory.html#cb487-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sqrt</span>(<span class="fu">anova</span>(m0, m1)<span class="sc">$</span>F[<span class="dv">2</span>])</span></code></pre></div>
<pre><code>## [1] 9.459</code></pre>
<div class="sourceCode" id="cb489"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb489-1"><a href="the-essentials-of-linear-modeling-theory.html#cb489-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m1)<span class="sc">$</span>coefficients[<span class="dv">2</span>, <span class="dv">3</span>]</span></code></pre></div>
<pre><code>## [1] -9.459</code></pre>
<p>This is because <span class="math inline">\(t^2 = F\)</span>. The proof is discussed on page 9 of the Dobson and Barnett book, but we also briefly alluded to this in the likelihood ratio test chapter.</p>
<p>The ANOVA works as follows. We will use the matrix formulation throughout. First define the residual as:</p>
<p><span class="math display">\[\begin{equation}
e = Y - X\hat\beta
\end{equation}\]</span></p>
<p>The square of this is:</p>
<p><span class="math display">\[\begin{equation}
e^T e = (Y - X\hat \beta)^T (Y - X\hat \beta)
\end{equation}\]</span></p>
<p>Define the <strong>deviance</strong> as:</p>
<p><span class="math display">\[\begin{equation}
\begin{split} \label{eq:deviance}
D =&amp; \frac{1}{\sigma^2} (Y - X\hat \beta)^T (Y - X\hat \beta)\\
=&amp; \frac{1}{\sigma^2}  (Y^T - \hat \beta^TX^T)(Y - X\hat \beta)\\
=&amp; \frac{1}{\sigma^2} (Y^T Y - Y^TX\hat \beta - \hat\beta^TX^T Y + \hat\beta^TX^T  X\hat \beta)\\
%=&amp; \frac{1}{\sigma^2} (Y^T Y - \hat\beta^TX^T Y)\\
\end{split}
\end{equation}\]</span></p>
<p>Now, recall that <span class="math inline">\(\hat \beta = (X^T X)^{-1} X^T Y\)</span>. Premultiplying both sides with <span class="math inline">\((X^T X)\)</span>, we get</p>
<p><span class="math inline">\((X^T X)\hat \beta = X^T Y\)</span></p>
<p>It follows that we can rewrite the last line in equation  as follows: We can replace <span class="math inline">\((X^T X)\hat \beta\)</span> with
<span class="math inline">\(X^T Y\)</span>.</p>
<p><span class="math display">\[\begin{equation}
\begin{split}
D =&amp; \frac{1}{\sigma^2} (Y^T Y - Y^TX\hat \beta - \hat\beta^TX^T Y + \hat\beta^T \underline{X^T  X\hat \beta})\\
=&amp; \frac{1}{\sigma^2} (Y^T Y - Y^TX\hat \beta - \hat\beta^TX^T Y + \hat\beta^T \underline{X^T Y})\\
=&amp; \frac{1}{\sigma^2} (Y^T Y - Y^TX\hat \beta) \\
\end{split}
\end{equation}\]</span></p>
<p>Notice that <span class="math inline">\(Y^TX\hat \beta\)</span> is a scalar (<span class="math inline">\(1\times 1\)</span>) and is identical to <span class="math inline">\(\beta^TX^T Y\)</span> (check this), so we could write:</p>
<p><span class="math inline">\(D= \frac{1}{\sigma^2} (Y^T Y - \hat \beta^T X^T Y)\)</span></p>
<p>Assume now that we have data of size n.
Suppose we have a null hypothesis <span class="math inline">\(H_0: \beta=\beta_0\)</span> and an alternative hypothesis <span class="math inline">\(H_1: \beta=\beta_{1}\)</span>. Let the null hypothesis have q parameters, and the alternative p, where <span class="math inline">\(q&lt;p&lt;n\)</span>.
Let <span class="math inline">\(X_0\)</span> be the design matrix for <span class="math inline">\(H_0\)</span>, and <span class="math inline">\(X_1\)</span> the design matrix for <span class="math inline">\(H_1\)</span>.
Compute the deviances <span class="math inline">\(D_0\)</span> and <span class="math inline">\(D_1\)</span> for each hypothesis, and compute <span class="math inline">\(\Delta D\)</span>:</p>
<p><span class="math display">\[\begin{equation}
\begin{split}
\Delta D =&amp; D_0 - D_1 = \frac{1}{\sigma^2} [(Y^TY - \hat \beta_0^T X_0^T Y) -  (Y^TY - \hat \beta_1^T X_1^T Y)]\\
=&amp; \frac{1}{\sigma^2} [\hat \beta_1^T X_1^T Y - \hat \beta_0^T X_0^T Y]\\
\end{split}
\end{equation}\]</span></p>
<p>It turns out that the F-statistic has the following distribution (called the F-distribution, defined in terms of two numerical parameters) if the null hypothesis is true:</p>
<p><span class="math display">\[\begin{equation}
F=\frac{\Delta D/(p-q)}{D_1/(n-p)} \sim F(p-q,n-p)
\end{equation}\]</span></p>
<p>So, an observed F value that lies in the tail of the F-distribution is inconsistent with the null hypothesis and allows us to reject it.</p>
<p>The observed F-statistic is:</p>
<p><span class="math display">\[\begin{equation}
\begin{split}
F=&amp;\frac{\Delta D/(p-q)}{D_1/(n-p)} \\
=&amp; \frac{\hat \beta_1^T X_1^T Y - \hat \beta_0^T X_0^T Y}{p-q} /
\frac{Y^T Y - \hat \beta_1^T X_1^TY}{n-p}\\
\end{split}
\end{equation}\]</span></p>
<p>Traditionally, the way the F-test is summarized is shown in Table .</p>
</div>
<div id="some-further-important-topics-in-linear-modeling" class="section level3" number="5.2.4">
<h3><span class="header-section-number">5.2.4</span> Some further important topics in linear modeling</h3>
<div id="the-variance-inflation-factor-checking-for-multicollinearity" class="section level4" number="5.2.4.1">
<h4><span class="header-section-number">5.2.4.1</span> The variance inflation factor: Checking for multicollinearity</h4>
<p>The linear modeling framework is very flexible, and allows us to add multiple predictors at the same time. For example, in the lexical decision data set, we could look at the effect of Frequency and FamilySize:</p>
<div class="sourceCode" id="cb491"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb491-1"><a href="the-essentials-of-linear-modeling-theory.html#cb491-1" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">lm</span>(RT <span class="sc">~</span> Frequency <span class="sc">+</span> FamilySize, lexdec)</span>
<span id="cb491-2"><a href="the-essentials-of-linear-modeling-theory.html#cb491-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = RT ~ Frequency + FamilySize, data = lexdec)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -0.5510 -0.1608 -0.0344  0.1204  1.0969 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  6.56385    0.02683  244.68  &lt; 2e-16 ***
## Frequency   -0.03531    0.00641   -5.51  4.1e-08 ***
## FamilySize  -0.01565    0.00938   -1.67    0.095 .  
## ---
## Signif. codes:  
## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.235 on 1656 degrees of freedom
## Multiple R-squared:  0.0528, Adjusted R-squared:  0.0517 
## F-statistic: 46.2 on 2 and 1656 DF,  p-value: &lt;2e-16</code></pre>
<p>An important issue here is <strong>multicollinearity</strong>. This occurs when multiple predictors are highly correlated. The consequence of this is that <span class="math inline">\(X^T X\)</span> can be nearly singular and the estimation equation</p>
<p><span class="math display">\[\begin{equation}
X^TX \beta = X^T Y 
\end{equation}\]</span></p>
<p>is ill-conditioned: small changes in the data can cause large changes in <span class="math inline">\(\beta\)</span> (signs will flip for example). Also, some of the elements of <span class="math inline">\(\sigma^2 (X^TX)^{-1}\)</span> will be large–standard errors and covariances can be large.</p>
<p>We can check for multicollinearity using the Variance Inflation Factor, VIF.</p>
<p>Suppose you have fitted a model with several predictors.
The definition of <span class="math inline">\(VIF_j\)</span> for a predictor j is:</p>
<p><span class="math display">\[\begin{equation}
VIF_j = \frac{1}{1-R_j^2}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(R_j^2\)</span> is called the coefficient of determination for predictor j.
It is obtained by regressing the j-th explanatory variable against all the
other explanatory variables.
If a predictor j is uncorrelated with all other predictors, then <span class="math inline">\(VIF_j=1\)</span>, and
if it is highly correlated with (some of) other predictors, the VIF will be high.</p>
<p>Here is a practical example.
Consider frequency and family size in the lexdec data:</p>
<div class="sourceCode" id="cb493"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb493-1"><a href="the-essentials-of-linear-modeling-theory.html#cb493-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(car)</span>
<span id="cb493-2"><a href="the-essentials-of-linear-modeling-theory.html#cb493-2" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">lm</span>(RT <span class="sc">~</span> Frequency <span class="sc">+</span> FamilySize, lexdec)</span>
<span id="cb493-3"><a href="the-essentials-of-linear-modeling-theory.html#cb493-3" aria-hidden="true" tabindex="-1"></a><span class="fu">vif</span>(m)</span></code></pre></div>
<pre><code>##  Frequency FamilySize 
##          2          2</code></pre>
<p>Here is a somewhat worse situation:</p>
<div class="sourceCode" id="cb495"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb495-1"><a href="the-essentials-of-linear-modeling-theory.html#cb495-1" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">lm</span>(RT <span class="sc">~</span> FreqSingular <span class="sc">+</span> FreqPlural, lexdec)</span>
<span id="cb495-2"><a href="the-essentials-of-linear-modeling-theory.html#cb495-2" aria-hidden="true" tabindex="-1"></a><span class="fu">vif</span>(m)</span></code></pre></div>
<pre><code>## FreqSingular   FreqPlural 
##        4.681        4.681</code></pre>
<p>If the predictors are uncorrelated, VIF will be near 1 in each case. Dobson et al mention that VIF of greater than 5 is cause for worry.</p>
</div>
<div id="checking-model-assumptions-1" class="section level4" number="5.2.4.2">
<h4><span class="header-section-number">5.2.4.2</span> Checking model assumptions</h4>
<p>In practical terms, when doing hypothesis testing with the linear model, the first thing you need to check is whether
the residuals are normally distributed. This is because the normality assumption is a crucial part of the assumptions in the hypothesis test (if the goal is only estimation of the parameters, the normality assumption is less important; however, there are subtleties even in this case, which we will discuss in the simulation chapter).
One common way to check for (approximate) normality is to plot the residuals against the quantiles of the normal distribution:</p>
<div class="sourceCode" id="cb497"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb497-1"><a href="the-essentials-of-linear-modeling-theory.html#cb497-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(car)</span>
<span id="cb497-2"><a href="the-essentials-of-linear-modeling-theory.html#cb497-2" aria-hidden="true" tabindex="-1"></a><span class="fu">qqPlot</span>(<span class="fu">residuals</span>(m))</span></code></pre></div>
<p><img src="Freq_CogSci_files/figure-html/unnamed-chunk-206-1.svg" width="672" /></p>
<pre><code>## 1194 1619 
## 1273  750</code></pre>
<p>How to test for normality of residuals?
Kolmogorov-Smirnov and Shapiro-Wilk are formal tests of normality and are only useful for large samples; they not very powerful and not much better than diagnostic plots like the qq-plot shown above.</p>
<p>Apart from normality, we should also check the independence assumption (the errors are assumed to be independent). Index-plots display residuals against observation number; note that they are not useful for small samples. An alternative is to compute the correlation between <span class="math inline">\(e_i, e_{i+1}\)</span> pairs of residuals. The auto-correlation function is not normally used in linear modeling (it’s used more in time-series analyses), but can be used to check for this correlation:</p>
<div class="sourceCode" id="cb499"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb499-1"><a href="the-essentials-of-linear-modeling-theory.html#cb499-1" aria-hidden="true" tabindex="-1"></a><span class="fu">acf</span>(<span class="fu">residuals</span>(m))</span></code></pre></div>
<p><img src="Freq_CogSci_files/figure-html/unnamed-chunk-207-1.svg" width="672" /></p>
<p>In our model (which is the multiple regression we did in connection with the collinearity issue), we have a serious violation of independence. This is because in this model we are not taking into account the fact that we have repeated measures. The repeated measures create a dependency in the residuals, violating the independence assumption. This problem can be largely solved by fitting a linear mixed model:</p>
<div class="sourceCode" id="cb500"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb500-1"><a href="the-essentials-of-linear-modeling-theory.html#cb500-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(lme4)</span>
<span id="cb500-2"><a href="the-essentials-of-linear-modeling-theory.html#cb500-2" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">lmer</span>(RT <span class="sc">~</span> FreqSingular <span class="sc">+</span> FreqPlural <span class="sc">+</span> (<span class="dv">1</span> <span class="sc">|</span> Subject), lexdec)</span>
<span id="cb500-3"><a href="the-essentials-of-linear-modeling-theory.html#cb500-3" aria-hidden="true" tabindex="-1"></a><span class="fu">acf</span>(<span class="fu">residuals</span>(m))</span></code></pre></div>
<p><img src="Freq_CogSci_files/figure-html/unnamed-chunk-208-1.svg" width="672" /></p>
<p>The linear mixed model is taking the repeated measurements property into account.</p>
<p>Finally, we should check for homoscedasticity (equality of variance).
For checking this, plot residuals against fitted values. A fanning-out suggests violation. A quadratic trend in a plot of residuals against predictor x could suggest that a quadratic predictor term is needed; note that we will never have a perfect straight line in such a plot.</p>
<p>R also provides a diagnostics plot, which is generated using the model fit:</p>
<div class="sourceCode" id="cb501"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb501-1"><a href="the-essentials-of-linear-modeling-theory.html#cb501-1" aria-hidden="true" tabindex="-1"></a>op <span class="ot">&lt;-</span> <span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>), <span class="at">pty =</span> <span class="st">&quot;s&quot;</span>)</span>
<span id="cb501-2"><a href="the-essentials-of-linear-modeling-theory.html#cb501-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(m)</span></code></pre></div>
<p><img src="Freq_CogSci_files/figure-html/unnamed-chunk-209-1.svg" width="672" /></p>
<p>In linear mixed models, there is a package called <code>influence.ME</code> <span class="citation">(<a href="#ref-influenceme" role="doc-biblioref">R. Nieuwenhuis, Te Grotenhuis, and Pelzer 2012</a>)</span> that allows one to check model assumptions.</p>
</div>
</div>
<div id="generalized-linear-models" class="section level3" number="5.2.5">
<h3><span class="header-section-number">5.2.5</span> Generalized linear models</h3>
<div id="introduction-logistic-regression" class="section level4" number="5.2.5.1">
<h4><span class="header-section-number">5.2.5.1</span> Introduction: Logistic regression</h4>
<p>We start with an example data-set that appears in the <span class="citation"><a href="#ref-dobson2011introduction" role="doc-biblioref">Dobson and Barnett</a> (<a href="#ref-dobson2011introduction" role="doc-biblioref">2011</a>)</span> book: the Beetle data-set. Although not a linguistic or psychological data-set, it is a good entry point into generalized linear models. Later we will apply the theory to psycholinguistic data.</p>
<p>The Beetle data-set shows the number of beetles killed after five hours’ exposure to gaseous carbon disulphide at various concentrations (<span class="math inline">\(\log_{10}CS_2 mg^{-1}\)</span>). The source for the data is <span class="citation"><a href="#ref-bliss1935calculation" role="doc-biblioref">Bliss</a> (<a href="#ref-bliss1935calculation" role="doc-biblioref">1935</a>)</span>.</p>
<div class="sourceCode" id="cb502"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb502-1"><a href="the-essentials-of-linear-modeling-theory.html#cb502-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dobson)</span>
<span id="cb502-2"><a href="the-essentials-of-linear-modeling-theory.html#cb502-2" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(beetle)</span>
<span id="cb502-3"><a href="the-essentials-of-linear-modeling-theory.html#cb502-3" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(beetle) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;dose&quot;</span>, <span class="st">&quot;number&quot;</span>, <span class="st">&quot;killed&quot;</span>)</span>
<span id="cb502-4"><a href="the-essentials-of-linear-modeling-theory.html#cb502-4" aria-hidden="true" tabindex="-1"></a>beetle</span></code></pre></div>
<pre><code>## # A tibble: 8 × 3
##    dose number killed
##   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;
## 1  1.69     59      6
## 2  1.72     60     13
## 3  1.76     62     18
## 4  1.78     56     28
## 5  1.81     63     52
## 6  1.84     59     53
## 7  1.86     62     61
## 8  1.88     60     60</code></pre>
<p>The research question is: does dose affect probability of killing insects? The first thing we probably want to do is calculate the proportions:</p>
<div class="sourceCode" id="cb504"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb504-1"><a href="the-essentials-of-linear-modeling-theory.html#cb504-1" aria-hidden="true" tabindex="-1"></a>(beetle<span class="sc">$</span>propn.dead <span class="ot">&lt;-</span> beetle<span class="sc">$</span>killed <span class="sc">/</span> beetle<span class="sc">$</span>number)</span></code></pre></div>
<pre><code>## [1] 0.1017 0.2167 0.2903 0.5000 0.8254 0.8983 0.9839
## [8] 1.0000</code></pre>
<p>It’s also reasonable to just plot the relationship between dose and proportion of deaths.</p>
<div class="sourceCode" id="cb506"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb506-1"><a href="the-essentials-of-linear-modeling-theory.html#cb506-1" aria-hidden="true" tabindex="-1"></a><span class="fu">with</span>(beetle, <span class="fu">plot</span>(dose, propn.dead))</span></code></pre></div>
<p><img src="Freq_CogSci_files/figure-html/unnamed-chunk-212-1.svg" width="672" /></p>
<p>Notice that the y-axis is by definition bounded between 0 and 1.</p>
<p>We could easily fit a linear model to this data-set. We may want to center the predictor, for reasons discussed earlier:</p>
<div class="sourceCode" id="cb507"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb507-1"><a href="the-essentials-of-linear-modeling-theory.html#cb507-1" aria-hidden="true" tabindex="-1"></a>fm <span class="ot">&lt;-</span> <span class="fu">lm</span>(propn.dead <span class="sc">~</span> <span class="fu">scale</span>(dose, <span class="at">scale =</span> <span class="cn">FALSE</span>), beetle)</span>
<span id="cb507-2"><a href="the-essentials-of-linear-modeling-theory.html#cb507-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fm)<span class="sc">$</span>coefficients</span></code></pre></div>
<pre><code>##                            Estimate Std. Error t value
## (Intercept)                   0.602    0.03065   19.64
## scale(dose, scale = FALSE)    5.325    0.48573   10.96
##                             Pr(&gt;|t|)
## (Intercept)                1.129e-06
## scale(dose, scale = FALSE) 3.422e-05</code></pre>
<p>Next, add the best-fit (linear model) line to the plot:</p>
<div class="sourceCode" id="cb509"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb509-1"><a href="the-essentials-of-linear-modeling-theory.html#cb509-1" aria-hidden="true" tabindex="-1"></a><span class="fu">with</span>(beetle, <span class="fu">plot</span>(</span>
<span id="cb509-2"><a href="the-essentials-of-linear-modeling-theory.html#cb509-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale</span>(dose, <span class="at">scale =</span> <span class="cn">FALSE</span>),</span>
<span id="cb509-3"><a href="the-essentials-of-linear-modeling-theory.html#cb509-3" aria-hidden="true" tabindex="-1"></a>  propn.dead</span>
<span id="cb509-4"><a href="the-essentials-of-linear-modeling-theory.html#cb509-4" aria-hidden="true" tabindex="-1"></a>))</span>
<span id="cb509-5"><a href="the-essentials-of-linear-modeling-theory.html#cb509-5" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">coef</span>(fm))</span></code></pre></div>
<p><img src="Freq_CogSci_files/figure-html/unnamed-chunk-214-1.svg" width="672" /></p>
<p>What’s the interpretation of the coefficients? Since the predictor is centered, the intercept represents the mean proportion of beetles killed (0.62), and the slope represents the increase in the proportion of beetles killed when the dose is increased by one unit, from an initial dose of 0. The proportion of beetles killed with one unit increase in dose is larger than 1, which is obviously impossible for a proportion. It is because of problems like these that a simple linear model does not suffice for such data.</p>
<p>Instead of using the linear model, we model log odds instead of proportions as a function of dose. Odds are defined as the ratio of the probability of success and the probability of failure (here, success is operationalized as the beetles being killed):</p>
<p><span class="math display">\[\begin{equation}
\frac{p}{1-p}
\end{equation}\]</span></p>
<p>and taking the <span class="math inline">\(\log\)</span> will give us log odds.</p>
<p>We are going to model log odds (instead of probability) as a linear function of dose.</p>
<p><span class="math display">\[\begin{equation}
\log \frac{p}{1-p} = \beta_0 + \beta_1 \hbox{dose}
\end{equation}\]</span></p>
<p>The model above is called the logistic regression model.</p>
<p>Once we have estimated the <span class="math inline">\(\beta\)</span> parameters,<br />
we can move back from log odds space to probability space using simple algebra.</p>
<p>Given a model like</p>
<p><span class="math display">\[\begin{equation}
\log \frac{p}{1-p} = \beta_0 + \beta_1 \hbox{dose}
\end{equation}\]</span></p>
<p>If we exponentiate each side, we get:</p>
<p><span class="math display">\[\begin{equation}
\exp \log \frac{p}{1-p} = \frac{p}{1-p} = \exp( \beta_0 + \beta_1 \hbox{dose})
\end{equation}\]</span></p>
<p>So now we just solve for p, and get (check this):</p>
<p><span class="math display">\[\begin{equation} \label{problogisticregression}
p = \frac{\exp( \beta_0 + \beta_1 \hbox{dose})}{1+\exp( \beta_0 + \beta_1 \hbox{dose})}
\end{equation}\]</span></p>
<p>We fit the model in R as follows. Note that as long as we are willing to avoid interpreting the intercept and just interpret the estimate of <span class="math inline">\(\beta_1\)</span>, there is no need to center the predictor here:</p>
<div class="sourceCode" id="cb510"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb510-1"><a href="the-essentials-of-linear-modeling-theory.html#cb510-1" aria-hidden="true" tabindex="-1"></a>fm1 <span class="ot">&lt;-</span> <span class="fu">glm</span>(propn.dead <span class="sc">~</span> dose,</span>
<span id="cb510-2"><a href="the-essentials-of-linear-modeling-theory.html#cb510-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">family =</span> <span class="fu">binomial</span>(logit),</span>
<span id="cb510-3"><a href="the-essentials-of-linear-modeling-theory.html#cb510-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">weights =</span> number,</span>
<span id="cb510-4"><a href="the-essentials-of-linear-modeling-theory.html#cb510-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> beetle</span>
<span id="cb510-5"><a href="the-essentials-of-linear-modeling-theory.html#cb510-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb510-6"><a href="the-essentials-of-linear-modeling-theory.html#cb510-6" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fm1)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = propn.dead ~ dose, family = binomial(logit), data = beetle, 
##     weights = number)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -1.594  -0.394   0.833   1.259   1.594  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)   -60.72       5.18   -11.7   &lt;2e-16 ***
## dose           34.27       2.91    11.8   &lt;2e-16 ***
## ---
## Signif. codes:  
## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 284.202  on 7  degrees of freedom
## Residual deviance:  11.232  on 6  degrees of freedom
## AIC: 41.43
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>In the glm function call, <code>binomial(logit)</code> can be written simply as <code>binomial()</code>, as the logit link is the default. Other possible links are probit, cauchit, log, and cloglog—see the documentation in R for <code>family</code> for details.</p>
<p>We can also plot the observed proportions and the fitted values together; the fit looks pretty good.</p>
<div class="sourceCode" id="cb512"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb512-1"><a href="the-essentials-of-linear-modeling-theory.html#cb512-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(propn.dead <span class="sc">~</span> dose, beetle)</span>
<span id="cb512-2"><a href="the-essentials-of-linear-modeling-theory.html#cb512-2" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(fm1<span class="sc">$</span>fitted <span class="sc">~</span> dose, beetle, <span class="at">pch =</span> <span class="dv">4</span>)</span></code></pre></div>
<p><img src="Freq_CogSci_files/figure-html/propndeadplot-1.svg" width="576" /></p>
<p>We can now compute the log odds of death for concentration 1.7552 (for example):</p>
<div class="sourceCode" id="cb513"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb513-1"><a href="the-essentials-of-linear-modeling-theory.html#cb513-1" aria-hidden="true" tabindex="-1"></a><span class="do">## compute log odds of death for</span></span>
<span id="cb513-2"><a href="the-essentials-of-linear-modeling-theory.html#cb513-2" aria-hidden="true" tabindex="-1"></a><span class="do">## concentration 1.7552:</span></span>
<span id="cb513-3"><a href="the-essentials-of-linear-modeling-theory.html#cb513-3" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>, <span class="fl">1.7552</span>))</span>
<span id="cb513-4"><a href="the-essentials-of-linear-modeling-theory.html#cb513-4" aria-hidden="true" tabindex="-1"></a><span class="co"># log odds:</span></span>
<span id="cb513-5"><a href="the-essentials-of-linear-modeling-theory.html#cb513-5" aria-hidden="true" tabindex="-1"></a>(log.odds <span class="ot">&lt;-</span> <span class="fu">t</span>(x) <span class="sc">%*%</span> <span class="fu">coef</span>(fm1))</span></code></pre></div>
<pre><code>##         [,1]
## [1,] -0.5662</code></pre>
<p>We can also obtain the variance-covariance matrix of the fitted coefficients, and use it to compute the variance-covariance matrix for the does 1.7552:</p>
<div class="sourceCode" id="cb515"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb515-1"><a href="the-essentials-of-linear-modeling-theory.html#cb515-1" aria-hidden="true" tabindex="-1"></a><span class="do">### compute CI for log odds:</span></span>
<span id="cb515-2"><a href="the-essentials-of-linear-modeling-theory.html#cb515-2" aria-hidden="true" tabindex="-1"></a><span class="do">## Get vcov matrix:</span></span>
<span id="cb515-3"><a href="the-essentials-of-linear-modeling-theory.html#cb515-3" aria-hidden="true" tabindex="-1"></a>(vcovmat <span class="ot">&lt;-</span> <span class="fu">vcov</span>(fm1))</span></code></pre></div>
<pre><code>##             (Intercept)    dose
## (Intercept)       26.84 -15.082
## dose             -15.08   8.481</code></pre>
<div class="sourceCode" id="cb517"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb517-1"><a href="the-essentials-of-linear-modeling-theory.html#cb517-1" aria-hidden="true" tabindex="-1"></a><span class="do">## x^T VCOV x for dose 1.7552:</span></span>
<span id="cb517-2"><a href="the-essentials-of-linear-modeling-theory.html#cb517-2" aria-hidden="true" tabindex="-1"></a>(var.log.odds <span class="ot">&lt;-</span> <span class="fu">t</span>(x) <span class="sc">%*%</span> vcovmat <span class="sc">%*%</span> x)</span></code></pre></div>
<pre><code>##         [,1]
## [1,] 0.02168</code></pre>
<p>Assuming that the log odds have a distribution that is approximately normal, we can compute the confidence interval for the log odds
of death given dose 1.7552:</p>
<div class="sourceCode" id="cb519"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb519-1"><a href="the-essentials-of-linear-modeling-theory.html#cb519-1" aria-hidden="true" tabindex="-1"></a><span class="do">## lower</span></span>
<span id="cb519-2"><a href="the-essentials-of-linear-modeling-theory.html#cb519-2" aria-hidden="true" tabindex="-1"></a>(lower <span class="ot">&lt;-</span> log.odds <span class="sc">-</span> <span class="fl">1.96</span> <span class="sc">*</span> <span class="fu">sqrt</span>(var.log.odds))</span></code></pre></div>
<pre><code>##         [,1]
## [1,] -0.8548</code></pre>
<div class="sourceCode" id="cb521"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb521-1"><a href="the-essentials-of-linear-modeling-theory.html#cb521-1" aria-hidden="true" tabindex="-1"></a><span class="do">## upper</span></span>
<span id="cb521-2"><a href="the-essentials-of-linear-modeling-theory.html#cb521-2" aria-hidden="true" tabindex="-1"></a>(upper <span class="ot">&lt;-</span> log.odds <span class="sc">+</span> <span class="fl">1.96</span> <span class="sc">*</span> <span class="fu">sqrt</span>(var.log.odds))</span></code></pre></div>
<pre><code>##         [,1]
## [1,] -0.2776</code></pre>
<p>The lower and upper confidence interval bounds on the
probability scale can be computed by
using equation .</p>
<div class="sourceCode" id="cb523"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb523-1"><a href="the-essentials-of-linear-modeling-theory.html#cb523-1" aria-hidden="true" tabindex="-1"></a>(meanprob <span class="ot">&lt;-</span> <span class="fu">exp</span>(log.odds) <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">+</span> <span class="fu">exp</span>(log.odds)))</span></code></pre></div>
<pre><code>##        [,1]
## [1,] 0.3621</code></pre>
<div class="sourceCode" id="cb525"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb525-1"><a href="the-essentials-of-linear-modeling-theory.html#cb525-1" aria-hidden="true" tabindex="-1"></a>(lowerprob <span class="ot">&lt;-</span> <span class="fu">exp</span>(lower) <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">+</span> <span class="fu">exp</span>(lower)))</span></code></pre></div>
<pre><code>##        [,1]
## [1,] 0.2984</code></pre>
<div class="sourceCode" id="cb527"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb527-1"><a href="the-essentials-of-linear-modeling-theory.html#cb527-1" aria-hidden="true" tabindex="-1"></a>(upperprob <span class="ot">&lt;-</span> <span class="fu">exp</span>(upper) <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">+</span> <span class="fu">exp</span>(upper)))</span></code></pre></div>
<pre><code>##       [,1]
## [1,] 0.431</code></pre>
<p>So for dose 1.7552, the probability of death is 0.36,
with 95% confidence intervals 0.30 and
0.43.</p>
<p>Note that one should not try to predict outside the range of the design matrix. For example, in the beetle data, the dose ranges from 1.69 to 1.88. We should not try to compute probabilities for dose 2.5, say, since we have no knowledge about whether the relationship remains unchanged beyond the upper bound of our design matrix.</p>
</div>
<div id="multiple-logistic-regression-example-from-hindi-data" class="section level4" number="5.2.5.2">
<h4><span class="header-section-number">5.2.5.2</span> Multiple logistic regression: Example from Hindi data}</h4>
<p>We have eyetracking data from 10 participants reading Hindi sentences. The data come from <span class="citation"><a href="#ref-HusainVasishthNarayanan2015" role="doc-biblioref">Husain, Vasishth, and Srinivasan</a> (<a href="#ref-HusainVasishthNarayanan2015" role="doc-biblioref">2015</a>)</span>. We can compute skipping probability, the probability of skipping a word entirely (i.e., never fixating it). We first have to create a vector that has value 1 if the word has 0~ms total reading time, and 0 otherwise.</p>
<div class="sourceCode" id="cb529"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb529-1"><a href="the-essentials-of-linear-modeling-theory.html#cb529-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(lingpsych)</span>
<span id="cb529-2"><a href="the-essentials-of-linear-modeling-theory.html#cb529-2" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(<span class="st">&quot;df_hindi&quot;</span>)</span>
<span id="cb529-3"><a href="the-essentials-of-linear-modeling-theory.html#cb529-3" aria-hidden="true" tabindex="-1"></a>hindi10 <span class="ot">&lt;-</span> df_hindi</span>
<span id="cb529-4"><a href="the-essentials-of-linear-modeling-theory.html#cb529-4" aria-hidden="true" tabindex="-1"></a>skip <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(hindi10<span class="sc">$</span>TFT <span class="sc">==</span> <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb529-5"><a href="the-essentials-of-linear-modeling-theory.html#cb529-5" aria-hidden="true" tabindex="-1"></a>hindi10<span class="sc">$</span>skip <span class="ot">&lt;-</span> skip</span>
<span id="cb529-6"><a href="the-essentials-of-linear-modeling-theory.html#cb529-6" aria-hidden="true" tabindex="-1"></a><span class="do">## display relevant columns:</span></span>
<span id="cb529-7"><a href="the-essentials-of-linear-modeling-theory.html#cb529-7" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(hindi10[, <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">24</span>, <span class="dv">33</span>, <span class="dv">34</span>)])</span></code></pre></div>
<pre><code>##   subj expt item word_complex SC skip
## 1   10 hnd1    6          0.0  1    1
## 2   10 hnd1    6          0.0  1    1
## 3   10 hnd1    6          0.0  2    0
## 4   10 hnd1    6          1.5  1    1
## 5   10 hnd1    6          0.0  1    1
## 6   10 hnd1    6          0.5  1    0</code></pre>
<div class="sourceCode" id="cb531"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb531-1"><a href="the-essentials-of-linear-modeling-theory.html#cb531-1" aria-hidden="true" tabindex="-1"></a>fm_skip <span class="ot">&lt;-</span> <span class="fu">glm</span>(skip <span class="sc">~</span> word_complex <span class="sc">+</span> SC, <span class="at">family =</span> <span class="fu">binomial</span>(), hindi10)</span>
<span id="cb531-2"><a href="the-essentials-of-linear-modeling-theory.html#cb531-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fm_skip)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = skip ~ word_complex + SC, family = binomial(), 
##     data = hindi10)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -1.100  -0.884  -0.674   1.256   2.682  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)   -0.1838     0.0266   -6.91  4.7e-12 ***
## word_complex  -0.6291     0.0281  -22.38  &lt; 2e-16 ***
## SC            -0.5538     0.0224  -24.70  &lt; 2e-16 ***
## ---
## Signif. codes:  
## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 31753  on 27065  degrees of freedom
## Residual deviance: 30492  on 27063  degrees of freedom
## AIC: 30498
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>The above example also illustrates the second way to set up the data for logistic (multiple) regression: the dependent variable can simply be a 1,0 value instead of proportions. So, in the beetle data, you could recode the data to have 1s and 0s instead of proportions. Assuming that you have recoded the column for status (dead or alive after exposure), the glm function call for the Beetle data-set would be:</p>
<p>Note that logistic regression assumes independence of each data point; this assumption is violated in the Hindi data, because it has repeated measures data.</p>
</div>
<div id="deviance" class="section level4" number="5.2.5.3">
<h4><span class="header-section-number">5.2.5.3</span> Deviance</h4>
<p>We saw encountered deviance earlier in connection with ANOVA.</p>
<p>The deviance is more generally defined as</p>
<p><span class="math display">\[\begin{equation}
D = 2[logLik(\bar{x}; y) - logLik(mu_0; y)]
\end{equation}\]</span></p>
<p>
where <span class="math inline">\(logLik(\bar{x}; y)\)</span> is the log likelihood of the saturated model (the model with the maximal number of parameters that can be fit), and <span class="math inline">\(logLik(mu_0; y)\)</span> is the log likelihood of the model with parameter of interest having the null hypothesis value <span class="math inline">\(\mu_0\)</span>.
As we saw earlier, D has a chi-squared distribution.</p>
<p>The deviance for the normal distribution is</p>
<p><span class="math display">\[\begin{equation}
D = \frac{1}{\sigma^2}\sum (y_i - \bar{y})^2
\end{equation}\]</span></p>
<p>[See p. 80 onwards in the Dobson et al book for proofs and more detail.]</p>
<p>Deviance for the binomial distribution is defined as <span class="math inline">\(D=\sum d_i\)</span>, where:</p>
<p><span class="math display">\[\begin{equation}
d_i = -2 \times n_i [ y_i \log\left(\frac{\hat{\mu}_i}{y_i}\right) + (1-y_i) \log \left(\frac{1-\hat{\mu}_i}{1-y_i}\right) ]  
\end{equation}\]</span></p>
<p>The basic idea here is that if the model fit is good, Deviance will have a <span class="math inline">\(\chi^2\)</span> distribution with <span class="math inline">\(N-p\)</span> degrees of freedom, where <span class="math inline">\(N\)</span> is the number of data-points, and <span class="math inline">\(p\)</span> the number of parameters.
So that is what we will use for assessing model fit.</p>
<p>We will also use deviance for hypothesis testing.
The difference in deviance (confusingly called residual deviance) between two models also has a <span class="math inline">\(\chi^2\)</span> distribution (this should remind you of ANOVA), with dfs being <span class="math inline">\(p-q\)</span>, where <span class="math inline">\(q\)</span> is the number of parameters in the nukk model, and <span class="math inline">\(p\)</span> the number of parameters in the full model.</p>
<p>We discuss hypothesis testing first, then evaluating goodness of fit using deviance.</p>
<div id="hypothesis-testing-residual-deviance" class="section level5" number="5.2.5.3.1">
<h5><span class="header-section-number">5.2.5.3.1</span> Hypothesis testing: Residual deviance</h5>
<p>Returning to our beetle data, let’s say we fit our model:</p>
<div class="sourceCode" id="cb533"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb533-1"><a href="the-essentials-of-linear-modeling-theory.html#cb533-1" aria-hidden="true" tabindex="-1"></a>glm1 <span class="ot">&lt;-</span> <span class="fu">glm</span>(propn.dead <span class="sc">~</span> dose, <span class="fu">binomial</span>(logit),</span>
<span id="cb533-2"><a href="the-essentials-of-linear-modeling-theory.html#cb533-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">weights =</span> number, <span class="at">data =</span> beetle</span>
<span id="cb533-3"><a href="the-essentials-of-linear-modeling-theory.html#cb533-3" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p>The summary output shows us the number of iterations that led to the parameter estimates:</p>
<div class="sourceCode" id="cb534"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb534-1"><a href="the-essentials-of-linear-modeling-theory.html#cb534-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(glm1)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = propn.dead ~ dose, family = binomial(logit), data = beetle, 
##     weights = number)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -1.594  -0.394   0.833   1.259   1.594  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)   -60.72       5.18   -11.7   &lt;2e-16 ***
## dose           34.27       2.91    11.8   &lt;2e-16 ***
## ---
## Signif. codes:  
## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 284.202  on 7  degrees of freedom
## Residual deviance:  11.232  on 6  degrees of freedom
## AIC: 41.43
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>But we also see something called <strong>Null deviance</strong> and <strong>Residual deviance</strong>. These are used to evaluate quality of model fit. Recall that we can compute the fitted values and compare them to the observed values:</p>
<div class="sourceCode" id="cb536"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb536-1"><a href="the-essentials-of-linear-modeling-theory.html#cb536-1" aria-hidden="true" tabindex="-1"></a><span class="co"># beta.hat is (-60.71745 ,   34.27033)</span></span>
<span id="cb536-2"><a href="the-essentials-of-linear-modeling-theory.html#cb536-2" aria-hidden="true" tabindex="-1"></a>(eta.hat <span class="ot">&lt;-</span> <span class="sc">-</span><span class="fl">60.71745</span> <span class="sc">+</span> <span class="fl">34.27033</span> <span class="sc">*</span> beetle<span class="sc">$</span>dose)</span></code></pre></div>
<pre><code>## [1] -2.7766 -1.6285 -0.5662  0.4277  1.3564  2.2337
## [7]  3.0596  3.8444</code></pre>
<div class="sourceCode" id="cb538"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb538-1"><a href="the-essentials-of-linear-modeling-theory.html#cb538-1" aria-hidden="true" tabindex="-1"></a>(mu.hat <span class="ot">&lt;-</span> <span class="fu">exp</span>(eta.hat) <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">+</span> <span class="fu">exp</span>(eta.hat)))</span></code></pre></div>
<pre><code>## [1] 0.0586 0.1640 0.3621 0.6053 0.7952 0.9032 0.9552
## [8] 0.9790</code></pre>
<div class="sourceCode" id="cb540"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb540-1"><a href="the-essentials-of-linear-modeling-theory.html#cb540-1" aria-hidden="true" tabindex="-1"></a><span class="co"># compare mu.hat with observed proportions</span></span>
<span id="cb540-2"><a href="the-essentials-of-linear-modeling-theory.html#cb540-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(mu.hat, beetle<span class="sc">$</span>propn.dead)</span>
<span id="cb540-3"><a href="the-essentials-of-linear-modeling-theory.html#cb540-3" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="dv">0</span>, <span class="dv">1</span>)</span></code></pre></div>
<p><img src="Freq_CogSci_files/figure-html/propndead2-1.svg" width="576" /></p>
<p>To evaluate whether dose has an effect, we will do something analogous to the model comparison methods we saw earlier. First, fit a model with only an intercept. Notice that the null deviance is 284 on 7 degrees of freedom.</p>
<div class="sourceCode" id="cb541"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb541-1"><a href="the-essentials-of-linear-modeling-theory.html#cb541-1" aria-hidden="true" tabindex="-1"></a>null.glm <span class="ot">&lt;-</span> <span class="fu">glm</span>(propn.dead <span class="sc">~</span> <span class="dv">1</span>, <span class="fu">binomial</span>(logit),</span>
<span id="cb541-2"><a href="the-essentials-of-linear-modeling-theory.html#cb541-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">weights =</span> number, <span class="at">data =</span> beetle</span>
<span id="cb541-3"><a href="the-essentials-of-linear-modeling-theory.html#cb541-3" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb541-4"><a href="the-essentials-of-linear-modeling-theory.html#cb541-4" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(null.glm)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = propn.dead ~ 1, family = binomial(logit), data = beetle, 
##     weights = number)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
##  -8.11   -5.29    1.10    5.62    7.77  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)   0.4263     0.0933    4.57  4.9e-06 ***
## ---
## Signif. codes:  
## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 284.2  on 7  degrees of freedom
## Residual deviance: 284.2  on 7  degrees of freedom
## AIC: 312.4
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<div class="sourceCode" id="cb543"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb543-1"><a href="the-essentials-of-linear-modeling-theory.html#cb543-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(beetle<span class="sc">$</span>dose, beetle<span class="sc">$</span>propn.dead,</span>
<span id="cb543-2"><a href="the-essentials-of-linear-modeling-theory.html#cb543-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">xlab =</span> <span class="st">&quot;log concentration&quot;</span>,</span>
<span id="cb543-3"><a href="the-essentials-of-linear-modeling-theory.html#cb543-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">ylab =</span> <span class="st">&quot;proportion dead&quot;</span>, <span class="at">main =</span> <span class="st">&quot;minimal model&quot;</span></span>
<span id="cb543-4"><a href="the-essentials-of-linear-modeling-theory.html#cb543-4" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb543-5"><a href="the-essentials-of-linear-modeling-theory.html#cb543-5" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(beetle<span class="sc">$</span>dose, null.glm<span class="sc">$</span>fitted, <span class="at">pch =</span> <span class="dv">4</span>)</span></code></pre></div>
<p><img src="Freq_CogSci_files/figure-html/propndead3-1.svg" width="576" /></p>
<p>Add a term for dose. Now, the residual deviance is 11.2 on 6 dfs:</p>
<div class="sourceCode" id="cb544"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb544-1"><a href="the-essentials-of-linear-modeling-theory.html#cb544-1" aria-hidden="true" tabindex="-1"></a>dose.glm <span class="ot">&lt;-</span> <span class="fu">glm</span>(propn.dead <span class="sc">~</span> dose, <span class="fu">binomial</span>(logit),</span>
<span id="cb544-2"><a href="the-essentials-of-linear-modeling-theory.html#cb544-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">weights =</span> number, <span class="at">data =</span> beetle</span>
<span id="cb544-3"><a href="the-essentials-of-linear-modeling-theory.html#cb544-3" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb544-4"><a href="the-essentials-of-linear-modeling-theory.html#cb544-4" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(dose.glm)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = propn.dead ~ dose, family = binomial(logit), data = beetle, 
##     weights = number)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -1.594  -0.394   0.833   1.259   1.594  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)   -60.72       5.18   -11.7   &lt;2e-16 ***
## dose           34.27       2.91    11.8   &lt;2e-16 ***
## ---
## Signif. codes:  
## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 284.202  on 7  degrees of freedom
## Residual deviance:  11.232  on 6  degrees of freedom
## AIC: 41.43
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<div class="sourceCode" id="cb546"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb546-1"><a href="the-essentials-of-linear-modeling-theory.html#cb546-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(beetle<span class="sc">$</span>dose, beetle<span class="sc">$</span>propn.dead,</span>
<span id="cb546-2"><a href="the-essentials-of-linear-modeling-theory.html#cb546-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">xlab =</span> <span class="st">&quot;log concentration&quot;</span>,</span>
<span id="cb546-3"><a href="the-essentials-of-linear-modeling-theory.html#cb546-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">ylab =</span> <span class="st">&quot;proportion dead&quot;</span>, <span class="at">main =</span> <span class="st">&quot;dose model&quot;</span></span>
<span id="cb546-4"><a href="the-essentials-of-linear-modeling-theory.html#cb546-4" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb546-5"><a href="the-essentials-of-linear-modeling-theory.html#cb546-5" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(beetle<span class="sc">$</span>dose, dose.glm<span class="sc">$</span>fitted, <span class="at">pch =</span> <span class="dv">4</span>)</span></code></pre></div>
<p><img src="Freq_CogSci_files/figure-html/propndead4-1.svg" width="576" /></p>
<p>The change in deviance from the null model is 284.2-11.2=273 on 1 df. Since the critical <span class="math inline">\(\chi_1^2 = 3.84\)</span>, we reject the null hypothesis that <span class="math inline">\(\beta_1 = 0\)</span>.</p>
<p>You can do the model comparison using the anova function. Note that no statistical test is calculated; you need to do that yourself.</p>
<div class="sourceCode" id="cb547"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb547-1"><a href="the-essentials-of-linear-modeling-theory.html#cb547-1" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(null.glm, dose.glm)</span></code></pre></div>
<pre><code>## Analysis of Deviance Table
## 
## Model 1: propn.dead ~ 1
## Model 2: propn.dead ~ dose
##   Resid. Df Resid. Dev Df Deviance
## 1         7      284.2            
## 2         6       11.2  1      273</code></pre>
<p>Actually, you don’t even need to define the null model; the anova function automatically compares the fitted model to the null model:</p>
<div class="sourceCode" id="cb549"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb549-1"><a href="the-essentials-of-linear-modeling-theory.html#cb549-1" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(dose.glm)</span></code></pre></div>
<pre><code>## Analysis of Deviance Table
## 
## Model: binomial, link: logit
## 
## Response: propn.dead
## 
## Terms added sequentially (first to last)
## 
## 
##      Df Deviance Resid. Df Resid. Dev
## NULL                     7      284.2
## dose  1      273         6       11.2</code></pre>
</div>
<div id="assessing-goodness-of-fit-of-a-fitted-model" class="section level5" number="5.2.5.3.2">
<h5><span class="header-section-number">5.2.5.3.2</span> Assessing goodness of fit of a fitted model</h5>
<p>The deviance for a given degrees of freedom <span class="math inline">\(v\)</span> should have a <span class="math inline">\(\chi_v^2\)</span> distribution for the model to be adequate. As an example, consider the null model above. The deviance is clearly much larger than the 95th percentile cutoff point of the chi-squared distribution with 7 dfs, so the model is not adequate.</p>
<div class="sourceCode" id="cb551"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb551-1"><a href="the-essentials-of-linear-modeling-theory.html#cb551-1" aria-hidden="true" tabindex="-1"></a><span class="fu">deviance</span>(null.glm)</span></code></pre></div>
<pre><code>## [1] 284.2</code></pre>
<div class="sourceCode" id="cb553"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb553-1"><a href="the-essentials-of-linear-modeling-theory.html#cb553-1" aria-hidden="true" tabindex="-1"></a><span class="do">## critical value:</span></span>
<span id="cb553-2"><a href="the-essentials-of-linear-modeling-theory.html#cb553-2" aria-hidden="true" tabindex="-1"></a><span class="fu">qchisq</span>(<span class="fl">0.95</span>, <span class="at">df =</span> <span class="dv">7</span>)</span></code></pre></div>
<pre><code>## [1] 14.07</code></pre>
<p>Now consider the model with dose as predictor. The deviance is less than the 95th percentile, so the fit is adequate.</p>
<div class="sourceCode" id="cb555"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb555-1"><a href="the-essentials-of-linear-modeling-theory.html#cb555-1" aria-hidden="true" tabindex="-1"></a><span class="fu">deviance</span>(dose.glm)</span></code></pre></div>
<pre><code>## [1] 11.23</code></pre>
<div class="sourceCode" id="cb557"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb557-1"><a href="the-essentials-of-linear-modeling-theory.html#cb557-1" aria-hidden="true" tabindex="-1"></a><span class="fu">qchisq</span>(<span class="fl">0.95</span>, <span class="at">df =</span> <span class="dv">6</span>)</span></code></pre></div>
<pre><code>## [1] 12.59</code></pre>
</div>
<div id="residuals-in-glms" class="section level5" number="5.2.5.3.3">
<h5><span class="header-section-number">5.2.5.3.3</span> Residuals in GLMs</h5>
<p>In the binomial distribution, Deviance <span class="math inline">\(D=\sum d_i\)</span>, where:</p>
<p><span class="math display">\[\begin{equation}
d_i = -2 \times n_i [ y_i \log(\frac{\hat{\mu}_i}{y_i}) + (1-y_i) \log (\frac{1-\hat{\mu}_i}{1-y_i}) ]  
\end{equation}\]</span></p>
<p>The <span class="math inline">\(i\)</span>-th deviance residual is defined as:</p>
<p><span class="math display">\[\begin{equation}
  e_{D,i}= sgn(y_i-\hat{\mu}_i) \times \sqrt{d_i}
\end{equation}\]</span></p>
<p>These can be used to check for model adequacy as discussed earlier in the context of linear models.
One can just use the plot function inspect the residuals:</p>
<div class="sourceCode" id="cb559"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb559-1"><a href="the-essentials-of-linear-modeling-theory.html#cb559-1" aria-hidden="true" tabindex="-1"></a>op <span class="ot">&lt;-</span> <span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>), <span class="at">pty =</span> <span class="st">&quot;s&quot;</span>)</span>
<span id="cb559-2"><a href="the-essentials-of-linear-modeling-theory.html#cb559-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(dose.glm)</span></code></pre></div>
<p><img src="Freq_CogSci_files/figure-html/residualsglm-1.svg" width="576" /></p>
<p>Alternatively, one can do this by hand:</p>
<div class="sourceCode" id="cb560"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb560-1"><a href="the-essentials-of-linear-modeling-theory.html#cb560-1" aria-hidden="true" tabindex="-1"></a>op <span class="ot">&lt;-</span> <span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>), <span class="at">pty =</span> <span class="st">&quot;s&quot;</span>)</span>
<span id="cb560-2"><a href="the-essentials-of-linear-modeling-theory.html#cb560-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(dose.glm<span class="sc">$</span>resid,</span>
<span id="cb560-3"><a href="the-essentials-of-linear-modeling-theory.html#cb560-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">xlab =</span> <span class="st">&quot;index&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;residuals&quot;</span>, <span class="at">main =</span> <span class="st">&quot;Index plot&quot;</span></span>
<span id="cb560-4"><a href="the-essentials-of-linear-modeling-theory.html#cb560-4" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb560-5"><a href="the-essentials-of-linear-modeling-theory.html#cb560-5" aria-hidden="true" tabindex="-1"></a><span class="fu">qqnorm</span>(dose.glm<span class="sc">$</span>resid, <span class="at">main =</span> <span class="st">&quot;QQ-plot&quot;</span>)</span>
<span id="cb560-6"><a href="the-essentials-of-linear-modeling-theory.html#cb560-6" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(dose.glm<span class="sc">$</span>resid, <span class="at">xlab =</span> <span class="st">&quot;Residuals&quot;</span>, <span class="at">main =</span> <span class="st">&quot;Histogram&quot;</span>)</span>
<span id="cb560-7"><a href="the-essentials-of-linear-modeling-theory.html#cb560-7" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(dose.glm<span class="sc">$</span>fit, dose.glm<span class="sc">$</span>resid,</span>
<span id="cb560-8"><a href="the-essentials-of-linear-modeling-theory.html#cb560-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">xlab =</span> <span class="st">&quot;Fitted values&quot;</span>,</span>
<span id="cb560-9"><a href="the-essentials-of-linear-modeling-theory.html#cb560-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">ylab =</span> <span class="st">&quot;Residuals&quot;</span>,</span>
<span id="cb560-10"><a href="the-essentials-of-linear-modeling-theory.html#cb560-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">main =</span> <span class="st">&quot;Residuals versus fitted values&quot;</span></span>
<span id="cb560-11"><a href="the-essentials-of-linear-modeling-theory.html#cb560-11" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p><img src="Freq_CogSci_files/figure-html/qqnormglm-1.svg" width="576" /></p>
</div>
</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-bliss1935calculation" class="csl-entry">
Bliss, Chester Ittner. 1935. <span>“The Calculation of the Dosage-Mortality Curve.”</span> <em>Annals of Applied Biology</em> 22 (1): 134–67.
</div>
<div id="ref-dobson2011introduction" class="csl-entry">
Dobson, Annette J, and Adrian Barnett. 2011. <em>An Introduction to Generalized Linear Models</em>. CRC press.
</div>
<div id="ref-HusainVasishthNarayanan2015" class="csl-entry">
Husain, Samar, Shravan Vasishth, and Narayanan Srinivasan. 2015. <span>“Integration and Prediction Difficulty in <span>H</span>indi Sentence Comprehension: <span>E</span>vidence from an Eye-Tracking Corpus.”</span> <em>Journal of Eye Movement Research</em> 8(2): 1–12.
</div>
<div id="ref-influenceme" class="csl-entry">
Nieuwenhuis, Rense, Manfred Te Grotenhuis, and Ben Pelzer. 2012. <span>“Influence.ME: Tools for Detecting Influential Data in Mixed Effects Models.”</span> <em>R Journal</em> 4 (2): 38–47.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="a-quick-review-of-some-basic-concepts-in-matrix-algebra.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="summary-4.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown/edit/master/inst/examples/05-LinearModelsMatrixForm.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Freq_CogSci.pdf", "Freq_CogSci.epub", "Freq_CogSci.mobi"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
