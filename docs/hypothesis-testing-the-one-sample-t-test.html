<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2.4 Hypothesis testing: The one sample t-test | Linear Mixed Models in Linguistics and Psychology: A Comprehensive Introduction</title>
  <meta name="description" content="Linear Mixed Models for Linguistics and Psychology: A Comprehensive Introduction" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="2.4 Hypothesis testing: The one sample t-test | Linear Mixed Models in Linguistics and Psychology: A Comprehensive Introduction" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://github.com/vasishth/Freq_CogSci" />
  <meta property="og:image" content="https://github.com/vasishth/Freq_CogSciimages/temporarycover.jpg" />
  <meta property="og:description" content="Linear Mixed Models for Linguistics and Psychology: A Comprehensive Introduction" />
  <meta name="github-repo" content="rstudio/bookdown" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2.4 Hypothesis testing: The one sample t-test | Linear Mixed Models in Linguistics and Psychology: A Comprehensive Introduction" />
  
  <meta name="twitter:description" content="Linear Mixed Models for Linguistics and Psychology: A Comprehensive Introduction" />
  <meta name="twitter:image" content="https://github.com/vasishth/Freq_CogSciimages/temporarycover.jpg" />

<meta name="author" content="Shravan Vasishth, Daniel Schad, Audrey Bürki, Reinhold Kliegl" />


<meta name="date" content="2020-08-06" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="the-confidence-interval-and-what-its-good-for.html"/>
<link rel="next" href="the-two-sample-t-test-vs-the-paired-t-test.html"/>
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    background-color: #232629;
    color: #7a7c7d;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #7a7c7d;  padding-left: 4px; }
div.sourceCode
  { color: #cfcfc2; background-color: #232629; }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span. { color: #cfcfc2; } /* Normal */
code span.al { color: #95da4c; } /* Alert */
code span.an { color: #3f8058; } /* Annotation */
code span.at { color: #2980b9; } /* Attribute */
code span.bn { color: #f67400; } /* BaseN */
code span.bu { color: #7f8c8d; } /* BuiltIn */
code span.cf { color: #fdbc4b; } /* ControlFlow */
code span.ch { color: #3daee9; } /* Char */
code span.cn { color: #27aeae; } /* Constant */
code span.co { color: #7a7c7d; } /* Comment */
code span.cv { color: #7f8c8d; } /* CommentVar */
code span.do { color: #a43340; } /* Documentation */
code span.dt { color: #2980b9; } /* DataType */
code span.dv { color: #f67400; } /* DecVal */
code span.er { color: #da4453; } /* Error */
code span.ex { color: #0099ff; } /* Extension */
code span.fl { color: #f67400; } /* Float */
code span.fu { color: #8e44ad; } /* Function */
code span.im { color: #27ae60; } /* Import */
code span.in { color: #c45b00; } /* Information */
code span.kw { color: #cfcfc2; } /* Keyword */
code span.op { color: #cfcfc2; } /* Operator */
code span.ot { color: #27ae60; } /* Other */
code span.pp { color: #27ae60; } /* Preprocessor */
code span.re { color: #2980b9; } /* RegionMarker */
code span.sc { color: #3daee9; } /* SpecialChar */
code span.ss { color: #da4453; } /* SpecialString */
code span.st { color: #f44f4f; } /* String */
code span.va { color: #27aeae; } /* Variable */
code span.vs { color: #da4453; } /* VerbatimString */
code span.wa { color: #da4453; } /* Warning */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Authoring Books with R Markdown</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="0.1" data-path="prerequisites.html"><a href="prerequisites.html"><i class="fa fa-check"></i><b>0.1</b> Prerequisites</a></li>
<li class="chapter" data-level="0.2" data-path="how-to-read-this-book.html"><a href="how-to-read-this-book.html"><i class="fa fa-check"></i><b>0.2</b> How to read this book</a></li>
<li class="chapter" data-level="0.3" data-path="online-materials.html"><a href="online-materials.html"><i class="fa fa-check"></i><b>0.3</b> Online materials</a></li>
<li class="chapter" data-level="0.4" data-path="software-needed.html"><a href="software-needed.html"><i class="fa fa-check"></i><b>0.4</b> Software needed</a></li>
<li class="chapter" data-level="0.5" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i><b>0.5</b> Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="about-the-authors.html"><a href="about-the-authors.html"><i class="fa fa-check"></i>About the Authors</a></li>
<li class="part"><span><b>I Foundational ideas</b></span></li>
<li class="chapter" data-level="1" data-path="some-important-facts-about-distributions.html"><a href="some-important-facts-about-distributions.html"><i class="fa fa-check"></i><b>1</b> Some important facts about distributions</a><ul>
<li class="chapter" data-level="1.1" data-path="discrete-random-variables-an-example-using-the-binomial-distribution.html"><a href="discrete-random-variables-an-example-using-the-binomial-distribution.html"><i class="fa fa-check"></i><b>1.1</b> Discrete random variables: An example using the Binomial distribution</a><ul>
<li class="chapter" data-level="1.1.1" data-path="discrete-random-variables-an-example-using-the-binomial-distribution.html"><a href="discrete-random-variables-an-example-using-the-binomial-distribution.html#the-mean-and-variance-of-the-binomial-distribution"><i class="fa fa-check"></i><b>1.1.1</b> The mean and variance of the Binomial distribution</a></li>
<li class="chapter" data-level="1.1.2" data-path="discrete-random-variables-an-example-using-the-binomial-distribution.html"><a href="discrete-random-variables-an-example-using-the-binomial-distribution.html#what-information-does-a-probability-distribution-provide"><i class="fa fa-check"></i><b>1.1.2</b> What information does a probability distribution provide?</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="continuous-random-variables-an-example-using-the-normal-distribution.html"><a href="continuous-random-variables-an-example-using-the-normal-distribution.html"><i class="fa fa-check"></i><b>1.2</b> Continuous random variables: An example using the Normal distribution</a></li>
<li class="chapter" data-level="1.3" data-path="bivariatemultivariate-distributions.html"><a href="bivariatemultivariate-distributions.html"><i class="fa fa-check"></i><b>1.3</b> Bivariate/multivariate distributions</a></li>
<li class="chapter" data-level="1.4" data-path="summary-of-useful-r-functions-relating-to-univariate-distributions.html"><a href="summary-of-useful-r-functions-relating-to-univariate-distributions.html"><i class="fa fa-check"></i><b>1.4</b> Summary of useful R functions relating to univariate distributions</a></li>
<li class="chapter" data-level="1.5" data-path="summary-of-random-variable-theory.html"><a href="summary-of-random-variable-theory.html"><i class="fa fa-check"></i><b>1.5</b> *Summary of random variable theory</a></li>
<li class="chapter" data-level="1.6" data-path="further-reading.html"><a href="further-reading.html"><i class="fa fa-check"></i><b>1.6</b> Further reading</a></li>
<li class="chapter" data-level="1.7" data-path="sec-Foundationsexercises.html"><a href="sec-Foundationsexercises.html"><i class="fa fa-check"></i><b>1.7</b> Exercises</a><ul>
<li class="chapter" data-level="1.7.1" data-path="sec-Foundationsexercises.html"><a href="sec-Foundationsexercises.html#sec:Foundationsexercisespnorm"><i class="fa fa-check"></i><b>1.7.1</b> Practice using the <code>pnorm</code> function</a></li>
<li class="chapter" data-level="1.7.2" data-path="sec-Foundationsexercises.html"><a href="sec-Foundationsexercises.html#sec:Foundationsexercisesqnorm"><i class="fa fa-check"></i><b>1.7.2</b> Practice using the <code>qnorm</code> function</a></li>
<li class="chapter" data-level="1.7.3" data-path="sec-Foundationsexercises.html"><a href="sec-Foundationsexercises.html#sec:Foundationsexercisesqt"><i class="fa fa-check"></i><b>1.7.3</b> Practice using <code>qt</code></a></li>
<li class="chapter" data-level="1.7.4" data-path="sec-Foundationsexercises.html"><a href="sec-Foundationsexercises.html#sec:FoundationsexercisesMLE1"><i class="fa fa-check"></i><b>1.7.4</b> Maximum likelihood estimation 1</a></li>
<li class="chapter" data-level="1.7.5" data-path="sec-Foundationsexercises.html"><a href="sec-Foundationsexercises.html#sec:FoundationsexercisesMLE2"><i class="fa fa-check"></i><b>1.7.5</b> Maximum likelihood estimation 2</a></li>
<li class="chapter" data-level="1.7.6" data-path="sec-Foundationsexercises.html"><a href="sec-Foundationsexercises.html#sec:Foundationsexercisesbivar"><i class="fa fa-check"></i><b>1.7.6</b> Generating bivariate data</a></li>
<li class="chapter" data-level="1.7.7" data-path="sec-Foundationsexercises.html"><a href="sec-Foundationsexercises.html#sec:Foundationsexercisesmultivar"><i class="fa fa-check"></i><b>1.7.7</b> Generating multivariate data</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="hypothetical-repeated-sampling-and-the-t-test.html"><a href="hypothetical-repeated-sampling-and-the-t-test.html"><i class="fa fa-check"></i><b>2</b> Hypothetical repeated sampling and the t-test</a><ul>
<li class="chapter" data-level="2.1" data-path="the-central-limit-theorem-using-simulation.html"><a href="the-central-limit-theorem-using-simulation.html"><i class="fa fa-check"></i><b>2.1</b> The central limit theorem using simulation</a></li>
<li class="chapter" data-level="2.2" data-path="three-examples-of-the-sampling-distribution.html"><a href="three-examples-of-the-sampling-distribution.html"><i class="fa fa-check"></i><b>2.2</b> Three examples of the sampling distribution</a></li>
<li class="chapter" data-level="2.3" data-path="the-confidence-interval-and-what-its-good-for.html"><a href="the-confidence-interval-and-what-its-good-for.html"><i class="fa fa-check"></i><b>2.3</b> The confidence interval, and what it’s good for</a></li>
<li class="chapter" data-level="2.4" data-path="hypothesis-testing-the-one-sample-t-test.html"><a href="hypothesis-testing-the-one-sample-t-test.html"><i class="fa fa-check"></i><b>2.4</b> Hypothesis testing: The one sample t-test</a><ul>
<li class="chapter" data-level="2.4.1" data-path="hypothesis-testing-the-one-sample-t-test.html"><a href="hypothesis-testing-the-one-sample-t-test.html#the-one-sample-t-test"><i class="fa fa-check"></i><b>2.4.1</b> The one-sample t-test</a></li>
<li class="chapter" data-level="2.4.2" data-path="hypothesis-testing-the-one-sample-t-test.html"><a href="hypothesis-testing-the-one-sample-t-test.html#type-i-ii-error-and-power"><i class="fa fa-check"></i><b>2.4.2</b> Type I, II error, and power</a></li>
<li class="chapter" data-level="2.4.3" data-path="hypothesis-testing-the-one-sample-t-test.html"><a href="hypothesis-testing-the-one-sample-t-test.html#how-to-compute-power-for-the-one-sample-t-test"><i class="fa fa-check"></i><b>2.4.3</b> How to compute power for the one-sample t-test</a></li>
<li class="chapter" data-level="2.4.4" data-path="hypothesis-testing-the-one-sample-t-test.html"><a href="hypothesis-testing-the-one-sample-t-test.html#the-p-value"><i class="fa fa-check"></i><b>2.4.4</b> The p-value</a></li>
<li class="chapter" data-level="2.4.5" data-path="hypothesis-testing-the-one-sample-t-test.html"><a href="hypothesis-testing-the-one-sample-t-test.html#type-m-and-s-error-in-the-face-of-low-power"><i class="fa fa-check"></i><b>2.4.5</b> Type M and S error in the face of low power</a></li>
<li class="chapter" data-level="2.4.6" data-path="hypothesis-testing-the-one-sample-t-test.html"><a href="hypothesis-testing-the-one-sample-t-test.html#searching-for-significance"><i class="fa fa-check"></i><b>2.4.6</b> Searching for significance</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="the-two-sample-t-test-vs-the-paired-t-test.html"><a href="the-two-sample-t-test-vs-the-paired-t-test.html"><i class="fa fa-check"></i><b>2.5</b> The two-sample t-test vs. the paired t-test</a><ul>
<li class="chapter" data-level="2.5.1" data-path="the-two-sample-t-test-vs-the-paired-t-test.html"><a href="the-two-sample-t-test-vs-the-paired-t-test.html#common-mistakes-involving-the-t-test"><i class="fa fa-check"></i><b>2.5.1</b> Common mistakes involving the t-test</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="sec-SamplingDistrnexercises.html"><a href="sec-SamplingDistrnexercises.html"><i class="fa fa-check"></i><b>2.6</b> Exercises</a><ul>
<li class="chapter" data-level="2.6.1" data-path="sec-SamplingDistrnexercises.html"><a href="sec-SamplingDistrnexercises.html#sec:SamplingDistrnexercisesPart1"><i class="fa fa-check"></i><b>2.6.1</b> Part 1</a></li>
<li class="chapter" data-level="2.6.2" data-path="sec-SamplingDistrnexercises.html"><a href="sec-SamplingDistrnexercises.html#sec:SamplingDistrnexercisesPart2"><i class="fa fa-check"></i><b>2.6.2</b> Part 2</a></li>
<li class="chapter" data-level="2.6.3" data-path="sec-SamplingDistrnexercises.html"><a href="sec-SamplingDistrnexercises.html#sec:SamplingDistrnexercisesPart3"><i class="fa fa-check"></i><b>2.6.3</b> Part 3</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="linear-models-and-linear-mixed-models.html"><a href="linear-models-and-linear-mixed-models.html"><i class="fa fa-check"></i><b>3</b> Linear models and linear mixed models</a><ul>
<li class="chapter" data-level="3.1" data-path="from-the-t-test-to-the-linear-mixed-model.html"><a href="from-the-t-test-to-the-linear-mixed-model.html"><i class="fa fa-check"></i><b>3.1</b> From the t-test to the linear (mixed) model</a></li>
<li class="chapter" data-level="3.2" data-path="sum-coding.html"><a href="sum-coding.html"><i class="fa fa-check"></i><b>3.2</b> Sum coding</a></li>
<li class="chapter" data-level="3.3" data-path="checking-model-assumptions.html"><a href="checking-model-assumptions.html"><i class="fa fa-check"></i><b>3.3</b> Checking model assumptions</a></li>
<li class="chapter" data-level="3.4" data-path="from-the-paired-t-test-to-the-linear-mixed-model.html"><a href="from-the-paired-t-test-to-the-linear-mixed-model.html"><i class="fa fa-check"></i><b>3.4</b> From the paired t-test to the linear mixed model</a></li>
<li class="chapter" data-level="3.5" data-path="linear-mixed-models.html"><a href="linear-mixed-models.html"><i class="fa fa-check"></i><b>3.5</b> Linear mixed models</a><ul>
<li class="chapter" data-level="3.5.1" data-path="linear-mixed-models.html"><a href="linear-mixed-models.html#model-type-1-varying-intercepts"><i class="fa fa-check"></i><b>3.5.1</b> Model type 1: Varying intercepts</a></li>
<li class="chapter" data-level="3.5.2" data-path="linear-mixed-models.html"><a href="linear-mixed-models.html#the-formal-statement-of-the-varying-intercepts-model"><i class="fa fa-check"></i><b>3.5.2</b> The formal statement of the varying intercepts model</a></li>
<li class="chapter" data-level="3.5.3" data-path="linear-mixed-models.html"><a href="linear-mixed-models.html#model-type-2-varying-intercepts-and-slopes-without-a-correlation"><i class="fa fa-check"></i><b>3.5.3</b> Model type 2: Varying intercepts and slopes, without a correlation</a></li>
<li class="chapter" data-level="3.5.4" data-path="linear-mixed-models.html"><a href="linear-mixed-models.html#model-type-3-varying-intercepts-and-varying-slopes-with-correlation"><i class="fa fa-check"></i><b>3.5.4</b> Model type 3: Varying intercepts and varying slopes, with correlation</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="shrinkage-in-linear-mixed-models.html"><a href="shrinkage-in-linear-mixed-models.html"><i class="fa fa-check"></i><b>3.6</b> Shrinkage in linear mixed models</a></li>
<li class="chapter" data-level="3.7" data-path="sec-LMExercises.html"><a href="sec-LMExercises.html"><i class="fa fa-check"></i><b>3.7</b> Exercises:</a><ul>
<li class="chapter" data-level="3.7.1" data-path="sec-LMExercises.html"><a href="sec-LMExercises.html#sec:LMExercisesPart1"><i class="fa fa-check"></i><b>3.7.1</b> Part 1</a></li>
<li class="chapter" data-level="3.7.2" data-path="sec-LMExercises.html"><a href="sec-LMExercises.html#sec:LMExercisesPart2"><i class="fa fa-check"></i><b>3.7.2</b> Part 2</a></li>
<li class="chapter" data-level="3.7.3" data-path="sec-LMExercises.html"><a href="sec-LMExercises.html#sec:LMExercisesPart3"><i class="fa fa-check"></i><b>3.7.3</b> Part 3</a></li>
<li class="chapter" data-level="3.7.4" data-path="sec-LMExercises.html"><a href="sec-LMExercises.html#sec:LMExercisesPart4"><i class="fa fa-check"></i><b>3.7.4</b> Part 4</a></li>
<li class="chapter" data-level="3.7.5" data-path="sec-LMExercises.html"><a href="sec-LMExercises.html#sec:LMExercisesPart5"><i class="fa fa-check"></i><b>3.7.5</b> Part 5</a></li>
<li class="chapter" data-level="3.7.6" data-path="sec-LMExercises.html"><a href="sec-LMExercises.html#sec:LMExercisesPart6"><i class="fa fa-check"></i><b>3.7.6</b> Part 6</a></li>
<li class="chapter" data-level="3.7.7" data-path="sec-LMExercises.html"><a href="sec-LMExercises.html#sec:LMExercisesPart7"><i class="fa fa-check"></i><b>3.7.7</b> Part 7</a></li>
<li class="chapter" data-level="3.7.8" data-path="sec-LMExercises.html"><a href="sec-LMExercises.html#sec:LMExercisesPart8"><i class="fa fa-check"></i><b>3.7.8</b> Part 8</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="hypothesis-testing-using-the-likelihood-ratio-test.html"><a href="hypothesis-testing-using-the-likelihood-ratio-test.html"><i class="fa fa-check"></i><b>4</b> Hypothesis testing using the likelihood ratio test</a><ul>
<li class="chapter" data-level="4.1" data-path="the-likelihood-ratio-test-the-theory.html"><a href="the-likelihood-ratio-test-the-theory.html"><i class="fa fa-check"></i><b>4.1</b> The likelihood ratio test: The theory</a></li>
<li class="chapter" data-level="4.2" data-path="a-practical-example-using-simulated-data.html"><a href="a-practical-example-using-simulated-data.html"><i class="fa fa-check"></i><b>4.2</b> A practical example using simulated data</a></li>
<li class="chapter" data-level="4.3" data-path="a-real-life-example-the-english-relative-clause-data.html"><a href="a-real-life-example-the-english-relative-clause-data.html"><i class="fa fa-check"></i><b>4.3</b> A real-life example: The English relative clause data</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="using-simulation-to-understand-your-model.html"><a href="using-simulation-to-understand-your-model.html"><i class="fa fa-check"></i><b>5</b> Using simulation to understand your model</a><ul>
<li class="chapter" data-level="5.1" data-path="a-reminder-the-maximal-linear-mixed-model.html"><a href="a-reminder-the-maximal-linear-mixed-model.html"><i class="fa fa-check"></i><b>5.1</b> A reminder: The maximal linear mixed model</a></li>
<li class="chapter" data-level="5.2" data-path="obtain-estimates-from-a-previous-study.html"><a href="obtain-estimates-from-a-previous-study.html"><i class="fa fa-check"></i><b>5.2</b> Obtain estimates from a previous study</a></li>
<li class="chapter" data-level="5.3" data-path="decide-on-a-range-of-plausible-values-of-the-effect-size.html"><a href="decide-on-a-range-of-plausible-values-of-the-effect-size.html"><i class="fa fa-check"></i><b>5.3</b> Decide on a range of plausible values of the effect size</a></li>
<li class="chapter" data-level="5.4" data-path="extract-parameter-estimates.html"><a href="extract-parameter-estimates.html"><i class="fa fa-check"></i><b>5.4</b> Extract parameter estimates</a></li>
<li class="chapter" data-level="5.5" data-path="define-a-function-for-generating-data.html"><a href="define-a-function-for-generating-data.html"><i class="fa fa-check"></i><b>5.5</b> Define a function for generating data</a></li>
<li class="chapter" data-level="5.6" data-path="repeated-generation-of-data-to-compute-power.html"><a href="repeated-generation-of-data-to-compute-power.html"><i class="fa fa-check"></i><b>5.6</b> Repeated generation of data to compute power</a></li>
<li class="chapter" data-level="5.7" data-path="what-you-can-now-do.html"><a href="what-you-can-now-do.html"><i class="fa fa-check"></i><b>5.7</b> What you can now do</a></li>
<li class="chapter" data-level="5.8" data-path="sec-Simulationexercises.html"><a href="sec-Simulationexercises.html"><i class="fa fa-check"></i><b>5.8</b> Exercises</a><ul>
<li class="chapter" data-level="5.8.1" data-path="sec-Simulationexercises.html"><a href="sec-Simulationexercises.html#sec:SimulationexercisesPart1"><i class="fa fa-check"></i><b>5.8.1</b> Part 1</a></li>
<li class="chapter" data-level="5.8.2" data-path="sec-Simulationexercises.html"><a href="sec-Simulationexercises.html#sec:SimulationexercisesPart2"><i class="fa fa-check"></i><b>5.8.2</b> Part 2</a></li>
<li class="chapter" data-level="5.8.3" data-path="sec-Simulationexercises.html"><a href="sec-Simulationexercises.html#sec:SimulationexercisesPart3"><i class="fa fa-check"></i><b>5.8.3</b> Part 3</a></li>
<li class="chapter" data-level="5.8.4" data-path="sec-Simulationexercises.html"><a href="sec-Simulationexercises.html#sec:SimulationexercisesPart4"><i class="fa fa-check"></i><b>5.8.4</b> Part 4</a></li>
<li class="chapter" data-level="5.8.5" data-path="sec-Simulationexercises.html"><a href="sec-Simulationexercises.html#sec:SimulationexercisesPart5"><i class="fa fa-check"></i><b>5.8.5</b> Part 5</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="important-distributions.html"><a href="important-distributions.html"><i class="fa fa-check"></i><b>6</b> Important distributions</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Linear Mixed Models in Linguistics and Psychology: A Comprehensive Introduction</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="hypothesis-testing-the-one-sample-t-test" class="section level2">
<h2><span class="header-section-number">2.4</span> Hypothesis testing: The one sample t-test</h2>
<p>With the central limit theorem and the idea of hypothetical repeated sampling behind us, we turn now to one of the simplest statistical tests that one can do with continuous data: the t-test.</p>
<p>Due to its simplicity, it is tempting to take only a cursory look at the t-test and move on immediately to the linear (mixed) model. This would be a mistake. The humble t-test is surprising in many ways, and holds several important lessons for us. There are subtleties in this test, and a close connection to the linear mixed model. For these reasons, it is worth slowing down and spending some time understanding this test. Once the t-test is clear, more complex statistical tests will be easier to follow, because the logic of these more complex tests will essentially be more of the same, or variations on this general theme. You will see later that t-test can be seen as an analysis of variance or ANOVA; and the paired t-test is exactly the linear mixed model with varying intercepts.</p>
<div id="the-one-sample-t-test" class="section level3">
<h3><span class="header-section-number">2.4.1</span> The one-sample t-test</h3>
<p>As in our running example, suppose we have a random sample <span class="math inline">\(y\)</span> of size <span class="math inline">\(n\)</span>, and the data come from a <span class="math inline">\(N(\mu,\sigma)\)</span> distribution, with unknown parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>. We can estimate <span class="math inline">\(\mu\)</span> from the sample mean <span class="math inline">\(\bar{y}\)</span>, which we will sometimes also write as <span class="math inline">\(\hat \mu\)</span>. We can also estimate <span class="math inline">\(\sigma\)</span> from the sample standard deviation <span class="math inline">\(s\)</span>, which we can also write as <span class="math inline">\(\hat\sigma\)</span>. These estimates in turn allow us to estimate the sampling distribution of the mean under (hypothetical) repeated sampling:</p>
<p><span class="math display">\[\begin{equation}
N(\hat\mu,\frac{\hat \sigma}{\sqrt{n}})
\end{equation}\]</span></p>
<p>It is important to realize here that the above sampling distribution is only as realistic as the estimates of the mean and standard deviation parameters—if those are inaccurately estimated, then the sampling distribution is not realistic either.</p>
<p>Assume as before that we take an independent random sample of size <span class="math inline">\(1000\)</span> from a random variable <span class="math inline">\(Y\)</span> that is normally distributed, with mean 500 and standard deviation 100. As usual, begin by estimating the mean and SE:</p>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb65-1" data-line-number="1">n &lt;-<span class="st"> </span><span class="dv">1000</span></a>
<a class="sourceLine" id="cb65-2" data-line-number="2">mu &lt;-<span class="st"> </span><span class="dv">500</span></a>
<a class="sourceLine" id="cb65-3" data-line-number="3">sigma &lt;-<span class="st"> </span><span class="dv">100</span></a>
<a class="sourceLine" id="cb65-4" data-line-number="4"><span class="co">## generate simulated data:</span></a>
<a class="sourceLine" id="cb65-5" data-line-number="5">y &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">1000</span>, <span class="dt">mean =</span> <span class="dv">500</span>, <span class="dt">sd =</span> <span class="dv">100</span>)</a>
<a class="sourceLine" id="cb65-6" data-line-number="6"><span class="co">## compute summary statistics:</span></a>
<a class="sourceLine" id="cb65-7" data-line-number="7">y_bar &lt;-<span class="st"> </span><span class="kw">mean</span>(y)</a>
<a class="sourceLine" id="cb65-8" data-line-number="8">SE &lt;-<span class="st"> </span><span class="kw">sd</span>(y)<span class="op">/</span><span class="kw">sqrt</span>(n)</a></code></pre></div>
<p>The null hypothesis significance testing (NHST) approach as practised in psychology and other areas is to set up a null hypothesis that <span class="math inline">\(\mu\)</span> has some fixed value. Just as an example, assume that our null hypothesis is:</p>
<p><span class="math display">\[\begin{equation}
H_0: \mu = 450
\end{equation}\]</span></p>
<p>This amounts to assuming that the true sampling distribution of sample means is (approximately) normally distributed and centered around 450, with the standard error estimated from the data.</p>
<p><img src="bookdown_files/figure-html/unnamed-chunk-35-1.svg" width="672" /></p>
<p>The intuitive idea here is that
- if the sample mean <span class="math inline">\(\bar{y}\)</span> is “near” the hypothesized <span class="math inline">\(\mu\)</span> (here, 450), the data are possibly consistent with the null hypothesis distribution.
- if the sample mean <span class="math inline">\(\bar{y}\)</span> is “far” from the hypothesized <span class="math inline">\(\mu\)</span>, the data are inconsistent with the null hypothesis distribution.</p>
<p>The terms “near” and “far” will be quantified by determining how many standard error units the sample mean is from the hypothesized mean. This way of thinking shifts the focus away from the sampling distribution above, towards the distance measured in standard error units.</p>
<p>The distance between the sample mean and the hypothesized mean can be written in SE units. We will say that the sample mean is <span class="math inline">\(t\)</span> standard errors away from the hypothesized mean:</p>
<p><span class="math display">\[\begin{equation}
t \times SE = \bar{x} - \mu 
\end{equation}\]</span></p>
<p>If we divide both sides with the standard error, we obtain the so-called observed t-statistic:</p>
<p><span class="math display">\[\begin{equation}
t  = \frac{\bar{x} - \mu}{SE}
\end{equation}\]</span></p>
<p>This observed t-value, an expression of the distance between the sample mean and the hypothesized mean, becomes the basis for the statistical test.</p>
<p>Notice that the t-value is a random variable: it is a transformation of <span class="math inline">\(\bar{X}\)</span>, the random variable generating the sample means. The t-value can therefore be seen as an instance of the following transformed random variable <span class="math inline">\(T\)</span>:</p>
<p><span class="math display">\[\begin{equation}
T  = \frac{\bar{X} - \mu}{SE}
\end{equation}\]</span></p>
<p>This random variable has a PDF associated with it, the t-distribution, which is defined in terms of the sample size <span class="math inline">\(n\)</span>; the pdf is written <span class="math inline">\(t(n-1)\)</span>. Under repeated sampling, the t-distribution is generated from this random variable <span class="math inline">\(T\)</span>.</p>
<p>We will compactly express the statement that “the observed t-value is assumed to be generated under repeated sampling from a t-distribution with n-1 degrees of freedom” as:</p>
<p><span class="math display">\[\begin{equation}
T \sim t(n-1)
\end{equation}\]</span></p>
<p>For large <span class="math inline">\(n\)</span>, the PDF of the random variable <span class="math inline">\(T\)</span> approaches <span class="math inline">\(N(0,1)\)</span>. This is illustrated in Figure <a href="hypothesis-testing-the-one-sample-t-test.html#fig:tnormal">2.7</a>; notice that the t-distribution has fatter tails than the normal for small <span class="math inline">\(n\)</span>, say <span class="math inline">\(n&lt;20\)</span>, but for larger n, the t-distribution and the normal are essentially identical. Incidentally, when n=2, the t-distribution <span class="math inline">\(t(1)\)</span> is the Cauchy distribution we saw earlier; this distribution is characterized by fat tails, and has no mean or variance defined for it.</p>
<div class="figure"><span id="fig:tnormal"></span>
<img src="bookdown_files/figure-html/tnormal-1.svg" alt="A visual comparison of the t-distribution (with degrees of freedom ranging from 1 to 50) with the standard normal distribution (N(0,1))." width="672" />
<p class="caption">
FIGURE 2.7: A visual comparison of the t-distribution (with degrees of freedom ranging from 1 to 50) with the standard normal distribution (N(0,1)).
</p>
</div>
<p>Thus, given a sample size <span class="math inline">\(n\)</span>, we can define a t-distribution corresponding to the null hypothesis distribution. For large values of <span class="math inline">\(n\)</span>, we could even use <span class="math inline">\(N(0,1)\)</span>, although it is traditional in psychology and linguistics to always use the t-distribution no matter how large <span class="math inline">\(n\)</span> is.</p>
<p>The null hypothesis testing procedure proceeds as follows:</p>
<ul>
<li>Define the null hypothesis: in our example, the null hypothesis was that <span class="math inline">\(\mu = 450\)</span>. This amounts to making a commitment about what fixed value we think the true underlying distribution of sample means is centered at.</li>
<li>Given data of size <span class="math inline">\(n\)</span>, estimate <span class="math inline">\(\bar{y}\)</span>, standard deviation <span class="math inline">\(s\)</span>, and from that, estimate the standard error <span class="math inline">\(s/\sqrt{n}\)</span>. The standard error will be used to describe the sampling distribution’s standard deviation.</li>
<li>Compute the observed t-value:</li>
</ul>
<p><span class="math display">\[\begin{equation}
t=\frac{\bar{y}-\mu}{s/\sqrt{n}}
\end{equation}\]</span></p>
<ul>
<li>Reject null hypothesis if the observed t-value is “large” (to be made more precise next).</li>
<li>Fail to reject the null hypothesis, or (under some conditions) even go so far as to accept the null hypothesis, if the observed t-value is “small”.</li>
</ul>
<p>What constitutes a large or small observed t-value?
Intuitively, the t-value from the sample is large when we end up far in <em>either</em> tail of the distribution. The two tails of the t-distribution will be referred to as the <em>rejection region</em>. The word <em>region</em> here refers to the real number line along the x-axis, under the tails of the distribution. The rejection region will go off to infinity on the outer sides, and is demarcated by a vertical line on the inner side of each tail. This is shown in Figure <a href="hypothesis-testing-the-one-sample-t-test.html#fig:tails">2.8</a>. It goes off to infinity because the support—the range of possible values—of the random variable that the t-distribution belongs to stretches from minus infinity to plus infinity.</p>
<div class="figure"><span id="fig:tails"></span>
<img src="bookdown_files/figure-html/tails-1.svg" alt="The rejection region in a t-distribution." width="672" />
<p class="caption">
FIGURE 2.8: The rejection region in a t-distribution.
</p>
</div>
<p>The location of the vertical lines is determined by the so-called <em>critical t-value</em> along the x-axis of the t-distribution. This is the value such that the area under the curve in the tails to the left or right of the tails is 0.025. As discussed in chapter 1, this area under the curve represents the probability of observing a value as extreme as the critical t-value, or some value that is more extreme. Notice that if we ask ourselves what the probability is of observing some particular t-value (a point value), the answer must necessarily be <span class="math inline">\(0\)</span> (if you are unclear about why, re-read chapter 1). But we can ask the question: what is the absolute t-value, written <span class="math inline">\(|t|\)</span>, such that <span class="math inline">\(P(T&gt;|t|)=0.025\)</span>? That’s the critical t-value.</p>
<p>For a given sample size <span class="math inline">\(n\)</span>, we can identify the rejection region by using the <code>qt</code> function, whose usage is analogous to the <code>qnorm</code> function, discussed in chapter 1.</p>
<p>Because the shape of the t-distribution depends on the degrees of freedom (n-1), the critical t-value beyond which we reject the null hypothesis will change depending on sample size. For large sample sizes, say <span class="math inline">\(n&gt;50\)</span>, the rejection point is about 2.</p>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb66-1" data-line-number="1"><span class="kw">abs</span>(<span class="kw">qt</span>(<span class="fl">0.025</span>, <span class="dt">df =</span> <span class="dv">15</span>))</a></code></pre></div>
<pre><code>## [1] 2.131</code></pre>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb68-1" data-line-number="1"><span class="kw">abs</span>(<span class="kw">qt</span>(<span class="fl">0.025</span>, <span class="dt">df =</span> <span class="dv">50</span>))</a></code></pre></div>
<pre><code>## [1] 2.009</code></pre>
<p>Consider the observed t-value from our sample in our running example:</p>
<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb70-1" data-line-number="1"><span class="co">## null hypothesis mean:</span></a>
<a class="sourceLine" id="cb70-2" data-line-number="2">mu &lt;-<span class="st"> </span><span class="dv">450</span></a>
<a class="sourceLine" id="cb70-3" data-line-number="3">(t_value &lt;-<span class="st"> </span>(y_bar <span class="op">-</span><span class="st"> </span>mu)<span class="op">/</span>SE)</a></code></pre></div>
<pre><code>## [1] 15.49</code></pre>
<p>This observed t-value is huge and is telling you the distance of the sample mean from the null hypothesis mean <span class="math inline">\(\mu\)</span> in standard error units.</p>
<p><span class="math display">\[\begin{equation}
t=\frac{\bar{y}-\mu_0}{s/\sqrt{n}} \hbox{ or } t\frac{s}{\sqrt{n}}=\bar{y}-\mu_0
\end{equation}\]</span></p>
<p>For large sample sizes, if the absolute t-value <span class="math inline">\(|t|\)</span> is greater than <span class="math inline">\(2\)</span> (approximately), we will reject the null hypothesis. The choice of <span class="math inline">\(2\)</span> is purely conventional and comes from standard practice in psychology and related disciplines (as we will see in this book, standard practice is sometimes not a good-enough reason to decide on such things!).</p>
<p>For a smaller sample size <span class="math inline">\(n\)</span>, you can compute the exact critical t-value:</p>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb72-1" data-line-number="1"><span class="kw">qt</span>(<span class="fl">0.025</span>, <span class="dt">df =</span> n <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)</a></code></pre></div>
<pre><code>## [1] -1.962</code></pre>
<p>Why is this critical t-value negative in sign? That is because it is on the left-hand side of the t-distribution, which is symmetric.
The corresponding value on the right-hand side is:</p>
<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb74-1" data-line-number="1"><span class="kw">qt</span>(<span class="fl">0.975</span>, <span class="dt">df =</span> n <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)</a></code></pre></div>
<pre><code>## [1] 1.962</code></pre>
<p>These values are of course identical if we ignore the sign. This is why we always frame our discussion around the absolute t-value.</p>
<p>In R, the built-in function <code>t.test</code> delivers the observed t-value. Given our running example, with the null hypothesis <span class="math inline">\(\mu=450\)</span>, R returns the following:</p>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb76-1" data-line-number="1"><span class="co">## observed t-value from t-test function:</span></a>
<a class="sourceLine" id="cb76-2" data-line-number="2"><span class="kw">t.test</span>(y, <span class="dt">mu =</span> <span class="dv">450</span>)<span class="op">$</span>statistic</a></code></pre></div>
<pre><code>##     t 
## 15.49</code></pre>
<p>The default value for the null hypothesis mean <span class="math inline">\(\mu\)</span> in this function is 0; so if one doesn’t define a null hypothesis mean, the statistical test is done with reference to a null hypothesis that <span class="math inline">\(\mu=0\)</span>. That is why this t-value does not match our calculation above:</p>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb78-1" data-line-number="1"><span class="kw">t.test</span>(y)<span class="op">$</span>statistic</a></code></pre></div>
<pre><code>##     t 
## 157.8</code></pre>
<p>In the most common usage of the t-test, the null hypothesis mean will be <span class="math inline">\(0\)</span>, because usually one is comparing a difference in means between two conditions or two sets of conditions. So the above line of code will work out correctly in those cases; but if you ever have a different null hypothesis mean than <span class="math inline">\(0\)</span>, then you have to specify it in the <code>t.test</code> function.</p>
<p>So, the t-test is used as if it furnishes a <em>decision rule</em>: either reject the null hypothesis or fail to reject it. Whenever we do an experiment and carry out a t-test, we use the t-test to make a decision: reject or fail to reject the null hypothesis.</p>
<p>Recall that behind the t-test lies the assumption that the observed t-value is coming from a random variable, <span class="math inline">\(T\sim t(n-1)\)</span>. The particular t-value we observe from a particular data-set belongs to a distribution of t-values under hypothetical repeated sampling. Thus, implicit in the logic of the t-test—and indeed every frequentist statistical test—is the assumption that the experiment is in principle repeatable: the experiment can in principle be re-run as many times as we want, assuming we have the necessary resources and time.</p>
<p>This implicit idea of the experiment’s repeatability leads to an important aspect of the t-test: its long-run properties. In other words, its ability (at least in theory) to lead the researchers to the correct decision in the long run, i.e., under (hypothetical) repeated sampling. We turn to this issue next.</p>
</div>
<div id="type-i-ii-error-and-power" class="section level3">
<h3><span class="header-section-number">2.4.2</span> Type I, II error, and power</h3>
<p>When we do a hypothesis test using the t-test, the observed t-value will either fall in the rejection region, leading us to reject the null hypothesis,
or it will land in the non-rejection region, leading us to fail to reject the null. That is a single, one-time event.</p>
<p>However, the null hypothesis can be either true or not true; we don’t know which of those two possibilities is the reality.
When we say that the null could be true, we mean that the parameter <span class="math inline">\(\mu\)</span> actually does have the hypothesized value <span class="math inline">\(\mu_0\)</span>; when we say that the null could be false, we mean that the parameter <span class="math inline">\(\mu\)</span> has some <em>specific</em> value <span class="math inline">\(\mu_{alt}\)</span> other than <span class="math inline">\(\mu_0\)</span>.
We can represent these two alternative possible realities in a tabular form, as shown below. The two columns show the two possible worlds, one in which the null is true, and the other in which it is false. The two rows show the two possible decisions we can take based on the observed t-value: reject the null or fail to reject it.</p>

<p>As the table shows, we can make two kinds of mistakes:</p>
<ul>
<li>Type I error or <span class="math inline">\(\alpha\)</span>: Reject the null when it’s true.</li>
<li>Type II error or <span class="math inline">\(\beta\)</span>: Accept the null when it’s false.</li>
</ul>
<p>In psychology and related areas, Type I error is fixed a priori at 0.05. This stipulated Type I error value is why the absolute critical t-value is kept at approximately <span class="math inline">\(2\)</span>; if, following recommendations from <span class="citation">Benjamin et al. (<a href="#ref-benjamin2018redefine">2018</a>)</span>, we were to stipulate that the Type I error be 0.005, then the critical t-value would have had to be set at:</p>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb80-1" data-line-number="1"><span class="kw">abs</span>(<span class="kw">qt</span>(<span class="fl">0.0025</span>, <span class="dt">df =</span> n <span class="op">-</span><span class="st"> </span><span class="dv">1</span>))</a></code></pre></div>
<pre><code>## [1] 2.813</code></pre>
<p>This suggested change in convention hasn’t been taken up yet in cognitive science, but this could well change one day.</p>
<p>Type II error, the probability of incorrectly accepting the null hypothesis when it is false with some particular value for the parameter <span class="math inline">\(\mu\)</span>, is conventionally recommended (e.g., by the American Psychological Association) to be kept at 0.20 or lower. This implies that the probability of correctly rejecting a null hypothesis for some particular true value of <span class="math inline">\(\mu\)</span> is 1-Type II error. This probability, called statistical power, or just power, should then obviously be larger than 0.80. Again, there is nothing special about these stipulations; they are conventions that became the norm over time.</p>
<p>Next, we will consider the trade-off between Type I and II error. For simplicity, assume that the standard error is 1, and the null hypothesis is that <span class="math inline">\(\mu=0\)</span>. This means that the t-value is really the sample mean.</p>
<p>Consider the concrete situation where, in reality, the true value of <span class="math inline">\(\mu\)</span> is <span class="math inline">\(2\)</span>. As mentioned above, the null hypothesis <span class="math inline">\(H_0\)</span> is that <span class="math inline">\(\mu=0\)</span>. Now the <span class="math inline">\(H_0\)</span> is false because <span class="math inline">\(\mu=2\)</span> and not <span class="math inline">\(0\)</span>. Type I and II error can be visualized graphically as shown in Figure <a href="hypothesis-testing-the-one-sample-t-test.html#fig:type12">2.9</a>.</p>
<div class="figure"><span id="fig:type12"></span>
<img src="bookdown_files/figure-html/type12-1.svg" alt="A visualization of Type I  and II error. The dark-shaded tails of the left-hand side distribution represent Type I error; and  the light-colored shaded region of the right-hand side distribution represents Type II error. Power is the unshaded area under the curve in  the right-hand side distribution." width="672" />
<p class="caption">
FIGURE 2.9: A visualization of Type I and II error. The dark-shaded tails of the left-hand side distribution represent Type I error; and the light-colored shaded region of the right-hand side distribution represents Type II error. Power is the unshaded area under the curve in the right-hand side distribution.
</p>
</div>
<p>To understand Figure <a href="hypothesis-testing-the-one-sample-t-test.html#fig:type12">2.9</a>, one has to consider two distributions side by side. First, consider the null hypothesis distribution, centered at 0. Under the null hypothesis distribution, the rejection region lies below the dark colored tails of the distributions. The area under the curve in these dark-colored tails is the Type I error (conventionally set at 0.05) that we decide on before we even conduct the experiment and collect the data. Because the Type I error is set at 0.05, and because the t-distribution is symmetric, the area under the curve in each tail is 0.025. The absolute critical t-value helps us demarcates the inner boundaries of the rejection regions through the vertical lines shown in the figure. These vertical lines play a crucial role in helping us understand Type II error and power. This becomes clear when we consider the distribution representing the alternative possible value of <span class="math inline">\(\mu\)</span>, the distribution centered around 2. In this second distribution, consider now the area under the curve between the vertical lines demarcating the rejection region under the null. This area under the curve is the probability of accepting the null hypothesis when the null hypothesis is false with some specific value (here, when <span class="math inline">\(\mu\)</span> has value 2).</p>
<p>Some interesting observations follow. Suppose that the true effect is in fact <span class="math inline">\(\mu=2\)</span>, as in the above illustration. Then,</p>
<ul>
<li>Simply decreasing Type I error to a smaller value like 0.005 will also increase Type II error, which means that power (1-Type II error) will fall.</li>
<li>Increasing sample size will squeeze the vertical lines closer to each other, reducing Type II error, and therefore increasing power. Decreasing sample size will have the opposite effect.</li>
<li>If we design an experiment with a larger effect size, e.g., by setting up a stronger manipulation (concete examples will be discussed in this book later on), our Type II error will go down, and therefore power will go up. Figure <a href="hypothesis-testing-the-one-sample-t-test.html#fig:highpower">2.10</a> shows a graphical visualization of a situation where the true effect size is <span class="math inline">\(\mu=4\)</span>. Here, Type II error is much smaller compared to Figure <a href="hypothesis-testing-the-one-sample-t-test.html#fig:type12">2.9</a>, where <span class="math inline">\(\mu=2\)</span>.</li>
</ul>
<div class="figure"><span id="fig:highpower"></span>
<img src="bookdown_files/figure-html/highpower-1.svg" alt="The change in Type II error if the true effect has mean 4." width="672" />
<p class="caption">
FIGURE 2.10: The change in Type II error if the true effect has mean 4.
</p>
</div>
<p>In summary, when we plan out an experiment, we are also required to specify the Type I and II error associated with the design. Both sources of error are within our control, at least to some extent. The Type I error we decide to use will determine our critical t-value and therefore our decision criterion for rejecting, failing to reject, or even (under certain conditions, to be discussed below) accepting the null hypothesis.</p>
<p>The Type II error we decide on will determine the long-run probability of incorrectly accepting the null hypothesis; its inverse (1-Type II error), statistical power, will determine the long-run probability of correctly rejecting the null hypothesis under the assumption that the <span class="math inline">\(\mu\)</span> has some particular assumed value.</p>
<p>That’s the theory anyway. In practice, researchers only rarely consider the power properties of their experiment design; the focus is almost exclusively on Type I error. The neglect of power in experiment design has had interesting consequences for theory development, as we will see later in this book. For a case study in psycholinguistics, see <span class="citation">Vasishth et al. (<a href="#ref-VasishthMertzenJaegerGelman2018">2018</a>)</span>.</p>
</div>
<div id="how-to-compute-power-for-the-one-sample-t-test" class="section level3">
<h3><span class="header-section-number">2.4.3</span> How to compute power for the one-sample t-test</h3>
<p>Power is a function of three variables:</p>
<ul>
<li>the effect size</li>
<li>the standard deviation</li>
<li>the sample size.</li>
</ul>
<p>There are two ways that one can compute power in connection with the t-test: either one can use the built-in R function, <code>power.t.test</code>, or one can use simulation.</p>
<div id="power-calculation-using-the-power.t.test" class="section level4">
<h4><span class="header-section-number">2.4.3.1</span> Power calculation using the power.t.test</h4>
<p>Suppose that we have an expectation that an effect size is 15 ms <span class="math inline">\(\pm 5\)</span> ms (this could be based on the predictions of a theoretical model, or prior data); suppose also that prior experiments show standard deviations ranging from 100 to 300 ms. This is enough information to compute a power curve as a function of effect size and standard deviation. See Figure <a href="hypothesis-testing-the-one-sample-t-test.html#fig:powercurve">2.11</a> and the associated code below.</p>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb82-1" data-line-number="1">sds &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">100</span>, <span class="dv">300</span>, <span class="dt">by =</span> <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb82-2" data-line-number="2">lower &lt;-<span class="st"> </span><span class="kw">power.t.test</span>(<span class="dt">delta =</span> <span class="dv">15</span> <span class="op">-</span><span class="st"> </span><span class="dv">5</span>, <span class="dt">sd =</span> sds, </a>
<a class="sourceLine" id="cb82-3" data-line-number="3">  <span class="dt">n =</span> <span class="dv">10</span>, <span class="dt">strict =</span> <span class="ot">TRUE</span>)<span class="op">$</span>power</a>
<a class="sourceLine" id="cb82-4" data-line-number="4">upper &lt;-<span class="st"> </span><span class="kw">power.t.test</span>(<span class="dt">delta =</span> <span class="dv">15</span> <span class="op">+</span><span class="st"> </span><span class="dv">5</span>, <span class="dt">sd =</span> sds, </a>
<a class="sourceLine" id="cb82-5" data-line-number="5">  <span class="dt">n =</span> <span class="dv">10</span>, <span class="dt">strict =</span> <span class="ot">TRUE</span>)<span class="op">$</span>power</a>
<a class="sourceLine" id="cb82-6" data-line-number="6">meanval &lt;-<span class="st"> </span><span class="kw">power.t.test</span>(<span class="dt">delta =</span> <span class="dv">15</span>, <span class="dt">sd =</span> sds, </a>
<a class="sourceLine" id="cb82-7" data-line-number="7">  <span class="dt">n =</span> <span class="dv">10</span>, <span class="dt">strict =</span> <span class="ot">TRUE</span>)<span class="op">$</span>power</a>
<a class="sourceLine" id="cb82-8" data-line-number="8"></a>
<a class="sourceLine" id="cb82-9" data-line-number="9"><span class="kw">plot</span>(sds, meanval, <span class="dt">type =</span> <span class="st">&quot;l&quot;</span>, <span class="dt">main =</span> <span class="st">&quot;Power curve (n=10)</span><span class="ch">\n</span><span class="st"> using power.t.test&quot;</span>, </a>
<a class="sourceLine" id="cb82-10" data-line-number="10">  <span class="dt">xlab =</span> <span class="st">&quot;standard deviation&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;effect size&quot;</span>)</a>
<a class="sourceLine" id="cb82-11" data-line-number="11"><span class="kw">lines</span>(sds, lower, <span class="dt">lty =</span> <span class="dv">2</span>)</a>
<a class="sourceLine" id="cb82-12" data-line-number="12"><span class="kw">lines</span>(sds, upper, <span class="dt">lty =</span> <span class="dv">2</span>)</a>
<a class="sourceLine" id="cb82-13" data-line-number="13"><span class="kw">text</span>(<span class="dv">200</span>, <span class="fl">0.05</span>, <span class="st">&quot;10&quot;</span>)</a>
<a class="sourceLine" id="cb82-14" data-line-number="14"><span class="kw">text</span>(<span class="dv">200</span>, <span class="fl">0.054</span>, <span class="st">&quot;15&quot;</span>)</a>
<a class="sourceLine" id="cb82-15" data-line-number="15"><span class="kw">text</span>(<span class="dv">200</span>, <span class="fl">0.056</span>, <span class="st">&quot;20&quot;</span>)</a></code></pre></div>
<div class="figure"><span id="fig:powercurve"></span>
<img src="bookdown_files/figure-html/powercurve-1.svg" alt="An illustration of a power curve for 10 participants, as a function of standard deviation, and three estimates of the effect: 15, 10, and 20." width="672" />
<p class="caption">
FIGURE 2.11: An illustration of a power curve for 10 participants, as a function of standard deviation, and three estimates of the effect: 15, 10, and 20.
</p>
</div>
</div>
<div id="power-calculations-using-simulation" class="section level4">
<h4><span class="header-section-number">2.4.3.2</span> Power calculations using simulation</h4>
<p>An analogous calculation as the one shown above using the <code>power.t.test</code> function can also be done using simulated data.
First, generate simulated data repeatedly for each possible combination of parameter values (here, effect size and standard deviation), and compute the proportion of significant effects for each parameter combination. This can be done by defining a function that takes as input the number of simulations, sample size, effect size, and standard deviation:</p>
<div class="sourceCode" id="cb83"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb83-1" data-line-number="1">compute_power &lt;-<span class="st"> </span><span class="cf">function</span>(<span class="dt">nsim =</span> <span class="dv">1000</span>, <span class="dt">n =</span> <span class="dv">10</span>, </a>
<a class="sourceLine" id="cb83-2" data-line-number="2">  <span class="dt">effect =</span> <span class="ot">NULL</span>, <span class="dt">stddev =</span> <span class="ot">NULL</span>) {</a>
<a class="sourceLine" id="cb83-3" data-line-number="3">  temp_power &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="ot">NA</span>, nsim)</a>
<a class="sourceLine" id="cb83-4" data-line-number="4">  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>nsim) {</a>
<a class="sourceLine" id="cb83-5" data-line-number="5">    y &lt;-<span class="st"> </span><span class="kw">rnorm</span>(n, <span class="dt">mean =</span> effect, <span class="dt">sd =</span> stddev)</a>
<a class="sourceLine" id="cb83-6" data-line-number="6">    temp_power[i] &lt;-<span class="st"> </span><span class="kw">ifelse</span>(<span class="kw">abs</span>(<span class="kw">t.test</span>(y)<span class="op">$</span>statistic) <span class="op">&gt;</span><span class="st"> </span></a>
<a class="sourceLine" id="cb83-7" data-line-number="7"><span class="st">      </span><span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">0</span>)</a>
<a class="sourceLine" id="cb83-8" data-line-number="8">  }</a>
<a class="sourceLine" id="cb83-9" data-line-number="9">  <span class="co">## return power calculation:</span></a>
<a class="sourceLine" id="cb83-10" data-line-number="10">  <span class="kw">mean</span>(temp_power)</a>
<a class="sourceLine" id="cb83-11" data-line-number="11">}</a></code></pre></div>
<p>Then, plot the power curves as a function of effect size and standard deviation, exactly as in Figure <a href="hypothesis-testing-the-one-sample-t-test.html#fig:powercurve">2.11</a>. Power calculations using simulations are shown in Figure <a href="hypothesis-testing-the-one-sample-t-test.html#fig:powercurve2">2.12</a>. It is clear that simulation-based power estimation is going to be noisy; this is because each time we are generating simulated data and then carrying out a statistical test on it. This is no longer a closed-form mathematical calculation as done in <code>power.t.test</code> (this function simply implements a formula for power calculation specified for this simple case). Because the power estimates will be noisy, we show a smoothed lowess line for each effect size estimate.</p>
<div class="figure"><span id="fig:powercurve2"></span>
<img src="bookdown_files/figure-html/powercurve2-1.svg" alt="An illustration of a power curve using simulation, for 10 participants, as a function of standard deviation, and three estimates of the effect: 15, 10, and 20. The power curves are lowess-smoothed." width="672" />
<p class="caption">
FIGURE 2.12: An illustration of a power curve using simulation, for 10 participants, as a function of standard deviation, and three estimates of the effect: 15, 10, and 20. The power curves are lowess-smoothed.
</p>
</div>
<p>In the above example, simulation-based power calculation is overkill, and completely unnecessary because we have <code>power.t.test</code>. However, the technique shown above will be extended and will become our bread-and-butter method once we switch to power calculations for complicated linear mixed models. There, no closed form calculation can be done to compute power, at least not without oversimplifying the model; simulation will be the only practical way to calculate power.</p>
<p>It is important to appreciate the fact that power is a <em>function</em>; it isn’t a single number. Because we can never be sure what the true effect size is, or what the true standard deviation is, we should always aim to report a power function, as a function of plausible values of the relevant parameters.</p>
</div>
</div>
<div id="the-p-value" class="section level3">
<h3><span class="header-section-number">2.4.4</span> The p-value</h3>
<p>Continuing with our t-test example, the <code>t.test</code> function in R will not only print out a t-value as shown above, but also a probability known as a <em>p-value</em>. This is the probability of obtaining the observed t-value that we did, or some value more extreme than that, conditional on the assumption that the null hypothesis is true.</p>
<p>We can compute the p-value “by hand”. This can be computed, as done earlier, simply by calculating the area under the curve that lies beyond the observed t-value. It is standard practice to take the tail probability on both sides of the t-distribution.</p>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb84-1" data-line-number="1">(t_value &lt;-<span class="st"> </span><span class="kw">t.test</span>(y, <span class="dt">mu =</span> <span class="dv">450</span>)<span class="op">$</span>statistic)</a></code></pre></div>
<pre><code>##     t 
## 15.49</code></pre>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb86-1" data-line-number="1"><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">pt</span>(<span class="kw">abs</span>(t_value), <span class="dt">df =</span> n <span class="op">-</span><span class="st"> </span><span class="dv">1</span>, <span class="dt">lower.tail =</span> <span class="ot">FALSE</span>)</a></code></pre></div>
<pre><code>##         t 
## 8.529e-08</code></pre>
<p>The area from both sides of the tail is taken because it is conventional to do a so-called <em>two-sided t-test</em>: our null hypothesis is that <span class="math inline">\(\mu=450\)</span>, and our alternative hypothesis is two-sided: <span class="math inline">\(\mu\)</span> is either less than 450 or <span class="math inline">\(\mu\)</span> is larger than 450. When we reject the null hypothesis, we are accepting this alternative, that <span class="math inline">\(\mu\)</span> could be some value other than 450. Notice that this alternative hypothesis is remarkably vague; we would reject the null hypothesis regardless of whether the sample mean turns out to be 600 or -600, for example. The practical implication is that the p-value gives us the strength of the evidence against the null hypothesis; it doesn’t give us evidence in favor of a specific alternative, such as saying that <span class="math inline">\(\mu\)</span> is positive or negative in sign. In psychology and allied disciplines, whenever the p-value falls below <span class="math inline">\(0.05\)</span>, it is common practice to write something along the lines that “there was reliable evidence for the predicted effect.” This statement is technically incorrect; we only ever have evidence against the null. By looking at the sample mean and its sign, we are making a very big leap that we have evidence for the specific sample mean we happened to get. As we will see below, the sample mean can be wildly far from the true mean that produced the data.</p>
<p>One need not have done a two-sided alternative; one could have defined the alternative to be one-sided. In that case, one would compute only one side of the area under the curve. This kind of one-sided test is not normally done, but one can imagine a situation where a one-sided test is justified (for example, when only one sign of the effect is possible, or if there is a strong theoretical reason to expect only one particular sign—positive or negative—on an effect). That said, in their scientific career, none of the authors of this book have ever had occasion to use a one-sided test.</p>
<p>The p-value is always interpreted with reference to the pre-defined Type I error. Conventionally, we reject the null if <span class="math inline">\(p&lt;0.05\)</span>. This is because we set the Type I error at 0.05. Keep in mind that Type I error and the p-value are two distinct things.
Type I error is the probability of your incorrectly rejecting the null under repeated sampling. This is not the same thing as your p-value. The latter will be obtained from a particular experiment, and will vary from experiment to experiment; it is a random variable. Type I error is a value we fix in advance.</p>
<div id="the-distribution-of-the-p-value-under-the-null-hypothesis" class="section level4">
<h4><span class="header-section-number">2.4.4.1</span> *The distribution of the p-value under the null hypothesis</h4>
<p>We have been talking about a continuous random variable as a dependent measure, and have learnt about the standard two-sided t-test, with a point null hypothesis. When we do such a test, we usually use the p-value to decide whether to reject the null hypothesis or not.</p>
<p>Sometimes, you will hear statisticians (e.g., Andrew Gelman on his blog) say that the null hypothesis significance test is a specific random number generator. What does that sentence mean? We explain this point here.</p>
<p>It’s worth briefly reflecting on the fact that the p-value is a random variable; call it <span class="math inline">\(Z\)</span>. The p-value is the cumulative distribution function (CDF) of the random variable <span class="math inline">\(T\)</span>, which itself is a transformation of the random variable <span class="math inline">\(\bar{Y}\)</span>:</p>
<p><span class="math inline">\(T=(\bar{X}-\mu)/(\sigma/\sqrt{n})\)</span></p>
<p>This random variable <span class="math inline">\(T\)</span> has some CDF <span class="math inline">\(F(T)\)</span>. It is possible to show that if a random variable <span class="math inline">\(Z=F(T)\)</span>, i.e., if <span class="math inline">\(Z\)</span> is the
CDF for the random variable <span class="math inline">\(T\)</span>, then <span class="math inline">\(Z \sim Uniform(0,1)\)</span>.</p>
<p>This is an amazing fact. To get a grip on this, let’s first think about the fact that when a random variable <span class="math inline">\(Z\)</span> comes from a <span class="math inline">\(Uniform(0,1)\)</span> distribution, then <span class="math inline">\(P(Z&lt;z)=z\)</span>. Consider some examples:</p>
<ul>
<li>when <span class="math inline">\(z=0\)</span>, then <span class="math inline">\(P(Z&lt;0)=0\)</span>;</li>
<li>when <span class="math inline">\(z=0.25\)</span>, then <span class="math inline">\(P(Z&lt;0.25)=0.25\)</span>;</li>
<li>when <span class="math inline">\(z=0.5\)</span>, then <span class="math inline">\(P(Z&lt;0.5)=0.5\)</span>;</li>
<li>when <span class="math inline">\(z=0.75\)</span>, then <span class="math inline">\(P(Z&lt;0.75)=0.75\)</span>;</li>
<li>when <span class="math inline">\(z=1\)</span>, then <span class="math inline">\(P(Z&lt;1)=1\)</span>.</li>
</ul>
<p>Next, we will prove the above statement, that if a random variable <span class="math inline">\(Z=F(T)\)</span>, i.e., if <span class="math inline">\(Z\)</span> is the
CDF for a random variable <span class="math inline">\(T\)</span>, then <span class="math inline">\(Z \sim Uniform(0,1)\)</span>.
The proof is actually quite astonishing and even has a name: it’s called the <em>probability integral transform</em>.</p>
<p>Suppose that <span class="math inline">\(Z\)</span> is the CDF of a random variable <span class="math inline">\(T\)</span>: <span class="math inline">\(Z=F(T)\)</span>. Then, it follows that <span class="math inline">\(P(Z\leq z)\)</span> can be rewritten in terms of the CDF of T: <span class="math inline">\(P(F(T)\leq z)\)</span>. Now, if we apply the inverse of the CDF (<span class="math inline">\(F^{-1}\)</span>) to both the left and right sides of the inequality, we get <span class="math inline">\(P(F^{-1}F(T)\leq F^{-1}(z))\)</span>.
But <span class="math inline">\(F^{-1}F(T)\)</span> gives us back <span class="math inline">\(T\)</span>; this holds because if we have a one-to-one onto function <span class="math inline">\(f(x)\)</span>, then applying the inverse <span class="math inline">\(f^{-1}\)</span> to this function gives us back <span class="math inline">\(x\)</span>.</p>
<p>The fact that <span class="math inline">\(F^{-1}F(T)\)</span> gives us back <span class="math inline">\(T\)</span> means that we can rewrite <span class="math inline">\(P(F^{-1}F(T)\leq F^{-1}(z))\)</span> as <span class="math inline">\(P(T\leq F^{-1}(z) )\)</span>. But this probability is simply the CDF <span class="math inline">\(F(F^{-1} (z))\)</span>, which simplifies to <span class="math inline">\(z\)</span>. This shows that <span class="math inline">\(P(Z\leq z) = z\)</span>; i.e., that the p-value has a uniform distribution under the null hypothesis.</p>
<p>The above proof is restated below compactly:</p>
<p><span class="math display">\[\begin{equation}
\begin{split}
P(Z\leq z) =&amp; P(F(T)\leq z)\\
=&amp; P(F^{-1}F(T)\leq F^{-1}(z))\\
=&amp; P(T\leq F^{-1}(z) ) \\
=&amp; F(F^{-1} (z))\\
=&amp; z\\
\end{split}
\end{equation}\]</span></p>
<p>It is for this reason that statisticians like Andrew Gelman periodically point out that “the null hypothesis significance test is a specific random number generator”. The practical implication of this observation is that we should not place our theory development exclusively at the feet of the p-value. As we discuss in this book, other considerations (such as replicability, uncertainty of the estimates, and power) are as or even more important.</p>
</div>
</div>
<div id="type-m-and-s-error-in-the-face-of-low-power" class="section level3">
<h3><span class="header-section-number">2.4.5</span> Type M and S error in the face of low power</h3>
<p>Beyond Type I and II error, there are also two other kinds of error to be aware of. These are Type M and S error; both sources of error are closely related to statistical power.</p>
<p>The terms Type M and S error were introduced by <span class="citation">Gelman et al. (<a href="#ref-Gelman14">2014</a>)</span>, but the ideas has been in existence for some time <span class="citation">(Hedges <a href="#ref-hedges1984estimation">1984</a>,<span class="citation">@lane1978estimating</span>)</span>. <span class="citation">Button et al. (<a href="#ref-powerfailure">2013</a>)</span> refer to Type M and S error as the “winner’s curse” and “the vibration of effects.” In related work, <span class="citation">Ioannidis (<a href="#ref-ioannidis2008most">2008</a>)</span> discusses refers to the vibration ratio in the context of epidemiology.</p>
<p>Type S and M error can be illustrated with the following example.
Suppose your true effect size is believed to be <span class="math inline">\(D=15\)</span>,
then we can compute (apart from statistical power) the following error rates, which are defined as follows:</p>
<ul>
<li><strong>Type S error</strong>: the probability that the sign of the effect is incorrect, given that the result is statistically significant.</li>
<li><strong>Type M error</strong>: the expectation of the ratio of the absolute magnitude of the effect to the hypothesized true effect size, given that result is significant.
Gelman and Carlin also call this the exaggeration ratio, which is perhaps more descriptive than “Type M error”.</li>
</ul>
<p>Suppose that our particular study has standard error <span class="math inline">\(46\)</span>, and sample size <span class="math inline">\(37\)</span>. And suppose that our true <span class="math inline">\(\mu=15\)</span>, as stated above. Then, we can compute statistical power, Type S and M error through simulation in the following manner:</p>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb88-1" data-line-number="1"><span class="co">## probable effect size, derived from past</span></a>
<a class="sourceLine" id="cb88-2" data-line-number="2"><span class="co">## studies:</span></a>
<a class="sourceLine" id="cb88-3" data-line-number="3">D &lt;-<span class="st"> </span><span class="dv">15</span></a>
<a class="sourceLine" id="cb88-4" data-line-number="4"><span class="co">## SE from the study of interest:</span></a>
<a class="sourceLine" id="cb88-5" data-line-number="5">se &lt;-<span class="st"> </span><span class="dv">46</span></a>
<a class="sourceLine" id="cb88-6" data-line-number="6">stddev &lt;-<span class="st"> </span>se <span class="op">*</span><span class="st"> </span><span class="kw">sqrt</span>(<span class="dv">37</span>)</a>
<a class="sourceLine" id="cb88-7" data-line-number="7">nsim &lt;-<span class="st"> </span><span class="dv">10000</span></a>
<a class="sourceLine" id="cb88-8" data-line-number="8">drep &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="ot">NA</span>, nsim)</a>
<a class="sourceLine" id="cb88-9" data-line-number="9"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>nsim) {</a>
<a class="sourceLine" id="cb88-10" data-line-number="10">  samp &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">37</span>, <span class="dt">mean =</span> D, <span class="dt">sd =</span> stddev)</a>
<a class="sourceLine" id="cb88-11" data-line-number="11">  drep[i] &lt;-<span class="st"> </span><span class="kw">mean</span>(samp)</a>
<a class="sourceLine" id="cb88-12" data-line-number="12">}</a></code></pre></div>
<p>Power can be computed by simply determining the proportion of times that the absolute observed t-value is larger than 2:</p>
<div class="sourceCode" id="cb89"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb89-1" data-line-number="1"><span class="co">## power: the proportion of cases where we</span></a>
<a class="sourceLine" id="cb89-2" data-line-number="2"><span class="co">## reject the null hypothesis correctly:</span></a>
<a class="sourceLine" id="cb89-3" data-line-number="3">(pow &lt;-<span class="st"> </span><span class="kw">mean</span>(<span class="kw">ifelse</span>(<span class="kw">abs</span>(drep<span class="op">/</span>se) <span class="op">&gt;</span><span class="st"> </span><span class="dv">2</span>, <span class="dv">1</span>, </a>
<a class="sourceLine" id="cb89-4" data-line-number="4">  <span class="dv">0</span>)))</a></code></pre></div>
<pre><code>## [1] 0.0608</code></pre>
<p>Power is quite low here (we deliberately chose an example with low power to illustrate Type S and M error).</p>
<p>Next, we figure out which of the cases are statistically significant (which simulated values yield <span class="math inline">\(p&lt;0.05\)</span>). As a criterion, we use a t-value of 2 to declare <span class="math inline">\(p&lt;0.05\)</span>; we could have done this more precisely by working out an exact critical t-value.</p>
<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb91-1" data-line-number="1"><span class="co">## which results in drep are significant</span></a>
<a class="sourceLine" id="cb91-2" data-line-number="2"><span class="co">## at alpha=0.05?</span></a>
<a class="sourceLine" id="cb91-3" data-line-number="3">signif &lt;-<span class="st"> </span><span class="kw">which</span>(<span class="kw">abs</span>(drep<span class="op">/</span>se) <span class="op">&gt;</span><span class="st"> </span><span class="dv">2</span>)</a></code></pre></div>
<p>Type S error is the proportion of significant cases with the wrong sign (sign error), and Type M error is the ratio by the true effect (of <span class="math inline">\(\mu=15\)</span>) is exaggerated in those simulations that happened to come out significant.</p>
<div class="sourceCode" id="cb92"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb92-1" data-line-number="1"><span class="co">## Type S error rate | signif:</span></a>
<a class="sourceLine" id="cb92-2" data-line-number="2">(types_sig &lt;-<span class="st"> </span><span class="kw">mean</span>(drep[signif] <span class="op">&lt;</span><span class="st"> </span><span class="dv">0</span>))</a></code></pre></div>
<pre><code>## [1] 0.1645</code></pre>
<div class="sourceCode" id="cb94"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb94-1" data-line-number="1"><span class="co">## Type M error rate | signif:</span></a>
<a class="sourceLine" id="cb94-2" data-line-number="2">(typem_sig &lt;-<span class="st"> </span><span class="kw">mean</span>(<span class="kw">abs</span>(drep[signif])<span class="op">/</span>D))</a></code></pre></div>
<pre><code>## [1] 7.414</code></pre>
<p>In this scenario, when power is approximately 6%, whenever we get a significant effect, the probability of obtaining the wrong sign is a whopping 16% and the effect is likely to be 7.4144 times larger than its true magnitude. The practical implication is as follows.</p>
<p>When power is low, relying on the p-value (statistical significance) to declare an effect as being present will be misleading because the decision will be based on an overestimate of the effect (Type M error), and even the sign of the effect could be wrong. This isn’t just a theoretical point; it has real-world consequences for theory development. For an example from psycholinguistics regarding this point, see <span class="citation">Vasishth et al. (<a href="#ref-VasishthMertzenJaegerGelman2018">2018</a>)</span>.</p>
<p>Another useful way to visualize Type M and S error is through the so-called funnel plot. As shown in Figure <a href="hypothesis-testing-the-one-sample-t-test.html#fig:funnel">2.13</a>, estimates obtained from low-powered studies will tend to be exaggerated (the lower part of the funnel), and as power goes up, the effect estimates start to cluster tightly around the true value of the effect.</p>
<div class="figure"><span id="fig:funnel"></span>
<img src="bookdown_files/figure-html/funnel-1.svg" alt="An illustration of a funnel plot. Shown are repeated samples of an effect estimate under different values of power, where the true value  of the effect is 15 (marked by the vertical line). Significant effects are shaded gray. The lower the power, the wider the fluctuation of the effect; under low power, it is the exaggerated effects that end up statistically significant, even though they are very biased relative to the true value. As power goes up, the effect estimates start to cluster around the true value, and significant effects are also accurate estimates of the effect. Thus, low power leads to exaggerated estimates of the effect, especially if the data are filtered by statistical significance." width="672" />
<p class="caption">
FIGURE 2.13: An illustration of a funnel plot. Shown are repeated samples of an effect estimate under different values of power, where the true value of the effect is 15 (marked by the vertical line). Significant effects are shaded gray. The lower the power, the wider the fluctuation of the effect; under low power, it is the exaggerated effects that end up statistically significant, even though they are very biased relative to the true value. As power goes up, the effect estimates start to cluster around the true value, and significant effects are also accurate estimates of the effect. Thus, low power leads to exaggerated estimates of the effect, especially if the data are filtered by statistical significance.
</p>
</div>
<p>What is important to appreciate here is the fact that significant effects “point to the truth” just in case power is high; when power is low, either null results will frequently be found even if the null is false, and those results that turn out significant will be based on Type M error.</p>
<p>In many fields, it is practically impossible to conduct a high-powered study. What should one do in this situation? When reporting results that are likely based on an underpowered study, the best approach ia to openly acknowledge the power limitation, to attempt to conduct a direct replication of the effect to establish robustness, and to attempt to synthesize the evidence from existing knowlege <span class="citation">(Cumming <a href="#ref-cumming2014new">2014</a>)</span>.</p>
<p>By direct replication, we mean that the study should be run multiple times with the same materials and design but new participants, to establish whether effect estimates in the original study and the replication study are consistent with each other. Direct replications stand in contrast to so-called conceptual replications, which are not exact repetitions of the original design, but involve some further or slightly different but related experimental manipulation. Conceptual replications are also a very useful tool for cross-validating the existence of an effect.</p>
<p>Direct replications will always differ from the original study in some way or another—the lab may differ, the protocols might differ slightly, the experimenter is different, etc. Such between-study variability is obviously unavoidable in direct-replication attempts, but they are still worthwhile for establishing the existence of an effect. To make the idea of establishing robustness through replication attempts, detailed examples of different kinds of replication attempts will be presented in this book’s example data-sets.</p>
</div>
<div id="searching-for-significance" class="section level3">
<h3><span class="header-section-number">2.4.6</span> Searching for significance</h3>
<p>The NHST procedure is essentially a decision procedure: if <span class="math inline">\(p&lt;0.05\)</span>, we reject the null hypothesis; otherwise, we fail to reject the null. Because significant results are easier to publish than non-significant results, a common approach taken by researchers (including the first author of this book, when he was a graduate student) is to run the experiment and periodically check if statistical significance has been reached. The procedure can be described as follows:</p>
<ul>
<li>The experimenter gathers <span class="math inline">\(n\)</span> data points, then checks for significance (is <span class="math inline">\(p&lt;0.05\)</span> or not?).</li>
<li>If the result is not significant, he gets more data (say, <span class="math inline">\(n\)</span> more data points). Then he checks for significance, and repeats.</li>
</ul>
<p>Since time and money are limited, he might decide to stop collecting data after some multiple of <span class="math inline">\(n\)</span> have been collected.</p>
<p>One can simulate different scenarios here. Suppose that <span class="math inline">\(n\)</span> is initially <span class="math inline">\(15\)</span>.<br />
Under the standard assumptions, we set Type I error to be <span class="math inline">\(0.05\)</span>. Let’s suppose that the null hypothesis that <span class="math inline">\(\mu=0\)</span> is in fact true, and that standard deviation is 250.</p>
<div class="sourceCode" id="cb96"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb96-1" data-line-number="1"><span class="co">## Standard properties of the t-test:</span></a>
<a class="sourceLine" id="cb96-2" data-line-number="2">pvals &lt;-<span class="st"> </span><span class="ot">NULL</span></a>
<a class="sourceLine" id="cb96-3" data-line-number="3">tstat_standard &lt;-<span class="st"> </span><span class="ot">NULL</span></a>
<a class="sourceLine" id="cb96-4" data-line-number="4">n &lt;-<span class="st"> </span><span class="dv">15</span></a>
<a class="sourceLine" id="cb96-5" data-line-number="5">nsim &lt;-<span class="st"> </span><span class="dv">10000</span></a>
<a class="sourceLine" id="cb96-6" data-line-number="6"><span class="co">## assume a standard dev of 1:</span></a>
<a class="sourceLine" id="cb96-7" data-line-number="7">stddev &lt;-<span class="st"> </span><span class="dv">250</span></a>
<a class="sourceLine" id="cb96-8" data-line-number="8">mn &lt;-<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb96-9" data-line-number="9"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>nsim) {</a>
<a class="sourceLine" id="cb96-10" data-line-number="10">  samp &lt;-<span class="st"> </span><span class="kw">rnorm</span>(n, <span class="dt">mean =</span> mn, <span class="dt">sd =</span> stddev)</a>
<a class="sourceLine" id="cb96-11" data-line-number="11">  pvals[i] &lt;-<span class="st"> </span><span class="kw">t.test</span>(samp)<span class="op">$</span>p.value</a>
<a class="sourceLine" id="cb96-12" data-line-number="12">  tstat_standard[i] &lt;-<span class="st"> </span><span class="kw">t.test</span>(samp)<span class="op">$</span>statistic</a>
<a class="sourceLine" id="cb96-13" data-line-number="13">}</a></code></pre></div>
<p>Type I error rate is about 5%, consistent with our expectations:</p>
<div class="sourceCode" id="cb97"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb97-1" data-line-number="1"><span class="kw">round</span>(<span class="kw">table</span>(pvals <span class="op">&lt;</span><span class="st"> </span><span class="fl">0.05</span>)[<span class="dv">2</span>]<span class="op">/</span>nsim, <span class="dv">2</span>)</a></code></pre></div>
<pre><code>## TRUE 
## 0.05</code></pre>
<p>But the situation quickly deteriorates as soon as we adopt the strategy outlined above. Below, we will also track the distribution of the t-statistic.</p>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb99-1" data-line-number="1">pvals &lt;-<span class="st"> </span><span class="ot">NULL</span></a>
<a class="sourceLine" id="cb99-2" data-line-number="2">tstat &lt;-<span class="st"> </span><span class="ot">NULL</span></a>
<a class="sourceLine" id="cb99-3" data-line-number="3"><span class="co">## how many subjects can I run?</span></a>
<a class="sourceLine" id="cb99-4" data-line-number="4">upper_bound &lt;-<span class="st"> </span>n <span class="op">*</span><span class="st"> </span><span class="dv">6</span></a>
<a class="sourceLine" id="cb99-5" data-line-number="5"></a>
<a class="sourceLine" id="cb99-6" data-line-number="6"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>nsim) {</a>
<a class="sourceLine" id="cb99-7" data-line-number="7">  significant &lt;-<span class="st"> </span><span class="ot">FALSE</span></a>
<a class="sourceLine" id="cb99-8" data-line-number="8">  x &lt;-<span class="st"> </span><span class="kw">rnorm</span>(n, <span class="dt">mean =</span> mn, <span class="dt">sd =</span> stddev)  <span class="co">## take sample</span></a>
<a class="sourceLine" id="cb99-9" data-line-number="9">  <span class="cf">while</span> (<span class="op">!</span>significant <span class="op">&amp;</span><span class="st"> </span><span class="kw">length</span>(x) <span class="op">&lt;</span><span class="st"> </span>upper_bound) {</a>
<a class="sourceLine" id="cb99-10" data-line-number="10">    <span class="co">## if not significant:</span></a>
<a class="sourceLine" id="cb99-11" data-line-number="11">    <span class="cf">if</span> (<span class="kw">t.test</span>(x)<span class="op">$</span>p.value <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.05</span>) {</a>
<a class="sourceLine" id="cb99-12" data-line-number="12">      x &lt;-<span class="st"> </span><span class="kw">append</span>(x, <span class="kw">rnorm</span>(n, <span class="dt">mean =</span> mn, </a>
<a class="sourceLine" id="cb99-13" data-line-number="13">        <span class="dt">sd =</span> stddev))  <span class="co">## get more data</span></a>
<a class="sourceLine" id="cb99-14" data-line-number="14">    } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb99-15" data-line-number="15">      significant &lt;-<span class="st"> </span><span class="ot">TRUE</span></a>
<a class="sourceLine" id="cb99-16" data-line-number="16">    }  <span class="co">## otherwise stop:</span></a>
<a class="sourceLine" id="cb99-17" data-line-number="17">  }</a>
<a class="sourceLine" id="cb99-18" data-line-number="18">  pvals[i] &lt;-<span class="st"> </span><span class="kw">t.test</span>(x)<span class="op">$</span>p.value</a>
<a class="sourceLine" id="cb99-19" data-line-number="19">  tstat[i] &lt;-<span class="st"> </span><span class="kw">t.test</span>(x)<span class="op">$</span>statistic</a>
<a class="sourceLine" id="cb99-20" data-line-number="20">}</a></code></pre></div>
<p>Now, Type I error rate is much higher than 5%:</p>
<div class="sourceCode" id="cb100"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb100-1" data-line-number="1"><span class="kw">round</span>(<span class="kw">table</span>(pvals <span class="op">&lt;</span><span class="st"> </span><span class="fl">0.05</span>)[<span class="dv">2</span>]<span class="op">/</span>nsim, <span class="dv">2</span>)</a></code></pre></div>
<pre><code>## TRUE 
## 0.15</code></pre>
<p>Figure <a href="hypothesis-testing-the-one-sample-t-test.html#fig:stoppingrule">2.14</a> shows the distributions of the t-statistic in the standard case vs with the above stopping rule:</p>
<div class="figure"><span id="fig:stoppingrule"></span>
<img src="bookdown_files/figure-html/stoppingrule-1.svg" alt="A comparison of the distribution of t-values with an a priori fixed stopping rule, versus a flexible stopping rule conditional on finding significance." width="672" />
<p class="caption">
FIGURE 2.14: A comparison of the distribution of t-values with an a priori fixed stopping rule, versus a flexible stopping rule conditional on finding significance.
</p>
</div>
<p>What is important to realize here is that the inflation in Type I error we observed above was due to the fact that the t-distribution is no longer a t-distribution: we have bumps in the tails when we use the flexible stopping rule, and these raise our Type I error. This demonstrates why one should fix one’s sample size in advance, based on a power analysis. One should not deploy a stopping rule like the one above; if we used such a stopping rule, we are much more likely to incorrectly declare a result as statistically significant. There can be compelling reasons to adopt the peek-and-run strategy; e.g., if one wants to avoid exposing patients to a treatment that might turn out to be harmful. In such situations, one can run an adaptive experimental trial by correcting for Type I error inflation <span class="citation">(Pocock <a href="#ref-pocock2013clinical">2013</a>)</span>. In this book, we will aim to develop a workflow whereby the sample size is fixed through power analysis, in advance of running an experiment.</p>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-benjamin2018redefine">
<p>Benjamin, Daniel J, James O Berger, Magnus Johannesson, Brian A Nosek, E-J Wagenmakers, Richard Berk, Kenneth A Bollen, et al. 2018. “Redefine Statistical Significance.” <em>Nature Human Behaviour</em> 2 (1). Nature Publishing Group: 6.</p>
</div>
<div id="ref-powerfailure">
<p>Button, Katherine S, John PA Ioannidis, Claire Mokrysz, Brian A Nosek, Jonathan Flint, Emma SJ Robinson, and Marcus R Munafò. 2013. “Power Failure: Why Small Sample Size Undermines the Reliability of Neuroscience.” <em>Nature Reviews Neuroscience</em> 14 (5). Nature Publishing Group: 365–76.</p>
</div>
<div id="ref-cumming2014new">
<p>Cumming, Geoff. 2014. “The New Statistics: Why and How.” <em>Psychological Science</em> 25 (1). Sage Publications Sage CA: Los Angeles, CA: 7–29.</p>
</div>
<div id="ref-Gelman14">
<p>Gelman, Andrew, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B. Rubin. 2014. <em>Bayesian Data Analysis</em>. Third. Boca Raton, FL: Chapman; Hall/CRC.</p>
</div>
<div id="ref-hedges1984estimation">
<p>Hedges, Larry V. 1984. “Estimation of Effect Size Under Nonrandom Sampling: The Effects of Censoring Studies Yielding Statistically Insignificant Mean Differences.” <em>Journal of Educational Statistics</em> 9 (1). Sage Publications Sage CA: Thousand Oaks, CA: 61–85.</p>
</div>
<div id="ref-ioannidis2008most">
<p>Ioannidis, John PA. 2008. “Why Most Discovered True Associations Are Inflated.” <em>Epidemiology</em> 19 (5). LWW: 640–48.</p>
</div>
<div id="ref-pocock2013clinical">
<p>Pocock, Stuart J. 2013. <em>Clinical Trials: A Practical Approach</em>. John Wiley &amp; Sons.</p>
</div>
<div id="ref-VasishthMertzenJaegerGelman2018">
<p>Vasishth, Shravan, Daniela Mertzen, Lena A. Jäger, and Andrew Gelman. 2018. “The Statistical Significance Filter Leads to Overoptimistic Expectations of Replicability.” <em>Journal of Memory and Language</em> 103: 151–75. <a href="https://doi.org/https://doi.org/10.1016/j.jml.2018.07.004">https://doi.org/https://doi.org/10.1016/j.jml.2018.07.004</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="the-confidence-interval-and-what-its-good-for.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="the-two-sample-t-test-vs-the-paired-t-test.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/lunr.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown/edit/master/inst/examples/02-SamplingDistributions.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown.pdf", "bookdown.epub", "bookdown.mobi"],
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
